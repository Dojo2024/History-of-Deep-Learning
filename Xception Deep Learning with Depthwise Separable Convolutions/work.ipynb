{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f318e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A depthwise separable convolution module. This module is a fundamental building block\n",
    "    for Xception. It consists of a depthwise convolution followed by a pointwise convolution.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (int): Size of the convolution kernel.\n",
    "        stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "        padding (int, optional): Padding added to all four sides of the input. Defaults to 0.\n",
    "        bias (bool, optional): If True, adds a learnable bias to the output. Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Depthwise Convolution ---\n",
    "        # This convolution acts on each input channel independently.\n",
    "        # 'groups=in_channels' is the key to making this a depthwise convolution.\n",
    "        # Each input channel gets its own filter, so there's no cross-channel information mixing.\n",
    "        # The number of output channels must be equal to in_channels because each channel is\n",
    "        # processed separately.\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels, # Must be same as in_channels\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=in_channels, # Key parameter for depthwise conv\n",
    "            bias=bias\n",
    "        )\n",
    "        \n",
    "        # --- Pointwise Convolution ---\n",
    "        # This is a standard 1x1 convolution.\n",
    "        # Its purpose is to take the output of the depthwise layer and mix the channel\n",
    "        # information together to create the final desired number of output channels.\n",
    "        # It learns linear combinations of the input channels.\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1, # This makes it a pointwise convolution\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the separable convolution.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Pass input through the depthwise convolution first\n",
    "        x = self.depthwise(x)\n",
    "        # Then pass the result through the pointwise convolution\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    The Xception Block. This is the main repeating unit of the Xception architecture.\n",
    "    It contains a series of separable convolutions and a residual connection.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        reps (int): Number of separable convolution layers in this block.\n",
    "        stride (int, optional): Stride for the first convolution. Used for downsampling. Defaults to 1.\n",
    "        start_with_relu (bool, optional): Whether to apply ReLU before the first convolution. Defaults to True.\n",
    "        grow_first (bool, optional): Whether to expand channels in the first or last layer. Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, reps, stride=1, start_with_relu=True, grow_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Residual Connection Path ---\n",
    "        # The residual connection adds the input 'x' to the output of the convolutional path.\n",
    "        # If the dimensions don't match (due to striding or change in channel count),\n",
    "        # we need a 'projection' convolution in the residual path to make them match.\n",
    "        if out_channels != in_channels or stride != 1:\n",
    "            # This 1x1 convolution will adjust the channel count and, if stride > 1,\n",
    "            # it will downsample the spatial dimensions to match the output of the main path.\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            # If dimensions match, the shortcut is just an identity connection (add the input directly).\n",
    "            self.shortcut = None\n",
    "\n",
    "        # --- Main Convolutional Path ---\n",
    "        layers = []\n",
    "        \n",
    "        # Determine the number of channels for the intermediate layers\n",
    "        # 'grow_first' controls whether the channel expansion happens at the first\n",
    "        # separable conv or the last one in the block.\n",
    "        intermediate_channels = out_channels if grow_first else in_channels\n",
    "\n",
    "        # The first layer in the block might have a ReLU before it, as per the paper's design.\n",
    "        if start_with_relu:\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # First separable convolution layer\n",
    "        layers.append(SeparableConv2d(in_channels, intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(intermediate_channels))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the middle separable convolution layers\n",
    "        for _ in range(reps - 2):\n",
    "            layers.append(SeparableConv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(intermediate_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # The last separable convolution layer, which may have a different stride for downsampling.\n",
    "        # It also ensures the final output has 'out_channels'.\n",
    "        layers.append(SeparableConv2d(intermediate_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        # Create the sequential module for the main path\n",
    "        self.main_path = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Xception block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Get the output from the main convolutional path\n",
    "        main_output = self.main_path(x)\n",
    "        \n",
    "        # Get the output from the residual path (either identity or projection)\n",
    "        if self.shortcut is not None:\n",
    "            residual_output = self.shortcut(x)\n",
    "        else:\n",
    "            residual_output = x\n",
    "            \n",
    "        # Add the main path output and the residual path output\n",
    "        # This is the core idea of residual connections.\n",
    "        return main_output + residual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    The Xception architecture model.\n",
    "    Based on the paper \"Xception: Deep Learning with Depthwise Separable Convolutions\"\n",
    "    by F. Chollet.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int, optional): Number of classes for the final classifier. Defaults to 1000.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ==================== Entry Flow ====================\n",
    "        \n",
    "        # The entry flow starts with two standard convolution layers.\n",
    "        # This is a common practice to quickly downsample the image and create a rich set of\n",
    "        # low-level features before moving to the more complex and efficient separable convolutions.\n",
    "        self.entry_flow_conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.entry_flow_bn1 = nn.BatchNorm2d(32)\n",
    "        self.entry_flow_relu1 = nn.ReLU()\n",
    "        \n",
    "        self.entry_flow_conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.entry_flow_bn2 = nn.BatchNorm2d(64)\n",
    "        self.entry_flow_relu2 = nn.ReLU()\n",
    "\n",
    "        # The rest of the entry flow consists of Xception blocks.\n",
    "        # Block 1: Downsamples spatial dimensions (stride=2) and increases channels.\n",
    "        self.entry_flow_block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False, grow_first=True)\n",
    "        # Block 2: Downsamples again and increases channels.\n",
    "        self.entry_flow_block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n",
    "        # Block 3: Final downsampling and channel increase in the entry flow.\n",
    "        self.entry_flow_block3 = Block(256, 728, reps=2, stride=2, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        # ==================== Middle Flow ====================\n",
    "        \n",
    "        # The middle flow is the main body of the network. It consists of 8 identical\n",
    "        # Xception blocks. These blocks do not downsample or change the number of channels.\n",
    "        # Their purpose is to repeatedly transform and refine the features at a fixed\n",
    "        # spatial resolution (19x19 in the paper) and channel depth (728).\n",
    "        middle_flow_blocks = []\n",
    "        for _ in range(8):\n",
    "            middle_flow_blocks.append(Block(728, 728, reps=3, stride=1, start_with_relu=True, grow_first=True))\n",
    "        self.middle_flow = nn.Sequential(*middle_flow_blocks)\n",
    "\n",
    "        # ==================== Exit Flow ====================\n",
    "\n",
    "        # The exit flow performs a final transformation and downsampling before the classification layer.\n",
    "        # Block 1: Increases channels and performs spatial downsampling.\n",
    "        self.exit_flow_block1 = Block(728, 1024, reps=2, stride=2, start_with_relu=True, grow_first=False)\n",
    "        \n",
    "        # The final part of the exit flow consists of two separable convolutions without a residual connection.\n",
    "        # It expands the channel space further to 2048, which serves as the final feature representation\n",
    "        # before global pooling.\n",
    "        self.exit_flow_sepconv1 = SeparableConv2d(1024, 1536, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.exit_flow_bn1 = nn.BatchNorm2d(1536)\n",
    "        self.exit_flow_relu1 = nn.ReLU()\n",
    "        \n",
    "        self.exit_flow_sepconv2 = SeparableConv2d(1536, 2048, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.exit_flow_bn2 = nn.BatchNorm2d(2048)\n",
    "        self.exit_flow_relu2 = nn.ReLU()\n",
    "        \n",
    "        # ==================== Classifier ====================\n",
    "        \n",
    "        # Global Average Pooling takes the HxWx2048 feature map and computes the average\n",
    "        # value for each of the 2048 channels, resulting in a 1x1x2048 feature vector.\n",
    "        # This is a powerful technique to reduce parameters and prevent overfitting compared\n",
    "        # to flattening and using multiple large fully-connected layers.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # The final fully-connected (linear) layer maps the 2048 features to the number of classes.\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the full Xception model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor, typically of shape (batch_size, 3, 299, 299).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits, of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # Entry Flow\n",
    "        x = self.entry_flow_conv1(x)\n",
    "        x = self.entry_flow_bn1(x)\n",
    "        x = self.entry_flow_relu1(x)\n",
    "        \n",
    "        x = self.entry_flow_conv2(x)\n",
    "        x = self.entry_flow_bn2(x)\n",
    "        x = self.entry_flow_relu2(x)\n",
    "        \n",
    "        x = self.entry_flow_block1(x)\n",
    "        x = self.entry_flow_block2(x)\n",
    "        x = self.entry_flow_block3(x)\n",
    "        \n",
    "        # Middle Flow\n",
    "        x = self.middle_flow(x)\n",
    "        \n",
    "        # Exit Flow\n",
    "        x = self.exit_flow_block1(x)\n",
    "        \n",
    "        x = self.exit_flow_sepconv1(x)\n",
    "        x = self.exit_flow_bn1(x)\n",
    "        x = self.exit_flow_relu1(x)\n",
    "        \n",
    "        x = self.exit_flow_sepconv2(x)\n",
    "        x = self.exit_flow_bn2(x)\n",
    "        x = self.exit_flow_relu2(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1) # Flatten the tensor for the FC layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
