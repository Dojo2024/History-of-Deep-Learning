{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30a080f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dccc81c521a436d955065d36f341322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd9502e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbentrevett/multi30k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2126\u001b[0m )\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1601\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1600\u001b[0m     _raise_if_offline_mode_is_enabled()\n\u001b[1;32m-> 1601\u001b[0m     dataset_readme_path \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREPOCARD_FILENAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1608\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dataset_readme_path))\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocalEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:5252\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[1;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5249\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[0;32m   5250\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m-> 5252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5264\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5268\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:923\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m--> 923\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1291\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:297\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_target\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    290\u001b[0m             \u001b[38;5;66;03m# This means it is a relative 'location' headers, as allowed by RFC 7231.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m             \u001b[38;5;66;03m# (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;66;03m# Highly inspired by `resolve_redirects` from requests library.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m             \u001b[38;5;66;03m# See https://github.com/psf/requests/blob/main/requests/sessions.py#L159\u001b[39;00m\n\u001b[0;32m    296\u001b[0m             next_url \u001b[38;5;241m=\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39m_replace(path\u001b[38;5;241m=\u001b[39mparsed_target\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mgeturl()\n\u001b[1;32m--> 297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Debojyoti Das\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329733f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache location: C:\\Users\\Debojyoti Das\\.cache\\huggingface\\hub\n",
      "- .locks\n",
      "- datasets--bentrevett--multi30k\n",
      "- models--gpt2\n",
      "- models--microsoft--deberta-v3-small\n",
      "- version.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "hf_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "print(\"Hugging Face cache location:\", hf_cache)\n",
    "\n",
    "# List top-level folders (e.g., datasets you've downloaded)\n",
    "for item in hf_cache.iterdir():\n",
    "    print(\"-\", item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78260ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Data Loading and Preparation ---\n",
    "user_home = os.path.expanduser(\"~\")\n",
    "base_path = os.path.join(user_home, \".cache\", \"huggingface\", \"hub\", \"datasets--bentrevett--multi30k\", \"snapshots\", \"4589883f3d09d4ef6361784e03f0ead219836469\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : os.path.join(base_path, \"train.jsonl\"),\n",
    "    \"validation\" : os.path.join(base_path, \"val.jsonl\"),\n",
    "    \"test\" : os.path.join(base_path, \"test.jsonl\")\n",
    "}\n",
    "\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "\n",
    "# --- 2. Tokenization ---\n",
    "def tokenize_de(text):\n",
    "    \"\"\" Tokenizes German text by converting to lowercase and splitting by whitespace and punctuation. \"\"\"\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text) # Keep German characters and letters\n",
    "    return [tok.lower() for tok in text.split()]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\" Tokenizes English text by converting to lowercase and splitting by whitespace and punctuation. \"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Keep English characters and letters\n",
    "    return [tok.lower() for tok in text.split()]\n",
    "\n",
    "\n",
    "# --- 3. Vocabulary Creation ---\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        # Initialize special tokens and mappings.\n",
    "        # <PAD>: Padding token for sequences of different lengths.\n",
    "        # <SOS>: Start of Sentence token.\n",
    "        # <EOS>: End of Sentence token.\n",
    "        # <UNK>: Unknown token for words not in our vocabulary.\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        \"\"\" Builds vocabulary from a list of sentences. \"\"\"\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "        \n",
    "        # Add words to the vocabulary only if their frequency is above the threshold.\n",
    "        # This helps in filtering out rare words that may not be useful for training.\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "                \n",
    "# --- 4. Custom Pytorch Dataset ---\n",
    "# This class will be used by the DataLoader to fetch batches of data at a time.\n",
    "\n",
    "class Multi30kDataset(Dataset):\n",
    "    def __init__(self, dataset_split, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "        self.dataset_split = dataset_split\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_split)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Fetch a single source-target pair.\n",
    "        src_sentence = self.dataset_split[index]['en']\n",
    "        trg_sentence = self.dataset_split[index]['de']\n",
    "\n",
    "        # Tokenize and numericalize the source sentence.\n",
    "        src_tokens = self.src_tokenizer(src_sentence)\n",
    "        src_numerical = [self.src_vocab.stoi[\"<SOS>\"]]\n",
    "        src_numerical.extend([self.src_vocab.stoi.get(token, self.src_vocab.stoi[\"<UNK>\"]) for token in src_tokens])\n",
    "        src_numerical.append(self.src_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # Tokenize and numericalize the target sentence.\n",
    "        trg_tokens = self.trg_tokenizer(trg_sentence)\n",
    "        trg_numerical = [self.trg_vocab.stoi[\"<SOS>\"]]\n",
    "        trg_numerical.extend([self.trg_vocab.stoi.get(token, self.trg_vocab.stoi[\"<UNK>\"]) for token in trg_tokens])\n",
    "        trg_numerical.append(self.trg_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # Return as PyTorch tensors.\n",
    "        return torch.tensor(src_numerical), torch.tensor(trg_numerical)\n",
    "    \n",
    "\n",
    "# --- 5. Collate Function for Padding ---\n",
    "class PadCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Separate the source and target sequences from the batch.\n",
    "        source_seqs = [item[0] for item in batch]\n",
    "        target_seqs = [item[1] for item in batch]\n",
    "\n",
    "        # Pad the sequences. `pad_sequence` stacks a list of tensors along a new dimension.\n",
    "        # `batch_first=False` is crucial for Transformer models in PyTorch that expect\n",
    "        # the sequence length to be the first dimension: (Seq_Len, Batch_Size).\n",
    "        padded_sources = pad_sequence(source_seqs, batch_first=False, padding_value=self.pad_idx)\n",
    "        padded_targets = pad_sequence(target_seqs, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return padded_sources, padded_targets\n",
    "\n",
    "# --- 6. Main Data Setup Function ---\n",
    "# This function orchestrates the entire process.\n",
    "\n",
    "def get_dataloaders(batch_size, freq_threshold=2):\n",
    "    # Load the raw datasets.\n",
    "    train_data = raw_datasets['train']\n",
    "    eng_sentences = [item['en'] for item in train_data]\n",
    "    ger_sentences = [item['de'] for item in train_data]\n",
    "    \n",
    "    # Build vocabularies\n",
    "    src_vocab = Vocabulary(freq_threshold)\n",
    "    trg_vocab = Vocabulary(freq_threshold)\n",
    "    src_vocab.build_vocabulary(eng_sentences, tokenize_en)\n",
    "    trg_vocab.build_vocabulary(ger_sentences, tokenize_de)\n",
    "    \n",
    "    # Instantiate the custom Dataset for each split.\n",
    "    train_dataset = Multi30kDataset(raw_datasets['train'], src_vocab, trg_vocab, tokenize_en, tokenize_de)\n",
    "    val_dataset = Multi30kDataset(raw_datasets['validation'], src_vocab, trg_vocab, tokenize_en, tokenize_de)\n",
    "    test_dataset = Multi30kDataset(raw_datasets['test'], src_vocab, trg_vocab, tokenize_en, tokenize_de)\n",
    "\n",
    "    # Get the padding index for our collate function.\n",
    "    pad_idx = src_vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    # Create DataLoader objects.\n",
    "    # The DataLoader will automatically use our custom Dataset to get items and our\n",
    "    # PadCollate class to form batches of padded tensors.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=PadCollate(pad_idx=pad_idx))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=PadCollate(pad_idx=pad_idx))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=PadCollate(pad_idx=pad_idx))\n",
    "\n",
    "    return train_loader, val_loader, test_loader, src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634f9361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (English) vocabulary size: 5919\n",
      "Target (German) vocabulary size: 7810\n",
      "\n",
      "--- Demonstrating one batch of data ---\n",
      "Shape of the source batch tensor: torch.Size([15, 4])\n",
      "Shape of the target batch tensor: torch.Size([14, 4])\n",
      "Note: Shape is (Sequence_Length, Batch_Size)\n",
      "\n",
      "--- Inspecting the first example in the batch ---\n",
      "Source (English) numericalized tensor:\n",
      "tensor([  1,  19, 240,  72, 370,  33,  15,  19, 557,   2,   0,   0,   0,   0,\n",
      "          0])\n",
      "\n",
      "Target (German) numericalized tensor:\n",
      "tensor([  1,  46, 122, 352,  31,  11,  28, 531,   2,   0,   0,   0,   0,   0])\n",
      "\n",
      "--- Converting the first example back to text ---\n",
      "Source Text: <SOS> a group of dogs standing in a river <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Target Text: <SOS> eine gruppe hunde steht in einem fluss <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Add this to the end of your step1_data_prep.py file to test it\n",
    "if __name__ == \"__main__\":\n",
    "    # Define hyperparameters\n",
    "    BATCH_SIZE = 4\n",
    "    FREQ_THRESHOLD = 2\n",
    "\n",
    "    # Get the DataLoaders and vocabularies\n",
    "    train_loader, _, _, src_vocab, trg_vocab = get_dataloaders(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        freq_threshold=FREQ_THRESHOLD\n",
    "    )\n",
    "\n",
    "    print(f\"Source (English) vocabulary size: {len(src_vocab)}\")\n",
    "    print(f\"Target (German) vocabulary size: {len(trg_vocab)}\")\n",
    "\n",
    "    # Fetch one batch from the training loader\n",
    "    print(\"\\n--- Demonstrating one batch of data ---\")\n",
    "    src_batch, trg_batch = next(iter(train_loader))\n",
    "\n",
    "    print(f\"Shape of the source batch tensor: {src_batch.shape}\")\n",
    "    print(f\"Shape of the target batch tensor: {trg_batch.shape}\")\n",
    "    print(\"Note: Shape is (Sequence_Length, Batch_Size)\")\n",
    "\n",
    "    # Select the first example from the batch to inspect\n",
    "    src_example = src_batch[:, 0]\n",
    "    trg_example = trg_batch[:, 0]\n",
    "\n",
    "    print(\"\\n--- Inspecting the first example in the batch ---\")\n",
    "    print(\"Source (English) numericalized tensor:\")\n",
    "    print(src_example)\n",
    "    print(\"\\nTarget (German) numericalized tensor:\")\n",
    "    print(trg_example)\n",
    "\n",
    "    # Convert the numericalized tensors back to text to verify\n",
    "    src_text = \" \".join([src_vocab.itos[idx.item()] for idx in src_example])\n",
    "    trg_text = \" \".join([trg_vocab.itos[idx.item()] for idx in trg_example])\n",
    "\n",
    "    print(\"\\n--- Converting the first example back to text ---\")\n",
    "    print(f\"Source Text: {src_text}\")\n",
    "    print(f\"Target Text: {trg_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        \"\"\"\n",
    "        Initializes the Multi-Head Attention layer. This is the core component\n",
    "        that allows the model to weigh the importance of different words in the\n",
    "        input sequence.\n",
    "\n",
    "        Args:\n",
    "            embed_size (int): The dimensionality of the input and output embeddings (d_model).\n",
    "            heads (int): The number of attention heads (h). The embedding will be split among these heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        # The dimension of each head's key, query, and value vectors.\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        # This assertion ensures that the embedding size can be evenly split into the number of heads.\n",
    "        # For example, if embed_size=512 and heads=8, head_dim will be 64.\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # We create one large linear layer for each of queries, keys, and values.\n",
    "        # This is more efficient than creating 'h' separate small linear layers.\n",
    "        # These layers will project the input embeddings into the Q, K, V spaces.\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        \n",
    "        # This is the final linear layer (W^O in the paper) that combines the outputs of all attention heads.\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        # In Encoder self-attention, the values, keys, and queries are all the same input that is passed to the attention layer.\n",
    "\n",
    "\n",
    "\n",
    "        # Get the batch size, which is the second dimension of the input tensors.\n",
    "        N = queries.shape[1]\n",
    "\n",
    "        # Get the sequence lengths for values, keys, and queries from the first dimension.\n",
    "        value_len, key_len, query_len = values.shape[0], keys.shape[0], queries.shape[0]\n",
    "\n",
    "        # Pass the inputs through their respective linear layers. The shape remains unchanged.\n",
    "        # Shape: (seq_len, N, embed_size) -> (seq_len, N, embed_size)\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Reshape the Q, K, V tensors to split the `embed_size` dimension into `heads` and `head_dim`.\n",
    "        # The reshape operation splits the last dimension.\n",
    "        # Shape: (seq_len, N, embed_size) -> (seq_len, N, heads, head_dim)\n",
    "        values = values.reshape(value_len, N, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(key_len, N, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(query_len, N, self.heads, self.head_dim)\n",
    "\n",
    "        # For batch matrix multiplication in PyTorch, the batch dimensions (N and heads) must come first.\n",
    "        # We use permute to reorder the dimensions.\n",
    "        # Original order: (0: seq_len, 1: N, 2: heads, 3: head_dim) -> (N, heads, seq_len, head_dim).\n",
    "        queries = queries.permute(1, 2, 0, 3)\n",
    "        keys = keys.permute(1, 2, 0, 3)\n",
    "        values = values.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Compute the dot product of queries and keys to get the attention \"energy\" or raw scores.\n",
    "        # Q shape: (N, heads, query_len, head_dim)\n",
    "        # K.transpose shape: (N, heads, head_dim, key_len)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "\n",
    "        # Apply the mask if it is provided. The mask is used to hide padding or future tokens. Where the mask is 0, we set the energy to a very small number.\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax along the last dimension (key_len) to get attention weights.\n",
    "        # This normalizes the scores into a probability distribution. Also apply the scaling factor by dividing by sqrt(d_model).\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "\n",
    "\n",
    "        # Multiply the attention weights with the value vectors. Attention shape: (N, heads, query_len, key_len). Values shape: (N, heads, value_len, head_dim)\n",
    "        # out shape: (N, heads, query_len, head_dim)\n",
    "        out = torch.matmul(attention, values)\n",
    "\n",
    "\n",
    "        # First, permute to bring seq_len back after the batch dimension, preparing for concatenation.\n",
    "        # Shape: (N, heads, query_len, head_dim) -> (N, query_len, heads, head_dim)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # Reshape to combine the heads back into a single `embed_size` dimension. This is the \"concatenation\" step.\n",
    "        # Shape: (N, query_len, heads, head_dim) -> (N, query_len, embed_size)\n",
    "        out = out.reshape(N, query_len, self.heads * self.head_dim)\n",
    "\n",
    "        # Transpose the result back to the (seq_len, N, embed_size) format, which is our convention.\n",
    "        # Shape: (N, query_len, embed_size) -> (query_len, N, embed_size)\n",
    "        out = out.transpose(0, 1)\n",
    "\n",
    "        # Pass the concatenated output through the final linear layer (W^O).\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        \"\"\"\n",
    "        Initializes a standard Transformer block, the repeating unit of the encoder.\n",
    "        It contains a multi-head attention layer and a feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            embed_size (int): The dimensionality of the embeddings (d_model).\n",
    "            heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate for regularization.\n",
    "            forward_expansion (int): The expansion factor for the FFN's inner dimension.\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # The first sub-layer: Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        \n",
    "        # Two Layer Normalization modules. One for each sub-layer.\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        # The second sub-layer: Position-wise Feed-Forward Network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        \n",
    "        # A dropout layer to be applied after each sub-layer's output before adding to the residual.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        The forward pass for the Transformer block.\n",
    "        \"\"\"\n",
    "        # --- First Sub-layer: Multi-Head Attention followed by Add & Norm ---\n",
    "        \n",
    "        attention_out = self.attention(value, key, query, mask)\n",
    "        \n",
    "        # Apply the residual connection (Add) and dropout.\n",
    "        # The 'query' input serves as the residual connection from before the attention layer.\n",
    "        add_attention = query + self.dropout(attention_out)\n",
    "        \n",
    "\n",
    "        norm_attention_out = self.norm1(add_attention)\n",
    "        \n",
    "        # --- Second Sub-layer: Feed-Forward Network followed by Add & Norm ---\n",
    "        \n",
    "        forward_out = self.feed_forward(norm_attention_out)\n",
    "        \n",
    "        add_forward = norm_attention_out + self.dropout(forward_out)\n",
    "        \n",
    "        norm_forward_out = self.norm2(add_forward)\n",
    "        \n",
    "        return norm_forward_out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder, which is a stack of TransformerBlocks.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # A lookup table for word embeddings.\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        # A lookup table for positional embeddings (learnable version).\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # A dropout layer for regularization after adding embeddings.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        The forward pass for the Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of token indices. Shape: (seq_len, N).\n",
    "            mask (torch.Tensor): Source padding mask.\n",
    "        \"\"\"\n",
    "        # Get sequence length and batch size from the input tensor shape.\n",
    "        seq_length, N = x.shape\n",
    "        \n",
    "        # Create a tensor of position indices: [0, 1, 2, ..., seq_len-1].\n",
    "        # .expand() repeats it for each item in the batch.\n",
    "        # .transpose() changes shape from (N, seq_len) to (seq_len, N) to match x.\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).transpose(0, 1).to(self.device)\n",
    "        \n",
    "        # --- Apply Embeddings and Positional Encoding ---\n",
    "        # The input x needs to be (N, seq_len) for nn.Embedding, so we transpose it.\n",
    "        word_embed = self.word_embedding(x.transpose(0, 1))\n",
    "        pos_embed = self.positional_embedding(positions.transpose(0, 1))\n",
    "        \n",
    "        # Add word and positional embeddings, apply dropout, and transpose back to our (seq_len, N, embed_size) convention.\n",
    "        out = self.dropout(word_embed + pos_embed).transpose(0, 1)\n",
    "        \n",
    "        # --- Pass through Transformer Blocks ---\n",
    "        # Sequentially pass the output through each TransformerBlock.\n",
    "        # For self-attention in the encoder, value, key, and query are all the same.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        \"\"\"\n",
    "        Initializes a single Decoder block. It has three sub-layers:\n",
    "        masked self-attention, cross-attention, and a feed-forward network.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        # This is the masked self-attention layer for the target sequence.\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        \n",
    "        # The second and third sub-layers (cross-attention + FFN) are encapsulated\n",
    "        # in a standard TransformerBlock for code reuse.\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        The forward pass for the Decoder block.\n",
    "        \"\"\"\n",
    "        # --- 1. Masked Multi-Head Self-Attention + Add & Norm ---\n",
    "        # The first sub-layer is self-attention on the target sequence `x`.\n",
    "        # We pass `trg_mask` to prevent it from attending to future tokens.\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        # Apply residual connection and dropout.\n",
    "        query = x + self.dropout(attention)\n",
    "        # Apply layer normalization.\n",
    "        query = self.norm(query)\n",
    "        \n",
    "        # --- 2. Cross-Attention + FFN (handled by TransformerBlock) ---\n",
    "        # The output of the first sub-layer (`query`) is used as the query for cross-attention.\n",
    "        # The `value` and `key` come from the encoder's output.\n",
    "        # The `src_mask` is used here to hide padding in the source sequence.\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder, a stack of DecoderBlocks, culminating in a final linear layer.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # A stack of N DecoderBlocks.\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # The final linear layer that projects the decoder output to the vocabulary size to get logits.\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        The forward pass for the Decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Target token indices. Shape: (trg_len, N).\n",
    "            enc_out (torch.Tensor): Output from the encoder. Shape: (src_len, N, embed_size).\n",
    "            src_mask, trg_mask: Masks for source and target sequences.\n",
    "        \"\"\"\n",
    "        # Get sequence length and batch size.\n",
    "        seq_length, N = x.shape\n",
    "        # Create position indices.\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).transpose(0, 1).to(self.device)\n",
    "        \n",
    "        # --- Apply Embeddings and Positional Encoding ---\n",
    "        # Transpose to (N, trg_len) for embedding, then add and transpose back to (trg_len, N, embed_size).\n",
    "        word_embed = self.word_embedding(x.transpose(0, 1))\n",
    "        pos_embed = self.positional_embedding(positions.transpose(0, 1))\n",
    "        x = self.dropout(word_embed + pos_embed).transpose(0, 1)\n",
    "        \n",
    "        # --- Pass through Decoder Blocks ---\n",
    "        # Sequentially pass through each DecoderBlock.\n",
    "        for layer in self.layers:\n",
    "            # `enc_out` is passed as the key and value for cross-attention in each block.\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "            \n",
    "        # --- Final Output Layer ---\n",
    "        # Pass the final output through the linear layer to get logits.\n",
    "        out = self.fc_out(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0.1,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the complete Transformer model by assembling the Encoder and Decoder.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Instantiate the Encoder.\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length\n",
    "        )\n",
    "\n",
    "        # Instantiate the Decoder.\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length\n",
    "        )\n",
    "\n",
    "        # Store padding indices and the device, which are needed for mask creation.\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        Creates a mask for the source sequence to hide padding tokens.\n",
    "        Input `src` has shape (src_len, N).\n",
    "        \"\"\"\n",
    "        # Transpose src to (N, src_len) to easily compare with the padding index.\n",
    "        src = src.transpose(0, 1)\n",
    "        # Create a boolean mask where `False` indicates a padding token.\n",
    "        # Then add dimensions to make it broadcastable with the attention energy matrix (N, heads, query_len, key_len).\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # Final shape: (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Creates a mask for the target sequence, combining a padding mask and a causal mask.\n",
    "        Input `trg` has shape (trg_len, N).\n",
    "        \"\"\"\n",
    "        # Transpose trg to (N, trg_len) to get batch size and sequence length.\n",
    "        trg = trg.transpose(0, 1)\n",
    "        N, trg_len = trg.shape\n",
    "        \n",
    "        # 1. Causal (Look-ahead) Mask: A lower triangular matrix of ones.\n",
    "        # Shape: (trg_len, trg_len)\n",
    "        trg_look_ahead_mask = torch.tril(torch.ones((trg_len, trg_len))).to(self.device)\n",
    "        \n",
    "        # 2. Padding Mask for the target sequence.\n",
    "        # Shape: (N, 1, trg_len, 1) after unsqueezing.\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2) # Swapped dimensions for correct broadcasting\n",
    "        \n",
    "        # 3. Combine the masks using a bitwise AND.\n",
    "        # The final mask will be 1 only if a token is not padding AND is not in the future.\n",
    "        # Broadcasting handles the dimension mismatch.\n",
    "        trg_mask = trg_pad_mask & trg_look_ahead_mask.bool()\n",
    "        # Final shape: (N, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        The forward pass for the entire Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence. Shape: (src_len, N).\n",
    "            trg (torch.Tensor): Target sequence. Shape: (trg_len, N).\n",
    "        \"\"\"\n",
    "        # Create the necessary masks for the source and target sequences.\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        # Pass the source sequence through the encoder to get the contextualized memory.\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Pass the encoder's output and the target sequence through the decoder to get the final logits.\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb44cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data and building vocabularies...\n",
      "Data loading complete.\n",
      "Source vocabulary size: 5919\n",
      "Target vocabulary size: 7810\n",
      "\n",
      "--- Starting Training ---\n",
      "[Epoch 1 / 30]\n",
      "  Batch 100/907 | Current Loss: 4.7173\n",
      "  Batch 200/907 | Current Loss: 4.1020\n",
      "  Batch 300/907 | Current Loss: 3.4979\n",
      "  Batch 400/907 | Current Loss: 3.4882\n",
      "  Batch 500/907 | Current Loss: 3.1177\n",
      "  Batch 600/907 | Current Loss: 2.8146\n",
      "  Batch 700/907 | Current Loss: 2.9691\n",
      "  Batch 800/907 | Current Loss: 2.7814\n",
      "  Batch 900/907 | Current Loss: 2.9507\n",
      "End of Epoch 1 | Average Loss: 3.4740\n",
      "\n",
      "[Epoch 2 / 30]\n",
      "  Batch 100/907 | Current Loss: 2.3982\n",
      "  Batch 200/907 | Current Loss: 2.2926\n",
      "  Batch 300/907 | Current Loss: 2.2116\n",
      "  Batch 400/907 | Current Loss: 2.1200\n",
      "  Batch 500/907 | Current Loss: 1.9316\n",
      "  Batch 600/907 | Current Loss: 2.1181\n",
      "  Batch 700/907 | Current Loss: 1.7589\n",
      "  Batch 800/907 | Current Loss: 1.8677\n",
      "  Batch 900/907 | Current Loss: 1.9866\n",
      "End of Epoch 2 | Average Loss: 2.1632\n",
      "\n",
      "[Epoch 3 / 30]\n",
      "  Batch 100/907 | Current Loss: 1.6176\n",
      "  Batch 200/907 | Current Loss: 1.6316\n",
      "  Batch 300/907 | Current Loss: 1.3352\n",
      "  Batch 400/907 | Current Loss: 1.7266\n",
      "  Batch 500/907 | Current Loss: 1.6574\n",
      "  Batch 600/907 | Current Loss: 1.6176\n",
      "  Batch 700/907 | Current Loss: 1.7850\n",
      "  Batch 800/907 | Current Loss: 1.8161\n",
      "  Batch 900/907 | Current Loss: 1.5305\n",
      "End of Epoch 3 | Average Loss: 1.6341\n",
      "\n",
      "[Epoch 4 / 30]\n",
      "  Batch 100/907 | Current Loss: 1.3126\n",
      "  Batch 200/907 | Current Loss: 1.1308\n",
      "  Batch 300/907 | Current Loss: 1.3392\n",
      "  Batch 400/907 | Current Loss: 1.2387\n",
      "  Batch 500/907 | Current Loss: 1.2334\n",
      "  Batch 600/907 | Current Loss: 1.1843\n",
      "  Batch 700/907 | Current Loss: 1.1387\n",
      "  Batch 800/907 | Current Loss: 1.3128\n",
      "  Batch 900/907 | Current Loss: 1.4354\n",
      "End of Epoch 4 | Average Loss: 1.2841\n",
      "\n",
      "[Epoch 5 / 30]\n",
      "  Batch 100/907 | Current Loss: 1.0646\n",
      "  Batch 200/907 | Current Loss: 1.0289\n",
      "  Batch 300/907 | Current Loss: 0.9913\n",
      "  Batch 400/907 | Current Loss: 0.9644\n",
      "  Batch 500/907 | Current Loss: 1.1926\n",
      "  Batch 600/907 | Current Loss: 1.0866\n",
      "  Batch 700/907 | Current Loss: 1.1058\n",
      "  Batch 800/907 | Current Loss: 1.2111\n",
      "  Batch 900/907 | Current Loss: 1.0142\n",
      "End of Epoch 5 | Average Loss: 1.0259\n",
      "\n",
      "[Epoch 6 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.7299\n",
      "  Batch 200/907 | Current Loss: 0.7434\n",
      "  Batch 300/907 | Current Loss: 1.0255\n",
      "  Batch 400/907 | Current Loss: 0.8009\n",
      "  Batch 500/907 | Current Loss: 0.9268\n",
      "  Batch 600/907 | Current Loss: 0.8053\n",
      "  Batch 700/907 | Current Loss: 0.7700\n",
      "  Batch 800/907 | Current Loss: 0.8166\n",
      "  Batch 900/907 | Current Loss: 0.8225\n",
      "End of Epoch 6 | Average Loss: 0.8231\n",
      "\n",
      "[Epoch 7 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.5022\n",
      "  Batch 200/907 | Current Loss: 0.7244\n",
      "  Batch 300/907 | Current Loss: 0.6565\n",
      "  Batch 400/907 | Current Loss: 0.6855\n",
      "  Batch 500/907 | Current Loss: 0.8740\n",
      "  Batch 600/907 | Current Loss: 0.7688\n",
      "  Batch 700/907 | Current Loss: 0.7038\n",
      "  Batch 800/907 | Current Loss: 0.7302\n",
      "  Batch 900/907 | Current Loss: 0.7362\n",
      "End of Epoch 7 | Average Loss: 0.6745\n",
      "\n",
      "[Epoch 8 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.4751\n",
      "  Batch 200/907 | Current Loss: 0.5096\n",
      "  Batch 300/907 | Current Loss: 0.4644\n",
      "  Batch 400/907 | Current Loss: 0.5661\n",
      "  Batch 500/907 | Current Loss: 0.5573\n",
      "  Batch 600/907 | Current Loss: 0.5381\n",
      "  Batch 700/907 | Current Loss: 0.5062\n",
      "  Batch 800/907 | Current Loss: 0.6720\n",
      "  Batch 900/907 | Current Loss: 0.5869\n",
      "End of Epoch 8 | Average Loss: 0.5593\n",
      "\n",
      "[Epoch 9 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.4274\n",
      "  Batch 200/907 | Current Loss: 0.3468\n",
      "  Batch 300/907 | Current Loss: 0.5109\n",
      "  Batch 400/907 | Current Loss: 0.4436\n",
      "  Batch 500/907 | Current Loss: 0.5187\n",
      "  Batch 600/907 | Current Loss: 0.5442\n",
      "  Batch 700/907 | Current Loss: 0.5815\n",
      "  Batch 800/907 | Current Loss: 0.5870\n",
      "  Batch 900/907 | Current Loss: 0.4705\n",
      "End of Epoch 9 | Average Loss: 0.4731\n",
      "\n",
      "[Epoch 10 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.3822\n",
      "  Batch 200/907 | Current Loss: 0.3793\n",
      "  Batch 300/907 | Current Loss: 0.3186\n",
      "  Batch 400/907 | Current Loss: 0.3891\n",
      "  Batch 500/907 | Current Loss: 0.3840\n",
      "  Batch 600/907 | Current Loss: 0.4338\n",
      "  Batch 700/907 | Current Loss: 0.4307\n",
      "  Batch 800/907 | Current Loss: 0.3742\n",
      "  Batch 900/907 | Current Loss: 0.4803\n",
      "End of Epoch 10 | Average Loss: 0.4101\n",
      "\n",
      "[Epoch 11 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.3263\n",
      "  Batch 200/907 | Current Loss: 0.3869\n",
      "  Batch 300/907 | Current Loss: 0.3253\n",
      "  Batch 400/907 | Current Loss: 0.2726\n",
      "  Batch 500/907 | Current Loss: 0.4359\n",
      "  Batch 600/907 | Current Loss: 0.4084\n",
      "  Batch 700/907 | Current Loss: 0.3907\n",
      "  Batch 800/907 | Current Loss: 0.3032\n",
      "  Batch 900/907 | Current Loss: 0.4250\n",
      "End of Epoch 11 | Average Loss: 0.3619\n",
      "\n",
      "[Epoch 12 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.3182\n",
      "  Batch 200/907 | Current Loss: 0.2881\n",
      "  Batch 300/907 | Current Loss: 0.2637\n",
      "  Batch 400/907 | Current Loss: 0.2761\n",
      "  Batch 500/907 | Current Loss: 0.4133\n",
      "  Batch 600/907 | Current Loss: 0.3931\n",
      "  Batch 700/907 | Current Loss: 0.3235\n",
      "  Batch 800/907 | Current Loss: 0.3457\n",
      "  Batch 900/907 | Current Loss: 0.4285\n",
      "End of Epoch 12 | Average Loss: 0.3250\n",
      "\n",
      "[Epoch 13 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2028\n",
      "  Batch 200/907 | Current Loss: 0.3135\n",
      "  Batch 300/907 | Current Loss: 0.2752\n",
      "  Batch 400/907 | Current Loss: 0.2908\n",
      "  Batch 500/907 | Current Loss: 0.4429\n",
      "  Batch 600/907 | Current Loss: 0.3148\n",
      "  Batch 700/907 | Current Loss: 0.3575\n",
      "  Batch 800/907 | Current Loss: 0.3866\n",
      "  Batch 900/907 | Current Loss: 0.3126\n",
      "End of Epoch 13 | Average Loss: 0.2962\n",
      "\n",
      "[Epoch 14 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2294\n",
      "  Batch 200/907 | Current Loss: 0.2650\n",
      "  Batch 300/907 | Current Loss: 0.3108\n",
      "  Batch 400/907 | Current Loss: 0.2277\n",
      "  Batch 500/907 | Current Loss: 0.3004\n",
      "  Batch 600/907 | Current Loss: 0.2962\n",
      "  Batch 700/907 | Current Loss: 0.2883\n",
      "  Batch 800/907 | Current Loss: 0.3117\n",
      "  Batch 900/907 | Current Loss: 0.2687\n",
      "End of Epoch 14 | Average Loss: 0.2754\n",
      "\n",
      "[Epoch 15 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2577\n",
      "  Batch 200/907 | Current Loss: 0.1855\n",
      "  Batch 300/907 | Current Loss: 0.2724\n",
      "  Batch 400/907 | Current Loss: 0.2413\n",
      "  Batch 500/907 | Current Loss: 0.2502\n",
      "  Batch 600/907 | Current Loss: 0.2119\n",
      "  Batch 700/907 | Current Loss: 0.3087\n",
      "  Batch 800/907 | Current Loss: 0.3386\n",
      "  Batch 900/907 | Current Loss: 0.3087\n",
      "End of Epoch 15 | Average Loss: 0.2542\n",
      "\n",
      "[Epoch 16 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2112\n",
      "  Batch 200/907 | Current Loss: 0.1611\n",
      "  Batch 300/907 | Current Loss: 0.2557\n",
      "  Batch 400/907 | Current Loss: 0.2612\n",
      "  Batch 500/907 | Current Loss: 0.3156\n",
      "  Batch 600/907 | Current Loss: 0.3046\n",
      "  Batch 700/907 | Current Loss: 0.2537\n",
      "  Batch 800/907 | Current Loss: 0.2561\n",
      "  Batch 900/907 | Current Loss: 0.2732\n",
      "End of Epoch 16 | Average Loss: 0.2411\n",
      "\n",
      "[Epoch 17 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1969\n",
      "  Batch 200/907 | Current Loss: 0.2032\n",
      "  Batch 300/907 | Current Loss: 0.2290\n",
      "  Batch 400/907 | Current Loss: 0.1731\n",
      "  Batch 500/907 | Current Loss: 0.2787\n",
      "  Batch 600/907 | Current Loss: 0.3027\n",
      "  Batch 700/907 | Current Loss: 0.2114\n",
      "  Batch 800/907 | Current Loss: 0.2670\n",
      "  Batch 900/907 | Current Loss: 0.2208\n",
      "End of Epoch 17 | Average Loss: 0.2249\n",
      "\n",
      "[Epoch 18 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2097\n",
      "  Batch 200/907 | Current Loss: 0.2482\n",
      "  Batch 300/907 | Current Loss: 0.2302\n",
      "  Batch 400/907 | Current Loss: 0.2598\n",
      "  Batch 500/907 | Current Loss: 0.2115\n",
      "  Batch 600/907 | Current Loss: 0.2025\n",
      "  Batch 700/907 | Current Loss: 0.2310\n",
      "  Batch 800/907 | Current Loss: 0.1727\n",
      "  Batch 900/907 | Current Loss: 0.2322\n",
      "End of Epoch 18 | Average Loss: 0.2156\n",
      "\n",
      "[Epoch 19 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2252\n",
      "  Batch 200/907 | Current Loss: 0.2026\n",
      "  Batch 300/907 | Current Loss: 0.2080\n",
      "  Batch 400/907 | Current Loss: 0.2326\n",
      "  Batch 500/907 | Current Loss: 0.2516\n",
      "  Batch 600/907 | Current Loss: 0.2447\n",
      "  Batch 700/907 | Current Loss: 0.2187\n",
      "  Batch 800/907 | Current Loss: 0.2475\n",
      "  Batch 900/907 | Current Loss: 0.2408\n",
      "End of Epoch 19 | Average Loss: 0.2040\n",
      "\n",
      "[Epoch 20 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.2495\n",
      "  Batch 200/907 | Current Loss: 0.1233\n",
      "  Batch 300/907 | Current Loss: 0.2437\n",
      "  Batch 400/907 | Current Loss: 0.2731\n",
      "  Batch 500/907 | Current Loss: 0.1917\n",
      "  Batch 600/907 | Current Loss: 0.1921\n",
      "  Batch 700/907 | Current Loss: 0.2206\n",
      "  Batch 800/907 | Current Loss: 0.2442\n",
      "  Batch 900/907 | Current Loss: 0.2305\n",
      "End of Epoch 20 | Average Loss: 0.1964\n",
      "\n",
      "[Epoch 21 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1445\n",
      "  Batch 200/907 | Current Loss: 0.1754\n",
      "  Batch 300/907 | Current Loss: 0.2022\n",
      "  Batch 400/907 | Current Loss: 0.1514\n",
      "  Batch 500/907 | Current Loss: 0.1908\n",
      "  Batch 600/907 | Current Loss: 0.1463\n",
      "  Batch 700/907 | Current Loss: 0.2374\n",
      "  Batch 800/907 | Current Loss: 0.1404\n",
      "  Batch 900/907 | Current Loss: 0.1751\n",
      "End of Epoch 21 | Average Loss: 0.1886\n",
      "\n",
      "[Epoch 22 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1566\n",
      "  Batch 200/907 | Current Loss: 0.1682\n",
      "  Batch 300/907 | Current Loss: 0.1268\n",
      "  Batch 400/907 | Current Loss: 0.1787\n",
      "  Batch 500/907 | Current Loss: 0.1611\n",
      "  Batch 600/907 | Current Loss: 0.2359\n",
      "  Batch 700/907 | Current Loss: 0.1844\n",
      "  Batch 800/907 | Current Loss: 0.1798\n",
      "  Batch 900/907 | Current Loss: 0.1741\n",
      "End of Epoch 22 | Average Loss: 0.1804\n",
      "\n",
      "[Epoch 23 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.0974\n",
      "  Batch 200/907 | Current Loss: 0.2762\n",
      "  Batch 300/907 | Current Loss: 0.2067\n",
      "  Batch 400/907 | Current Loss: 0.1979\n",
      "  Batch 500/907 | Current Loss: 0.1827\n",
      "  Batch 600/907 | Current Loss: 0.1688\n",
      "  Batch 700/907 | Current Loss: 0.2532\n",
      "  Batch 800/907 | Current Loss: 0.1592\n",
      "  Batch 900/907 | Current Loss: 0.2034\n",
      "End of Epoch 23 | Average Loss: 0.1739\n",
      "\n",
      "[Epoch 24 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1436\n",
      "  Batch 200/907 | Current Loss: 0.1925\n",
      "  Batch 300/907 | Current Loss: 0.1667\n",
      "  Batch 400/907 | Current Loss: 0.1676\n",
      "  Batch 500/907 | Current Loss: 0.2352\n",
      "  Batch 600/907 | Current Loss: 0.2062\n",
      "  Batch 700/907 | Current Loss: 0.1785\n",
      "  Batch 800/907 | Current Loss: 0.1896\n",
      "  Batch 900/907 | Current Loss: 0.1521\n",
      "End of Epoch 24 | Average Loss: 0.1668\n",
      "\n",
      "[Epoch 25 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1180\n",
      "  Batch 200/907 | Current Loss: 0.1280\n",
      "  Batch 300/907 | Current Loss: 0.2501\n",
      "  Batch 400/907 | Current Loss: 0.1831\n",
      "  Batch 500/907 | Current Loss: 0.1772\n",
      "  Batch 600/907 | Current Loss: 0.1340\n",
      "  Batch 700/907 | Current Loss: 0.1482\n",
      "  Batch 800/907 | Current Loss: 0.1674\n",
      "  Batch 900/907 | Current Loss: 0.1649\n",
      "End of Epoch 25 | Average Loss: 0.1633\n",
      "\n",
      "[Epoch 26 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1395\n",
      "  Batch 200/907 | Current Loss: 0.1541\n",
      "  Batch 300/907 | Current Loss: 0.1716\n",
      "  Batch 400/907 | Current Loss: 0.1508\n",
      "  Batch 500/907 | Current Loss: 0.1702\n",
      "  Batch 600/907 | Current Loss: 0.2186\n",
      "  Batch 700/907 | Current Loss: 0.1459\n",
      "  Batch 800/907 | Current Loss: 0.1910\n",
      "  Batch 900/907 | Current Loss: 0.1278\n",
      "End of Epoch 26 | Average Loss: 0.1575\n",
      "\n",
      "[Epoch 27 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1139\n",
      "  Batch 200/907 | Current Loss: 0.1165\n",
      "  Batch 300/907 | Current Loss: 0.1478\n",
      "  Batch 400/907 | Current Loss: 0.1761\n",
      "  Batch 500/907 | Current Loss: 0.2163\n",
      "  Batch 600/907 | Current Loss: 0.1628\n",
      "  Batch 700/907 | Current Loss: 0.1203\n",
      "  Batch 800/907 | Current Loss: 0.2365\n",
      "  Batch 900/907 | Current Loss: 0.1393\n",
      "End of Epoch 27 | Average Loss: 0.1553\n",
      "\n",
      "[Epoch 28 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1670\n",
      "  Batch 200/907 | Current Loss: 0.1307\n",
      "  Batch 300/907 | Current Loss: 0.1724\n",
      "  Batch 400/907 | Current Loss: 0.1263\n",
      "  Batch 500/907 | Current Loss: 0.1821\n",
      "  Batch 600/907 | Current Loss: 0.1438\n",
      "  Batch 700/907 | Current Loss: 0.1696\n",
      "  Batch 800/907 | Current Loss: 0.1679\n",
      "  Batch 900/907 | Current Loss: 0.1197\n",
      "End of Epoch 28 | Average Loss: 0.1501\n",
      "\n",
      "[Epoch 29 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.0844\n",
      "  Batch 200/907 | Current Loss: 0.1581\n",
      "  Batch 300/907 | Current Loss: 0.1409\n",
      "  Batch 400/907 | Current Loss: 0.1337\n",
      "  Batch 500/907 | Current Loss: 0.1975\n",
      "  Batch 600/907 | Current Loss: 0.1798\n",
      "  Batch 700/907 | Current Loss: 0.2092\n",
      "  Batch 800/907 | Current Loss: 0.1856\n",
      "  Batch 900/907 | Current Loss: 0.2064\n",
      "End of Epoch 29 | Average Loss: 0.1460\n",
      "\n",
      "[Epoch 30 / 30]\n",
      "  Batch 100/907 | Current Loss: 0.1005\n",
      "  Batch 200/907 | Current Loss: 0.1642\n",
      "  Batch 300/907 | Current Loss: 0.1906\n",
      "  Batch 400/907 | Current Loss: 0.2038\n",
      "  Batch 500/907 | Current Loss: 0.0913\n",
      "  Batch 600/907 | Current Loss: 0.1845\n",
      "  Batch 700/907 | Current Loss: 0.1549\n",
      "  Batch 800/907 | Current Loss: 0.1271\n",
      "  Batch 900/907 | Current Loss: 0.2027\n",
      "End of Epoch 30 | Average Loss: 0.1423\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 1. Setup and Hyperparameters ---\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 3e-4 # 0.0003\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "SRC_VOCAB_SIZE = 0 # Will be set after loading data\n",
    "TRG_VOCAB_SIZE = 0 # Will be set after loading data\n",
    "EMBED_SIZE = 512\n",
    "NUM_LAYERS = 3 # A smaller number for faster training on a single machine\n",
    "HEADS = 8\n",
    "FORWARD_EXPANSION = 4\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = 100\n",
    "SRC_PAD_IDX = 0 # Will be set after loading data\n",
    "TRG_PAD_IDX = 0 # Will be set after loading data\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "print(\"Loading data and building vocabularies...\")\n",
    "train_loader, _, _, src_vocab, trg_vocab = get_dataloaders(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Update vocab sizes and padding indices from the loaded data\n",
    "SRC_VOCAB_SIZE = len(src_vocab)\n",
    "TRG_VOCAB_SIZE = len(trg_vocab)\n",
    "SRC_PAD_IDX = src_vocab.stoi[\"<PAD>\"]\n",
    "TRG_PAD_IDX = trg_vocab.stoi[\"<PAD>\"]\n",
    "print(\"Data loading complete.\")\n",
    "print(f\"Source vocabulary size: {SRC_VOCAB_SIZE}\")\n",
    "print(f\"Target vocabulary size: {TRG_VOCAB_SIZE}\")\n",
    "\n",
    "# --- 3. Initialize Model, Optimizer, and Loss Function ---\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    trg_vocab_size=TRG_VOCAB_SIZE,\n",
    "    src_pad_idx=SRC_PAD_IDX,\n",
    "    trg_pad_idx=TRG_PAD_IDX,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    forward_expansion=FORWARD_EXPANSION,\n",
    "    heads=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    device=device,\n",
    "    max_length=MAX_LENGTH,\n",
    ").to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "# We pass the model's parameters to the optimizer so it knows what to update.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize the loss function\n",
    "# `ignore_index` is very important. It tells the loss function to ignore the loss\n",
    "# calculation for tokens that are padding.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "# --- 4. The Training Loop ---\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"[Epoch {epoch+1} / {NUM_EPOCHS}]\")\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # We will use this to track the average loss for the epoch\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Iterate over batches from the DataLoader\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move data to the same device as the model\n",
    "        src_data, trg_data = batch\n",
    "        src = src_data.to(device)\n",
    "        trg = trg_data.to(device)\n",
    "        \n",
    "        # --- Forward Pass ---\n",
    "        # The target sequence needs to be prepared for teacher forcing.\n",
    "        # The input to the decoder should be all tokens except the last one.\n",
    "        # The ground truth for the loss function should be all tokens except the first one (<SOS>).\n",
    "        trg_input = trg[:-1, :] # Shape: (trg_len - 1, N)\n",
    "        \n",
    "        # The model's forward pass\n",
    "        output = model(src, trg_input) # Shape: (trg_len - 1, N, trg_vocab_size)\n",
    "        \n",
    "        # --- Loss Calculation ---\n",
    "        # Reshape the output and target for the CrossEntropyLoss function.\n",
    "        # It expects the output to be (N * (trg_len - 1), trg_vocab_size)\n",
    "        # and the target to be (N * (trg_len - 1)).\n",
    "        output_for_loss = output.reshape(-1, output.shape[2])\n",
    "        target_for_loss = trg[1:, :].reshape(-1)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output_for_loss, target_for_loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # --- Backward Pass and Optimization ---\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Current Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and print average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"End of Epoch {epoch+1} | Average Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# torch.save(model.state_dict(), \"my_transformer_model.pth\")\n",
    "# print(\"Model saved to my_transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddc6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: This is a test .\n",
      "German translation: dies ist ein <UNK>\n"
     ]
    }
   ],
   "source": [
    "# Translate an English sentence using the trained Transformer model\n",
    "\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_length=50):\n",
    "    # Tokenize and numericalize the input sentence\n",
    "    tokens = [\"<SOS>\"] + [tok.lower() for tok in sentence.split()] + [\"<EOS>\"]\n",
    "    src_indices = [src_vocab.stoi.get(tok, src_vocab.stoi[\"<UNK>\"]) for tok in tokens]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(1).to(device)  # Shape: (seq_len, 1)\n",
    "\n",
    "    # Encode source sentence\n",
    "    with torch.no_grad():\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "        # Prepare initial target input (<SOS>)\n",
    "        trg_indices = [trg_vocab.stoi[\"<SOS>\"]]\n",
    "        for _ in range(max_length):\n",
    "            trg_tensor = torch.tensor(trg_indices).unsqueeze(1).to(device)  # Shape: (cur_len, 1)\n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "            output = model.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "            pred_token = output.argmax(2)[-1, 0].item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == trg_vocab.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "\n",
    "    # Convert indices to words\n",
    "    translated_tokens = [trg_vocab.itos[idx] for idx in trg_indices]\n",
    "    return \" \".join(translated_tokens[1:-1])  # Remove <SOS> and <EOS> for display\n",
    "\n",
    "# Example usage\n",
    "test_en_sentence = \"This is a test .\"\n",
    "translation = translate_sentence(test_en_sentence, src_vocab, trg_vocab, model, device)\n",
    "print(f\"English: {test_en_sentence}\")\n",
    "print(f\"German translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db969ac",
   "metadata": {},
   "source": [
    "### Let's work on a model that can identify nouns and replace the token UNK with an appropriate noun in the output.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
