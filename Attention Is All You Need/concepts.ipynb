{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb14ad3",
   "metadata": {},
   "source": [
    "### Implementation of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ac474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hyperparameters ---\n",
      "Sequence Length (S): 4\n",
      "Model/Embedding Dimension (d_model): 6\n",
      "Key/Query Dimension (d_k): 6\n",
      "Value Dimension (d_v): 6\n",
      "\n",
      "--- Input Matrix X (Shape: (4, 6)) ---\n",
      "This matrix represents a sequence of 4 tokens, each with a 6-dimensional embedding.\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696]\n",
      " [ 1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975]\n",
      " [ 0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733]\n",
      " [-0.90802408 -1.4123037   1.46564877 -0.2257763   0.0675282  -1.42474819]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Hyperparameters ---\n",
    "S = 4          # Sequence length (number of tokens)\n",
    "d_model = 6    # Dimensionality of the model/embeddings\n",
    "d_k = d_model  # Dimensionality of Keys and Queries (for single head)\n",
    "d_v = d_model  # Dimensionality of Values (for single head)\n",
    "\n",
    "print(f\"--- Hyperparameters ---\")\n",
    "print(f\"Sequence Length (S): {S}\")\n",
    "print(f\"Model/Embedding Dimension (d_model): {d_model}\")\n",
    "print(f\"Key/Query Dimension (d_k): {d_k}\")\n",
    "print(f\"Value Dimension (d_v): {d_v}\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Create Sample Input Data (Token Embeddings) ---\n",
    "# In a real model, this would be the output of an embedding layer.\n",
    "# For our purpose, we'll create a random matrix.\n",
    "# We use a fixed seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(S, d_model)\n",
    "\n",
    "print(f\"--- Input Matrix X (Shape: {X.shape}) ---\")\n",
    "print(\"This matrix represents a sequence of 4 tokens, each with a 6-dimensional embedding.\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b82343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part A: Query, Key, and Value Matrices ---\n",
      "Shape of Q: (4, 6)\n",
      "Shape of K: (4, 6)\n",
      "Shape of V: (4, 6)\n",
      "\n",
      "--- Part B: Raw Attention Scores (Q @ K.T) ---\n",
      "Shape of scores: (4, 4)\n",
      "This [S, S] matrix shows the raw compatibility between each query (row) and each key (column).\n",
      "[[  2.51889084  -7.29616433  13.79556179  12.64020041]\n",
      " [  4.97930562   1.55709123  28.89221706  11.53667062]\n",
      " [ 14.57857978  14.59243409 -59.0346232  -17.30294836]\n",
      " [  0.92217982  -6.4552404   -4.47717683   5.1656522 ]]\n",
      "\n",
      "\n",
      "--- Part C: Scaled Attention Scores ---\n",
      "Shape of scaled_scores: (4, 4)\n",
      "Scaling factor (sqrt(d_k)): 2.4495\n",
      "Scores after dividing by the scaling factor.\n",
      "[[  1.02833288  -2.97864661   5.63201452   5.16034021]\n",
      " [  2.03279301   0.63567983  11.79519822   4.70982606]\n",
      " [  5.95168027   5.95733627 -24.100784    -7.06389909]\n",
      " [  0.37647834  -2.63534086  -1.82779979   2.10886868]]\n",
      "\n",
      "\n",
      "--- Part D: Attention Weights (after Softmax) ---\n",
      "Shape of attention_weights: (4, 4)\n",
      "Each row now represents a probability distribution and sums to 1.\n",
      "This matrix A tells us 'where to look'.\n",
      "[[6.12849101e-03 1.11466527e-04 6.11937525e-01 3.81822518e-01]\n",
      " [5.75236792e-05 1.42261739e-05 9.99091747e-01 8.36502682e-04]\n",
      " [4.98585451e-01 5.01413439e-01 4.42710886e-14 1.10954598e-06]\n",
      " [1.46763454e-01 7.22106857e-03 1.61924326e-02 8.29823044e-01]]\n",
      "Sum of the first row of weights: 1.0000\n",
      "\n",
      "\n",
      "--- Part E: Final Output Matrix O (A @ V) ---\n",
      "Shape of O: (4, 6)\n",
      "This is the final contextualized output. Each row is a weighted sum of all value vectors.\n",
      "[[ 0.19028304 -2.70794733  1.58586341 -2.65571226 -1.22498655  2.9917326 ]\n",
      " [-1.87379942 -5.86071822 -0.31672245 -1.92588447 -1.4917405   1.71601181]\n",
      " [ 1.24175218 -0.80207389  0.7347901  -0.25686698 -0.99681396  0.46642629]\n",
      " [ 3.05734209  2.04335097  4.0918789  -3.20196136 -0.84105521  4.53283216]]\n"
     ]
    }
   ],
   "source": [
    "# We continue from Step 1, using the variables S, d_model, d_k, d_v, and X.\n",
    "\n",
    "# --- Part A: Linear Projections ---\n",
    "# In a real model, these weight matrices are learned during training.\n",
    "# We'll initialize them randomly for this demonstration.\n",
    "np.random.seed(0) # Use a different seed for weights\n",
    "W_Q = np.random.randn(d_model, d_k)\n",
    "W_K = np.random.randn(d_model, d_k)\n",
    "W_V = np.random.randn(d_model, d_v)\n",
    "\n",
    "# Project X into Q, K, and V spaces\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"--- Part A: Query, Key, and Value Matrices ---\")\n",
    "print(f\"Shape of Q: {Q.shape}\")\n",
    "print(f\"Shape of K: {K.shape}\")\n",
    "print(f\"Shape of V: {V.shape}\\n\")\n",
    "\n",
    "\n",
    "# --- Part B: Calculating Attention Scores ---\n",
    "scores = Q @ K.T\n",
    "# The @ operator in NumPy is used for matrix multiplication.\n",
    "# K.T is the transpose of the Key matrix.\n",
    "\n",
    "print(\"--- Part B: Raw Attention Scores (Q @ K.T) ---\")\n",
    "print(f\"Shape of scores: {scores.shape}\")\n",
    "print(\"This [S, S] matrix shows the raw compatibility between each query (row) and each key (column).\")\n",
    "print(scores)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Part C: Scaling ---\n",
    "# The scaling factor prevents the dot products from becoming too large.\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "print(\"--- Part C: Scaled Attention Scores ---\")\n",
    "print(f\"Shape of scaled_scores: {scaled_scores.shape}\")\n",
    "print(f\"Scaling factor (sqrt(d_k)): {np.sqrt(d_k):.4f}\")\n",
    "print(\"Scores after dividing by the scaling factor.\")\n",
    "print(scaled_scores)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Part D: Softmax Normalization ---\n",
    "# We define a helper function for numerical stability, though it's less critical here.\n",
    "def softmax(x, axis=-1):\n",
    "    # Subtracting the max for numerical stability (prevents overflow)\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "\n",
    "print(\"--- Part D: Attention Weights (after Softmax) ---\")\n",
    "print(f\"Shape of attention_weights: {attention_weights.shape}\")\n",
    "print(\"Each row now represents a probability distribution and sums to 1.\")\n",
    "print(\"This matrix A tells us 'where to look'.\")\n",
    "print(attention_weights)\n",
    "print(f\"Sum of the first row of weights: {attention_weights[0].sum():.4f}\") # Should be 1.0\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Part E: Weighted Sum of Values ---\n",
    "# This is the final step where information is aggregated.\n",
    "O = attention_weights @ V\n",
    "\n",
    "print(\"--- Part E: Final Output Matrix O (A @ V) ---\")\n",
    "print(f\"Shape of O: {O.shape}\")\n",
    "print(\"This is the final contextualized output. Each row is a weighted sum of all value vectors.\")\n",
    "print(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba4cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multi-Head Attention Hyperparameters ---\n",
      "Number of Heads (h): 3\n",
      "Model Dimension (d_model): 12\n",
      "Key/Query/Value Dimension per Head (d_k, d_v): 4\n",
      "\n",
      "--- New Input Matrix X (Shape: (4, 12)) ---\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975]\n",
      " [ 0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733\n",
      "  -0.90802408 -1.4123037   1.46564877 -0.2257763   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818 -0.01349722 -1.05771093  0.82254491 -1.22084365]\n",
      " [ 0.2088636  -1.95967012 -1.32818605  0.19686124  0.73846658  0.17136828\n",
      "  -0.11564828 -0.3011037  -1.47852199 -0.71984421 -0.46063877  1.05712223]]\n",
      "\n",
      "\n",
      "--- Part A & B: Created projection matrices for each of the 3 heads ---\n",
      "\n",
      "--- Part C: Calculating attention for each head in parallel ---\n",
      "\n",
      "--- Processing Head 1 ---\n",
      "Shape of Q_1, K_1, V_1: (4, 4), (4, 4), (4, 4)\n",
      "Shape of Head 1 output: (4, 4)\n",
      "\n",
      "--- Processing Head 2 ---\n",
      "Shape of Q_2, K_2, V_2: (4, 4), (4, 4), (4, 4)\n",
      "Shape of Head 2 output: (4, 4)\n",
      "\n",
      "--- Processing Head 3 ---\n",
      "Shape of Q_3, K_3, V_3: (4, 4), (4, 4), (4, 4)\n",
      "Shape of Head 3 output: (4, 4)\n",
      "\n",
      "--- Part D: Concatenation ---\n",
      "Shape of outputs from Head 1, 2, 3: [(4, 4), (4, 4), (4, 4)]\n",
      "Shape of concatenated matrix: (4, 12)\n",
      "This matrix contains the combined knowledge from all heads.\n",
      "\n",
      "--- Part E: Final Multi-Head Attention Output ---\n",
      "Shape of W_O: (12, 12)\n",
      "Shape of final output: (4, 12)\n",
      "This is the final, context-aware output matrix, ready for the next layer.\n",
      "[[  7.79295186  -7.07470145  -3.84667417   1.55489513  18.4086031\n",
      "   -3.63904855  -1.11443661 -13.51741509  29.46540535  -5.82572674\n",
      "   18.16651831  34.11530273]\n",
      " [ -2.66571192  -2.91330272  -0.6897321    3.76861955 -12.59583822\n",
      "    5.71016448  -6.05645706  -2.18746286  -2.43917674  -2.01599365\n",
      "   -1.05241602  -9.75982204]\n",
      " [ -4.69018606   6.65995078  -0.31706689   8.94147911   8.91977465\n",
      "   10.51255541   6.99510807 -11.01997823  13.48441576  -7.56145086\n",
      "   -7.37464875  -5.08689748]\n",
      " [  8.4340013   -6.87925795   3.3574754   -7.19760087  -4.22422953\n",
      "   -0.54703288 -13.21881224  -4.27016997   1.56102634  15.01944641\n",
      "    6.81842512  11.39998488]]\n"
     ]
    }
   ],
   "source": [
    "# --- Setup for Multi-Head Attention ---\n",
    "# We use the same S and d_model from Step 1.\n",
    "h = 3          # Number of attention heads\n",
    "# d_k and d_v must be divisible by h\n",
    "# We ensure d_model is divisible by h. Let's redefine it for this example.\n",
    "d_model = 12\n",
    "h = 3\n",
    "d_k = d_v = d_model // h # d_k = d_v = 4\n",
    "\n",
    "print(f\"--- Multi-Head Attention Hyperparameters ---\")\n",
    "print(f\"Number of Heads (h): {h}\")\n",
    "print(f\"Model Dimension (d_model): {d_model}\")\n",
    "print(f\"Key/Query/Value Dimension per Head (d_k, d_v): {d_k}\\n\")\n",
    "\n",
    "# Recreate input X with the new d_model\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(S, d_model)\n",
    "print(f\"--- New Input Matrix X (Shape: {X.shape}) ---\")\n",
    "print(X)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Part A & B: Parallel Linear Projections ---\n",
    "# We now need h sets of weight matrices.\n",
    "# In a real implementation, this would be one large weight matrix\n",
    "# that is then reshaped, but we'll create them separately for clarity.\n",
    "\n",
    "W_Q_heads = []\n",
    "W_K_heads = []\n",
    "W_V_heads = []\n",
    "heads_output = []\n",
    "\n",
    "np.random.seed(1) # Seed for head weights\n",
    "for i in range(h):\n",
    "    W_Q_heads.append(np.random.randn(d_model, d_k))\n",
    "    W_K_heads.append(np.random.randn(d_model, d_k))\n",
    "    W_V_heads.append(np.random.randn(d_model, d_v))\n",
    "\n",
    "print(\"--- Part A & B: Created projection matrices for each of the 3 heads ---\\n\")\n",
    "\n",
    "# --- Part C: Parallel Attention Calculation ---\n",
    "print(\"--- Part C: Calculating attention for each head in parallel ---\\n\")\n",
    "for i in range(h):\n",
    "    print(f\"--- Processing Head {i+1} ---\")\n",
    "    # Project X for the current head\n",
    "    Q_i = X @ W_Q_heads[i]\n",
    "    K_i = X @ W_K_heads[i]\n",
    "    V_i = X @ W_V_heads[i]\n",
    "    print(f\"Shape of Q_{i+1}, K_{i+1}, V_{i+1}: {Q_i.shape}, {K_i.shape}, {V_i.shape}\")\n",
    "\n",
    "    # Calculate attention scores for the current head\n",
    "    scores_i = Q_i @ K_i.T\n",
    "    scaled_scores_i = scores_i / np.sqrt(d_k)\n",
    "    attention_weights_i = softmax(scaled_scores_i)\n",
    "\n",
    "    # Calculate the output of the current head\n",
    "    head_i_output = attention_weights_i @ V_i\n",
    "    print(f\"Shape of Head {i+1} output: {head_i_output.shape}\\n\")\n",
    "    heads_output.append(head_i_output)\n",
    "\n",
    "# --- Part D: Concatenation ---\n",
    "# Concatenate the outputs of all heads along the last dimension\n",
    "concatenated_heads = np.concatenate(heads_output, axis=-1)\n",
    "\n",
    "print(\"--- Part D: Concatenation ---\")\n",
    "print(\"Shape of outputs from Head 1, 2, 3:\", [h.shape for h in heads_output])\n",
    "print(f\"Shape of concatenated matrix: {concatenated_heads.shape}\")\n",
    "print(\"This matrix contains the combined knowledge from all heads.\\n\")\n",
    "\n",
    "# --- Part E: Final Linear Projection ---\n",
    "# Create the final output projection matrix W_O\n",
    "# It maps the concatenated dimension back to d_model\n",
    "np.random.seed(2)\n",
    "W_O = np.random.randn(h * d_v, d_model)\n",
    "\n",
    "# Compute the final multi-head attention output\n",
    "multi_head_output = concatenated_heads @ W_O\n",
    "\n",
    "print(\"--- Part E: Final Multi-Head Attention Output ---\")\n",
    "print(f\"Shape of W_O: {W_O.shape}\")\n",
    "print(f\"Shape of final output: {multi_head_output.shape}\")\n",
    "print(\"This is the final, context-aware output matrix, ready for the next layer.\")\n",
    "print(multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8df1aa",
   "metadata": {},
   "source": [
    "### Implementation of Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the Scaled Dot-Product Attention as described in \"Attention is All You Need\".\n",
    "    This module does not have any learnable parameters itself, but it's the core\n",
    "    computation block for the Multi-Head Attention layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the ScaledDotProductAttention module.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate (float): The dropout probability to be applied to the attention\n",
    "                                  weights. Default is 0.1.\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. super(ScaledDotProductAttention, self).__init__()\n",
    "        #    This is a standard and necessary line in the constructor of any PyTorch module.\n",
    "        #    It calls the constructor of the parent class, nn.Module, which sets up\n",
    "        #    the necessary internal state for the module (like tracking parameters and sub-modules).\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        # 2. self.dropout = nn.Dropout(dropout_rate)\n",
    "        #    Here, we initialize a Dropout layer. Dropout is a regularization technique\n",
    "        #    to prevent overfitting. During training, it randomly sets a fraction of\n",
    "        #    its input elements to zero with probability `dropout_rate`. This is applied\n",
    "        #    to the attention weights before they are multiplied by the Value matrix.\n",
    "        #    `nn.Dropout` is a module, so we instantiate it here.\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The query tensor. Shape: [batch_size, n_heads, seq_len_q, d_k]\n",
    "            key (torch.Tensor): The key tensor. Shape: [batch_size, n_heads, seq_len_k, d_k]\n",
    "            value (torch.Tensor): The value tensor. Shape: [batch_size, n_heads, seq_len_v, d_v]\n",
    "                                  Note: seq_len_k and seq_len_v must be the same.\n",
    "            mask (torch.Tensor, optional): A mask tensor to prevent attention to certain positions.\n",
    "                                           Shape should be broadcastable to the scores tensor's shape.\n",
    "                                           Example shape: [batch_size, 1, 1, seq_len_k] for padding mask.\n",
    "                                           Example shape: [batch_size, 1, seq_len_q, seq_len_k] for look-ahead mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the attention mechanism. Shape: [batch_size, n_heads, seq_len_q, d_v]\n",
    "            torch.Tensor: The attention weights. Shape: [batch_size, n_heads, seq_len_q, seq_len_k]\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. d_k = query.size(-1)\n",
    "        #    We retrieve the dimensionality of the key/query vectors, d_k.\n",
    "        #    `query.size()` returns a tuple of the tensor's dimensions.\n",
    "        #    Using -1 as an index retrieves the size of the *last* dimension, which is d_k.\n",
    "        #    This is robust and works regardless of the number of preceding dimensions (batch, heads).\n",
    "        d_k = query.size(-1)\n",
    "\n",
    "        # 2. scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        #    This computes the dot product compatibility scores: Q * K^T.\n",
    "        #    `torch.matmul` performs matrix multiplication.\n",
    "        #    `key.transpose(-2, -1)` is crucial. It transposes the last two dimensions of the key tensor.\n",
    "        #    For a key tensor of shape [batch, n_heads, seq_len_k, d_k], this transpose\n",
    "        #    swaps the `seq_len_k` and `d_k` dimensions, resulting in a shape of\n",
    "        #    [batch, n_heads, d_k, seq_len_k].\n",
    "        #    Now, the matrix multiplication between `query` ([..., seq_len_q, d_k]) and the transposed `key`\n",
    "        #    ([..., d_k, seq_len_k]) is mathematically valid and produces a `scores` tensor of shape\n",
    "        #    [batch_size, n_heads, seq_len_q, seq_len_k].\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # 3. scores = scores / math.sqrt(d_k)\n",
    "        #    Here, we apply the scaling factor as per the formula. We divide all the scores\n",
    "        #    by the square root of d_k to stabilize the gradients.\n",
    "        scores = scores / math.sqrt(d_k)\n",
    "\n",
    "        # 4. if mask is not None:\n",
    "        #    We check if a mask was provided. The mask is used to hide certain positions\n",
    "        #    from the attention mechanism (e.g., padding tokens or future tokens in a decoder).\n",
    "        if mask is not None:\n",
    "            # 5. scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            #    `masked_fill` is a PyTorch operation that fills elements of a tensor with a\n",
    "            #    specified value where a condition (the mask) is True.\n",
    "            #    Our convention is that the mask has a value of 1 for positions we want to\n",
    "            #    keep and 0 for positions we want to mask out.\n",
    "            #    `mask == 0` creates a boolean tensor that is `True` for all positions to be masked.\n",
    "            #    We fill these positions in the `scores` tensor with a very large negative number\n",
    "            #    (-1e9, which is -1,000,000,000). This is a practical stand-in for negative infinity.\n",
    "            #    When softmax is applied next, e^(-1e9) will be effectively zero.\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 6. p_attn = F.softmax(scores, dim=-1)\n",
    "        #    We apply the softmax function to the scores to get the attention weights.\n",
    "        #    `F.softmax` is the functional version of the softmax layer.\n",
    "        #    `dim=-1` is critical. It specifies that the softmax operation should be\n",
    "        #    applied along the last dimension of the `scores` tensor (the `seq_len_k` dimension).\n",
    "        #    This normalizes the weights for each query independently, ensuring that for a given\n",
    "        #    query, the sum of its attention weights across all keys is 1.\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 7. p_attn = self.dropout(p_attn)\n",
    "        #    We apply the dropout layer we initialized earlier to the attention weights.\n",
    "        #    This helps in regularizing the model. During evaluation (`model.eval()`),\n",
    "        #    dropout is automatically disabled.\n",
    "        p_attn = self.dropout(p_attn)\n",
    "\n",
    "        # 8. return torch.matmul(p_attn, value), p_attn\n",
    "        #    Finally, we compute the weighted sum of the values by multiplying the attention\n",
    "        #    weights `p_attn` ([..., seq_len_q, seq_len_k]) with the `value` tensor\n",
    "        #    ([..., seq_len_v, d_v], where seq_len_k == seq_len_v).\n",
    "        #    The result is a tensor of shape [batch_size, n_heads, seq_len_q, d_v].\n",
    "        #    We also return the attention weights `p_attn`. This is very useful for\n",
    "        #    debugging and for visualizing what the model is \"paying attention to\".\n",
    "        output = torch.matmul(p_attn, value)\n",
    "\n",
    "        return output, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932d1253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input Tensor Shapes ---\n",
      "Query (Q) shape: torch.Size([2, 8, 10, 64])\n",
      "Key (K)   shape: torch.Size([2, 8, 10, 64])\n",
      "Value (V) shape: torch.Size([2, 8, 10, 64])\n",
      "------------------------------\n",
      "--- Mask Tensor ---\n",
      "Mask shape: torch.Size([2, 1, 1, 10])\n",
      "Mask content for the second sequence:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 0., 0., 0.]]])\n",
      "------------------------------\n",
      "--- Output Tensor Shapes ---\n",
      "Output shape: torch.Size([2, 8, 10, 64])\n",
      "Attention Weights shape: torch.Size([2, 8, 10, 10])\n",
      "------------------------------\n",
      "--- Verifying the Mask ---\n",
      "Attention weights for the first query of the second sequence (sum should be 1):\n",
      "tensor([0.0892, 0.2347, 0.1509, 0.2802, 0.1258, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Sum of these weights: 0.8808539509773254\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for our test run ---\n",
    "batch_size = 2   # How many sequences we process at once\n",
    "n_heads = 8      # Number of attention heads (we'll see this in the next step, for now it's just a dimension)\n",
    "seq_len = 10     # Length of our input sequence\n",
    "d_k = 64         # Dimension of Key/Query vectors (d_model / n_heads)\n",
    "d_v = 64         # Dimension of Value vectors (d_model / n_heads)\n",
    "\n",
    "# --- Create Dummy Tensors ---\n",
    "# In a real Transformer, these would be the outputs of linear projection layers.\n",
    "# torch.randn creates a tensor with random numbers from a standard normal distribution.\n",
    "query = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "key = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "value = torch.randn(batch_size, n_heads, seq_len, d_v)\n",
    "\n",
    "print(\"--- Input Tensor Shapes ---\")\n",
    "print(f\"Query (Q) shape: {query.shape}\")\n",
    "print(f\"Key (K)   shape: {key.shape}\")\n",
    "print(f\"Value (V) shape: {value.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Demonstrate with a Padding Mask ---\n",
    "# Let's pretend in our batch of 2 sequences, the first sequence has length 10\n",
    "# and the second sequence has length 7 (and 3 padding tokens).\n",
    "# The mask should be 1 for real tokens and 0 for padding tokens.\n",
    "# Shape: [batch_size, 1, 1, seq_len] so it can be broadcast across heads and query length.\n",
    "mask = torch.ones(batch_size, 1, 1, seq_len)\n",
    "mask[1, :, :, 7:] = 0  # Set the last 3 tokens of the second sequence in the batch to 0 (masked)\n",
    "\n",
    "print(\"--- Mask Tensor ---\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Mask content for the second sequence:\\n{mask[1]}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Instantiate and run the attention module ---\n",
    "attention_module = ScaledDotProductAttention(dropout_rate=0.1)\n",
    "output, attention_weights = attention_module(query, key, value, mask=mask)\n",
    "\n",
    "\n",
    "# --- Examine the Output ---\n",
    "print(\"--- Output Tensor Shapes ---\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention Weights shape: {attention_weights.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Let's verify the mask worked. The attention weights for the second sequence's\n",
    "# queries (rows) should be zero for the last 3 keys (columns).\n",
    "print(\"--- Verifying the Mask ---\")\n",
    "print(\"Attention weights for the first query of the second sequence (sum should be 1):\")\n",
    "print(attention_weights[1, 0, 0, :])\n",
    "# The sum of weights for any given query should be 1.0\n",
    "print(f\"Sum of these weights: {torch.sum(attention_weights[1, 0, 0, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479d7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "# We assume the ScaledDotProductAttention class from Step 1 is available\n",
    "# from step1_scaled_dot_product_attention import ScaledDotProductAttention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism as described in \"Attention is All You Need\".\n",
    "    This module contains the learnable linear projections for Q, K, V, and the final output.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output vectors.\n",
    "            n_heads (int): The number of attention heads.\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. Standard nn.Module constructor call.\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # 2. assert d_model % n_heads == 0\n",
    "        #    This is a critical sanity check. The model's architecture requires that\n",
    "        #    the embedding dimension `d_model` can be evenly split among the attention heads.\n",
    "        #    If not, the logic for reshaping and splitting dimensions will fail.\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        # 3. Store hyperparameters.\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        # 4. self.d_k = d_model // n_heads\n",
    "        #    Calculate the dimension of the key/query/value vectors for each head.\n",
    "        #    For example, if d_model=512 and n_heads=8, then each head works with\n",
    "        #    smaller 64-dimensional vectors.\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # 5. self.W_q = nn.Linear(d_model, d_model)\n",
    "        #    Initialize the four learnable linear layers. `nn.Linear(in_features, out_features)`\n",
    "        #    creates a fully connected layer that performs a matrix multiplication (W*x + b).\n",
    "        #    - W_q: Projects the input query into a representation for all heads.\n",
    "        #    - W_k: Projects the input key into a representation for all heads.\n",
    "        #    - W_v: Projects the input value into a representation for all heads.\n",
    "        #    - W_o: The final output projection layer that combines the heads' outputs.\n",
    "        #    We use `d_model` as both input and output size for the first three layers\n",
    "        #    because we will handle the splitting into heads via reshaping.\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 6. self.attention = ScaledDotProductAttention(dropout_rate)\n",
    "        #    Instantiate the attention mechanism module we built in Step 1. This module\n",
    "        #    will perform the core computation for all heads in parallel.\n",
    "        self.attention = ScaledDotProductAttention(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Multi-Head Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The query tensor. Shape: [batch_size, seq_len_q, d_model]\n",
    "            key (torch.Tensor): The key tensor. Shape: [batch_size, seq_len_k, d_model]\n",
    "            value (torch.Tensor): The value tensor. Shape: [batch_size, seq_len_v, d_model]\n",
    "            mask (torch.Tensor, optional): A mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the MHA mechanism. Shape: [batch_size, seq_len_q, d_model]\n",
    "            torch.Tensor: The attention weights. Shape: [batch_size, n_heads, seq_len_q, seq_len_k]\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. batch_size = query.size(0)\n",
    "        #    Get the batch size from the first dimension of the query tensor. This will be\n",
    "        #    needed for reshaping the tensors later.\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 2. q_proj = self.W_q(query)\n",
    "        #    Perform the initial linear projections for Q, K, and V.\n",
    "        #    Input query shape:  [batch_size, seq_len_q, d_model]\n",
    "        #    Output q_proj shape: [batch_size, seq_len_q, d_model]\n",
    "        q_proj = self.W_q(query)\n",
    "        k_proj = self.W_k(key)\n",
    "        v_proj = self.W_v(value)\n",
    "\n",
    "        # 3. q_split = q_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        #    *** This is the crucial \"split into separated heads\" step. ***\n",
    "        #    a) .view(batch_size, -1, self.n_heads, self.d_k): Reshapes the tensor.\n",
    "        #       - `batch_size` remains the same.\n",
    "        #       - `-1` tells PyTorch to automatically infer the sequence length (seq_len_q).\n",
    "        #       - The `d_model` dimension is split into `n_heads` and `d_k`.\n",
    "        #       - Shape becomes: [batch_size, seq_len_q, n_heads, d_k]\n",
    "        #    b) .transpose(1, 2): Swaps the second and third dimensions.\n",
    "        #       - This brings the `n_heads` dimension forward.\n",
    "        #       - Final shape: [batch_size, n_heads, seq_len_q, d_k]\n",
    "        #    Now the tensor is ready for our ScaledDotProductAttention module. Each of the `n_heads`\n",
    "        #    \"slices\" along the second dimension is an independent query matrix for one head.\n",
    "        q_split = q_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_split = k_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_split = v_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 4. if mask is not None: ...\n",
    "        #    The mask needs to be properly shaped to be broadcastable to the attention scores.\n",
    "        #    The scores will have shape [batch_size, n_heads, seq_len_q, seq_len_k].\n",
    "        #    A typical padding mask might have shape [batch_size, 1, 1, seq_len_k].\n",
    "        #    `unsqueeze(1)` adds a new dimension for the `n_heads`, allowing the same\n",
    "        #    mask to be applied to all heads.\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 5. context, attention_weights = self.attention(q_split, k_split, v_split, mask=mask)\n",
    "        #    Apply Scaled Dot-Product Attention.\n",
    "        #    - `context` (the output) will have shape: [batch_size, n_heads, seq_len_q, d_k]\n",
    "        #    - `attention_weights` will have shape: [batch_size, n_heads, seq_len_q, seq_len_k]\n",
    "        context, attention_weights = self.attention(q_split, k_split, v_split, mask=mask)\n",
    "\n",
    "        # 6. context_concat = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        #    *** This is the \"concatenate the heads\" step. ***\n",
    "        #    a) .transpose(1, 2): Reverses the earlier transpose, swapping `n_heads` and `seq_len_q`.\n",
    "        #       - Shape becomes: [batch_size, seq_len_q, n_heads, d_k]\n",
    "        #    b) .contiguous(): This is an important memory layout operation. A `transpose`\n",
    "        #       operation can make a tensor non-contiguous in memory. `view` requires a\n",
    "        #       contiguous tensor, so we call `.contiguous()` to create a contiguous copy.\n",
    "        #    c) .view(batch_size, -1, self.d_model): Reshapes the tensor back.\n",
    "        #       - It merges the `n_heads` and `d_k` dimensions back into the single `d_model` dimension.\n",
    "        #       - This effectively concatenates the outputs of all heads.\n",
    "        #       - Final shape: [batch_size, seq_len_q, d_model]\n",
    "        context_concat = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 7. output = self.W_o(context_concat)\n",
    "        #    Apply the final linear projection layer to mix the information from all heads.\n",
    "        #    The shape remains [batch_size, seq_len_q, d_model].\n",
    "        output = self.W_o(context_concat)\n",
    "\n",
    "        # 8. Return the final output and the attention weights for analysis.\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc4d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MHA Information Flow ---\n",
      "1. Initial Input (X) shape: \t\ttorch.Size([2, 5, 512])\n",
      "2. After Linear Projection (e.g., Q_proj) shape: torch.Size([2, 5, 512])\n",
      "3. After Splitting into Heads (e.g., Q_split) shape: torch.Size([2, 8, 5, 64])\n",
      "4. After Scaled Dot-Product Attention (context) shape: torch.Size([2, 8, 5, 64])\n",
      "5. After Concatenating Heads (context_concat) shape: torch.Size([2, 5, 512])\n",
      "6. After Final Linear Projection (Output) shape: torch.Size([2, 5, 512])\n",
      "------------------------------\n",
      "--- Final Output Shapes ---\n",
      "Final MHA Output shape: torch.Size([2, 5, 512])\n",
      "Attention Weights shape: torch.Size([2, 8, 5, 5])\n",
      "\n",
      "--- Verifying Mask on Final Attention Weights ---\n",
      "Attention weights for the 1st query of the 2nd sequence in the batch (head 0):\n",
      "tensor([0.5158, 0.3034, 0.2919, 0.0000, 0.0000], grad_fn=<SliceBackward0>)\n",
      "Sum of these weights: 1.1111112833023071\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for our test run ---\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "\n",
    "# --- Create Dummy Tensors for a self-attention scenario ---\n",
    "# In self-attention, Q, K, and V are all the same. This tensor represents\n",
    "# the output from the previous layer (e.g., the input embedding layer).\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# The mask will be for padding. Let's say the first sequence has length 5,\n",
    "# and the second has length 3 (2 padding tokens).\n",
    "# Shape: [batch_size, 1, seq_len]\n",
    "mask = torch.ones(batch_size, 1, seq_len)\n",
    "mask[1, :, 3:] = 0 # Mask out the last 2 tokens of the second sequence\n",
    "\n",
    "# --- Instantiate and run the MHA module ---\n",
    "mha_module = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "output, attention_weights = mha_module(query=x, key=x, value=x, mask=mask)\n",
    "\n",
    "\n",
    "# --- Examine the Information Flow via Tensor Shapes ---\n",
    "print(\"--- MHA Information Flow ---\")\n",
    "print(f\"1. Initial Input (X) shape: \\t\\t{x.shape}\")\n",
    "\n",
    "# Inside the forward pass...\n",
    "q_proj = mha_module.W_q(x)\n",
    "print(f\"2. After Linear Projection (e.g., Q_proj) shape: {q_proj.shape}\")\n",
    "\n",
    "q_split = q_proj.view(batch_size, -1, n_heads, d_model // n_heads).transpose(1, 2)\n",
    "print(f\"3. After Splitting into Heads (e.g., Q_split) shape: {q_split.shape}\")\n",
    "\n",
    "context, _ = mha_module.attention(q_split, q_split, q_split, mask.unsqueeze(1))\n",
    "print(f\"4. After Scaled Dot-Product Attention (context) shape: {context.shape}\")\n",
    "\n",
    "context_concat = context.transpose(1, 2).contiguous().view(batch_size, -1, d_model)\n",
    "print(f\"5. After Concatenating Heads (context_concat) shape: {context_concat.shape}\")\n",
    "\n",
    "final_output = mha_module.W_o(context_concat)\n",
    "print(f\"6. After Final Linear Projection (Output) shape: {final_output.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"--- Final Output Shapes ---\")\n",
    "print(f\"Final MHA Output shape: {output.shape}\")\n",
    "print(f\"Attention Weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Let's check the mask again on the final attention weights.\n",
    "# The weights for the second sequence should be zero for the 4th and 5th key positions.\n",
    "print(\"\\n--- Verifying Mask on Final Attention Weights ---\")\n",
    "print(\"Attention weights for the 1st query of the 2nd sequence in the batch (head 0):\")\n",
    "print(attention_weights[1, 0, 0, :])\n",
    "print(f\"Sum of these weights: {torch.sum(attention_weights[1, 0, 0, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766f9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Position-wise Feed-Forward Network (FFN) sub-layer of the Transformer.\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the PositionwiseFeedForward module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output vectors.\n",
    "            d_ff (int): The dimensionality of the inner-layer (the \"feed-forward\" dimension).\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. Standard nn.Module constructor call.\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # 2. self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        #    This is the first linear layer. It takes an input of size `d_model`\n",
    "        #    and projects it to the larger, inner dimension `d_ff`.\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "\n",
    "        # 3. self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        #    This is the second linear layer. It takes the `d_ff`-dimensional output\n",
    "        #    from the first layer and projects it back down to the original `d_model` size.\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # 4. self.relu = nn.ReLU()\n",
    "        #    We instantiate the ReLU activation function. This will be applied element-wise\n",
    "        #    to the output of the first linear layer. It introduces non-linearity by\n",
    "        #    clamping all negative values to zero: f(x) = max(0, x).\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 5. self.dropout = nn.Dropout(dropout_rate)\n",
    "        #    Initialize a Dropout layer. It is typically applied after the activation function\n",
    "        #    to regularize the network and prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the FFN.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor from the previous sub-layer.\n",
    "                              Shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor. Shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # The input `x` has shape [batch_size, seq_len, d_model].\n",
    "        # Because we use `nn.Linear`, PyTorch automatically applies the same linear\n",
    "        # transformation to every vector along the `seq_len` dimension, which is exactly\n",
    "        # what \"position-wise\" means.\n",
    "\n",
    "        # 1. x = self.linear1(x)\n",
    "        #    The input `x` is passed through the first linear layer.\n",
    "        #    Shape changes from [batch_size, seq_len, d_model] to [batch_size, seq_len, d_ff].\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        # 2. x = self.relu(x)\n",
    "        #    The ReLU activation is applied element-wise. The shape remains the same.\n",
    "        #    Shape: [batch_size, seq_len, d_ff].\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 3. x = self.dropout(x)\n",
    "        #    Dropout is applied to the activations. Shape remains the same.\n",
    "        #    Shape: [batch_size, seq_len, d_ff].\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4. x = self.linear2(x)\n",
    "        #    The result is passed through the second linear layer, projecting it back.\n",
    "        #    Shape changes from [batch_size, seq_len, d_ff] to [batch_size, seq_len, d_model].\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # 5. return x\n",
    "        #    Return the final processed tensor.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df631235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FFN Information Flow ---\n",
      "1. Initial Input shape: \t\ttorch.Size([2, 5, 512])\n",
      "2. After 1st Linear Layer (Expansion): \ttorch.Size([2, 5, 2048])\n",
      "3. After ReLU Activation: \t\ttorch.Size([2, 5, 2048])\n",
      "4. After 2nd Linear Layer (Projection): torch.Size([2, 5, 512])\n",
      "------------------------------\n",
      "--- Final Output Shape ---\n",
      "Final FFN Output shape: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for our test run ---\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "d_ff = 2048  # Standard practice: d_ff is 4 * d_model\n",
    "\n",
    "# --- Create a Dummy Input Tensor ---\n",
    "# This simulates the output from the previous sub-layer.\n",
    "# Shape: [batch_size, seq_len, d_model]\n",
    "input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# --- Instantiate and run the FFN module ---\n",
    "ffn_module = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "output_tensor = ffn_module(input_tensor)\n",
    "\n",
    "\n",
    "# --- Examine the Information Flow via Tensor Shapes ---\n",
    "print(\"--- FFN Information Flow ---\")\n",
    "print(f\"1. Initial Input shape: \\t\\t{input_tensor.shape}\")\n",
    "\n",
    "# Let's trace the shapes inside the forward pass manually to see the flow clearly\n",
    "intermediate_tensor = ffn_module.linear1(input_tensor)\n",
    "print(f\"2. After 1st Linear Layer (Expansion): \\t{intermediate_tensor.shape}\")\n",
    "\n",
    "intermediate_tensor = ffn_module.relu(intermediate_tensor)\n",
    "print(f\"3. After ReLU Activation: \\t\\t{intermediate_tensor.shape}\")\n",
    "\n",
    "# Dropout is applied here but doesn't change the shape.\n",
    "\n",
    "final_tensor = ffn_module.linear2(intermediate_tensor)\n",
    "print(f\"4. After 2nd Linear Layer (Projection): {final_tensor.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"--- Final Output Shape ---\")\n",
    "print(f\"Final FFN Output shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "269b88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# We assume the classes from previous steps are available\n",
    "# from step2_multi_head_attention import MultiHeadAttention\n",
    "# from step3_positionwise_feed_forward import PositionwiseFeedForward\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Encoder Layer of the Transformer.\n",
    "    It consists of a Multi-Head Self-Attention sub-layer and a Position-wise\n",
    "    Feed-Forward Network sub-layer. Each sub-layer is followed by a\n",
    "    residual connection and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the EncoderLayer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output vectors.\n",
    "            n_heads (int): The number of attention heads.\n",
    "            d_ff (int): The dimensionality of the inner-layer of the FFN.\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # 1. Standard nn.Module constructor call.\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 2. self.self_attention = MultiHeadAttention(...)\n",
    "        #    Instantiate the Multi-Head Attention module we built in Step 2. This will\n",
    "        #    be our first sub-layer.\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
    "\n",
    "        # 3. self.feed_forward = PositionwiseFeedForward(...)\n",
    "        #    Instantiate the Position-wise Feed-Forward Network module from Step 3. This\n",
    "        #    will be our second sub-layer.\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
    "\n",
    "        # 4. self.norm1 = nn.LayerNorm(d_model)\n",
    "        #    Initialize the two Layer Normalization modules. `nn.LayerNorm` takes the size\n",
    "        #    of the dimension to normalize over, which is `d_model`. PyTorch handles the\n",
    "        #    calculation of mean/std and the learnable gamma/beta parameters internally.\n",
    "        #    - `norm1` is for the output of the attention sub-layer.\n",
    "        #    - `norm2` is for the output of the FFN sub-layer.\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 5. self.dropout = nn.Dropout(dropout_rate)\n",
    "        #    We use dropout on the output of each sub-layer *before* the residual connection.\n",
    "        #    This is a common implementation choice following the original paper's design.\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the EncoderLayer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor from the previous layer.\n",
    "                              Shape: [batch_size, seq_len, d_model]\n",
    "            mask (torch.Tensor): The mask for the self-attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor. Shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # ----------------- Line-by-Line Explanation -----------------\n",
    "        # ----- Sub-layer 1: Multi-Head Self-Attention -----\n",
    "\n",
    "        # 1. residual = x\n",
    "        #    Store the original input `x` for the first residual connection.\n",
    "        residual = x\n",
    "\n",
    "        # 2. x, _ = self.self_attention(query=x, key=x, value=x, mask=mask)\n",
    "        #    Pass the input `x` through the self-attention layer. For self-attention,\n",
    "        #    the query, key, and value are all the same tensor. We also pass the mask.\n",
    "        #    The MHA module returns the output and the attention weights. We only need\n",
    "        #    the output for the forward pass, so we can ignore the weights with `_`.\n",
    "        #    Output `x` shape: [batch_size, seq_len, d_model]\n",
    "        x, _ = self.self_attention(query=x, key=x, value=x, mask=mask)\n",
    "\n",
    "        # 3. x = self.dropout(x)\n",
    "        #    Apply dropout to the output of the attention layer.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4. x = self.norm1(x + residual)\n",
    "        #    This is the \"Add & Norm\" step.\n",
    "        #    a) `x + residual`: The residual connection is performed. The output of the\n",
    "        #       sub-layer is added to its original input.\n",
    "        #    b) `self.norm1(...)`: The result is passed through the first layer normalization module.\n",
    "        attention_output = x\n",
    "\n",
    "        # ----- Sub-layer 2: Position-wise Feed-Forward Network -----\n",
    "\n",
    "        # 5. residual = x\n",
    "        #    Store the output from the first sub-layer (`attention_output`) for the\n",
    "        #    second residual connection.\n",
    "        residual = attention_output\n",
    "\n",
    "        # 6. x = self.feed_forward(x)\n",
    "        #    Pass the tensor through the FFN.\n",
    "        #    Output `x` shape: [batch_size, seq_len, d_model]\n",
    "        x = self.feed_forward(attention_output)\n",
    "\n",
    "        # 7. x = self.dropout(x)\n",
    "        #    Apply dropout to the output of the FFN.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 8. x = self.norm2(x + residual)\n",
    "        #    Perform the second \"Add & Norm\" step.\n",
    "        #    a) `x + residual`: Add the FFN output to its input.\n",
    "        #    b) `self.norm2(...)`: Normalize the result.\n",
    "        ffn_output = x\n",
    "\n",
    "        # 9. return x\n",
    "        #    Return the final output of the encoder layer.\n",
    "        return ffn_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
