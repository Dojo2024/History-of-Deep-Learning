{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb81770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1176d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy language models loaded successfully.\n",
      "Loaded 29000 training examples.\n",
      "Sample training pair:\n",
      "  German (source): Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "  English (target): Two young, White males are outside near many bushes.\n",
      "\n",
      "Tokenized sample:\n",
      "  German tokens (reversed): ['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n",
      "  English tokens: ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy language models loaded successfully.\")\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it. Reversing the source sequence is a common trick that was found to improve performance\n",
    "    in early sequence-to-sequence models\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens).\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "DATASET_PATH = r\"C:\\Users\\Debojyoti Das\\.cache\\huggingface\\hub\\datasets--bentrevett--multi30k\\snapshots\\4589883f3d09d4ef6361784e03f0ead219836469\"\n",
    "\n",
    "def load_data(path, file_prefix):\n",
    "    \"\"\"\n",
    "    Loads data from a .jsonl file and return a list of (source, target) tuples.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(path, f\"{file_prefix}.jsonl\")\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            data.append((entry['de'], entry['en']))\n",
    "        return data\n",
    "\n",
    "\n",
    "train_data = load_data(DATASET_PATH, \"train\")\n",
    "\n",
    "# Display a sample to verify\n",
    "print(f\"Loaded {len(train_data)} training examples.\")\n",
    "print(\"Sample training pair:\")\n",
    "print(f\"  German (source): {train_data[0][0]}\")\n",
    "print(f\"  English (target): {train_data[0][1]}\")\n",
    "\n",
    "\n",
    "# Tokenize the sample\n",
    "de_tokens = tokenize_de(train_data[0][0])\n",
    "en_tokens = tokenize_en(train_data[0][1])\n",
    "print(\"\\nTokenized sample:\")\n",
    "print(f\"  German tokens (reversed): {de_tokens}\")\n",
    "print(f\"  English tokens: {en_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852cb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        \"\"\"\n",
    "        Initializes the Vocabulary.\n",
    "        :param min_freq: The minimum frequency a token must have to be included in the vocabulary.\n",
    "        \"\"\"\n",
    "        # Initialize the vocabulary with special tokens\n",
    "        # itos : Index to string mapping\n",
    "        # stoi : String to index mapping\n",
    "        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary from a list of tokens.\n",
    "        :param sentence_list: A List of sentences(Strings) to build the vocabulary from.\n",
    "        :param tokenizer: The tokenizer function to use (e.g., tokenize_de or tokenize_en).\n",
    "        \"\"\"\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            tokens = tokenizer(sentence)\n",
    "            frequencies.update(tokens)\n",
    "            \n",
    "        # Create the vocabulary based on the frequencies\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "    def numericalize(self, text, tokenizer):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens to their corresponding indices in the vocabulary.\n",
    "        :param text: A string or list of strings to convert.\n",
    "        :param tokenizer: The tokenizer function to use (e.g., tokenize_de or tokenize_en).\n",
    "        :return: A list of indices corresponding to the tokens in the vocabulary.\n",
    "        \"\"\"\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        numericalized_text = []\n",
    "        for token in tokenized_text:\n",
    "            if token in self.stoi:\n",
    "                numericalized_text.append(self.stoi[token])\n",
    "            else:\n",
    "                numericalized_text.append(self.stoi[\"<unk>\"])\n",
    "                \n",
    "        return numericalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81627674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabularies built successfully.\n",
      "Source (German) Vocabulary Size: 8014\n",
      "Target (English) Vocabulary Size: 6191\n",
      "\n",
      "Original German: Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
      "Numericalized German: [22, 29, 11, 46, 47, 48, 49, 50, 51, 12, 52, 29, 53, 32, 54, 16]\n",
      "\n",
      "Original English: A man in green holds a guitar while the other man observes his shirt.\n",
      "Numericalized English: [25, 32, 17, 46, 47, 21, 48, 49, 42, 50, 32, 51, 52, 34, 14]\n"
     ]
    }
   ],
   "source": [
    "source_sentences = [pair[0] for pair in train_data]\n",
    "target_sentences = [pair[1] for pair in train_data]\n",
    "\n",
    "source_vocab = Vocabulary(min_freq=2)\n",
    "target_vocab = Vocabulary(min_freq=2)\n",
    "\n",
    "source_vocab.build_vocabulary(source_sentences, tokenize_de)\n",
    "target_vocab.build_vocabulary(target_sentences, tokenize_en)\n",
    "\n",
    "print(\"\\nVocabularies built successfully.\")\n",
    "print(f\"Source (German) Vocabulary Size: {len(source_vocab)}\")\n",
    "print(f\"Target (English) Vocabulary Size: {len(target_vocab)}\")\n",
    "\n",
    "# --- Test the vocabulary ---\n",
    "sample_german_sentence = train_data[5][0]\n",
    "sample_english_sentence = train_data[5][1]\n",
    "\n",
    "print(f\"\\nOriginal German: {sample_german_sentence}\")\n",
    "numericalized_german = source_vocab.numericalize(sample_german_sentence, tokenize_de)\n",
    "print(f\"Numericalized German: {numericalized_german}\")\n",
    "\n",
    "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
    "numericalized_english = target_vocab.numericalize(sample_english_sentence, tokenize_en)\n",
    "print(f\"Numericalized English: {numericalized_english}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656b0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying a single Dataset item ---\n",
      "Source Tensor: tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "Target Tensor (with <sos> and <eos>): tensor([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  2])\n",
      "Shape of source tensor: torch.Size([13])\n",
      "Shape of target tensor: torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "class Multi30kDataset(Dataset):\n",
    "    def __init__(self, data_path, file_prefix, source_vocab, target_vocab, source_tokenizer, target_tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the custom dataset.\n",
    "        :param data_path: Path to the dataset directory.\n",
    "        :param file_prefix: 'train', 'val', or 'test'.\n",
    "        :param source_vocab: The built source Vocabulary object.\n",
    "        :param target_vocab: The built target Vocabulary object.\n",
    "        :param source_tokenizer: The tokenizer for the source language.\n",
    "        :param target_tokenizer: The tokenizer for the target language.\n",
    "        \"\"\"\n",
    "        # Load the raw text data\n",
    "        self.data = load_data(data_path, file_prefix)\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves one data example, numericalizes it, and adds special tokens.\n",
    "        :param index: The index of the data example to retrieve.\n",
    "        :return: A tuple of (numericalized_source, numericalized_target).\n",
    "        \"\"\"\n",
    "        source_text, target_text = self.data[index]\n",
    "\n",
    "        numericalized_source = self.source_vocab.numericalize(source_text, self.source_tokenizer)\n",
    "        numericalized_target = self.target_vocab.numericalize(target_text, self.target_tokenizer)\n",
    "        \n",
    "\n",
    "        sos_token_idx = self.target_vocab.stoi[\"<sos>\"]\n",
    "        eos_token_idx = self.target_vocab.stoi[\"<eos>\"]\n",
    "        \n",
    "        processed_target = [sos_token_idx] + numericalized_target + [eos_token_idx]\n",
    "\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(processed_target)\n",
    "\n",
    "train_dataset = Multi30kDataset(DATASET_PATH, \"train\", source_vocab, target_vocab, tokenize_de, tokenize_en)\n",
    "val_dataset = Multi30kDataset(DATASET_PATH, \"val\", source_vocab, target_vocab, tokenize_de, tokenize_en)\n",
    "test_dataset = Multi30kDataset(DATASET_PATH, \"test\", source_vocab, target_vocab, tokenize_de, tokenize_en)\n",
    "\n",
    "# Verify one item from the training dataset\n",
    "source_tensor, target_tensor = train_dataset[0]\n",
    "print(\"--- Verifying a single Dataset item ---\")\n",
    "print(f\"Source Tensor: {source_tensor}\")\n",
    "print(f\"Target Tensor (with <sos> and <eos>): {target_tensor}\")\n",
    "print(f\"Shape of source tensor: {source_tensor.shape}\")\n",
    "print(f\"Shape of target tensor: {target_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbb2b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying a single DataLoader batch ---\n",
      "Shape of source batch tensor: torch.Size([22, 32])\n",
      "Shape of target batch tensor: torch.Size([28, 32])\n",
      "\n",
      "Source batch tensor (first 5 tokens of first 3 sentences):\n",
      "tensor([[  17,   22,   22],\n",
      "        [ 363,  232, 2244],\n",
      "        [ 266,  233, 1157],\n",
      "        [  34,   50,  212],\n",
      "        [  35,   12,  240]])\n"
     ]
    }
   ],
   "source": [
    "class PadCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        \"\"\"\n",
    "        Initializes the collation object.\n",
    "        :param pad_idx: The integer index of the <pad> token.\n",
    "        \"\"\"\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        This method is called by the DataLoader to process a batch of data.\n",
    "        :param batch: A list of (source_tensor, target_tensor) tuples.\n",
    "        :return: A tuple of (padded_sources, padded_targets).\n",
    "        \"\"\"\n",
    "        # Separate source and target sequences from the batch\n",
    "        sources = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "\n",
    "        # Use torch's built-in pad_sequence utility\n",
    "        # It pads sequences to the length of the longest sequence in the batch.\n",
    "        # batch_first=False makes the output shape [sequence_length, batch_size],\n",
    "        # which is the expected input format for PyTorch RNNs by default.\n",
    "        padded_sources = torch.nn.utils.rnn.pad_sequence(\n",
    "            sources, batch_first=False, padding_value=self.pad_idx\n",
    "        )\n",
    "        padded_targets = torch.nn.utils.rnn.pad_sequence(\n",
    "            targets, batch_first=False, padding_value=self.pad_idx\n",
    "        )\n",
    "        \n",
    "        return padded_sources, padded_targets\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "pad_idx = source_vocab.stoi[\"<pad>\"] \n",
    "\n",
    "\n",
    "collate_fn = PadCollate(pad_idx=pad_idx)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,         \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\n--- Verifying a single DataLoader batch ---\")\n",
    "\n",
    "source_batch, target_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Shape of source batch tensor: {source_batch.shape}\")\n",
    "print(f\"Shape of target batch tensor: {target_batch.shape}\")\n",
    "print(f\"\\nSource batch tensor (first 5 tokens of first 3 sentences):\")\n",
    "print(source_batch[:5, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835617f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying the Encoder module ---\n",
      "Encoder architecture:\n",
      "Encoder(\n",
      "  (embedding): Embedding(8014, 256, padding_idx=0)\n",
      "  (rnn): GRU(256, 512, bidirectional=True)\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Shape of encoder outputs (annotations): torch.Size([22, 32, 1024])\n",
      "This corresponds to [source_sequence_length, batch_size, encoder_hidden_dim * 2]\n",
      "\n",
      "Shape of final hidden state (for decoder input): torch.Size([32, 512])\n",
      "This corresponds to [batch_size, decoder_hidden_dim]\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "        :param input_dim: The size of the source vocabulary.\n",
    "        :param emb_dim: The dimensionality of the word embeddings.\n",
    "        :param enc_hid_dim: The dimensionality of the encoder's hidden state (for each direction).\n",
    "        :param dec_hid_dim: The dimensionality of the decoder's hidden state.\n",
    "        :param dropout: The dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hidden_dim, bidirectional=True)\n",
    "        \"\"\" It takes embeddings as input and outputs the hidden states for each time step.\n",
    "            :param enc_hidden_dim: The dimensionality of the hidden state for each direction.\n",
    "        \"\"\"\n",
    "\n",
    "        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "        \"\"\" The output of the bidirectional GRU is concatenated and passed through a linear layer to match the decoder's hidden state size.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \"\"\" Dropout layer to prevent overfitting.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the encoder.\n",
    "        :param src: Source sentence tensor. Shape: [src_len, batch_size]\n",
    "        :return:\n",
    "            - outputs: The concatenated top-layer hidden states from each time step.\n",
    "                       These are the annotations. Shape: [src_len, batch_size, enc_hid_dim * 2]\n",
    "            - hidden: The final decoder hidden state, derived from the encoder's final states.\n",
    "                      Shape: [batch_size, dec_hid_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # The embedded tensor has shape [src_len, batch_size, emb_dim]\n",
    "\n",
    "\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # Output contains concatenated hidden states from both directions for each time step.\n",
    "        # Hidden contains the final hidden states for both directions at the last time step.\n",
    "        \n",
    "        \n",
    "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # Concatenate the last hidden states from both directions to form the final hidden state.\n",
    "        \n",
    "        \n",
    "        hidden = torch.tanh(self.fc(hidden_cat))\n",
    "        # Apply a linear transformation and tanh activation to match the decoder's hidden state size.\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "\n",
    "INPUT_DIM = len(source_vocab)\n",
    "EMB_DIM = 256\n",
    "ENC_HID_DIM = 512  # Hidden state dimensionality for each direction\n",
    "DEC_HID_DIM = 512  # Hidden state dimensionality for the decoder\n",
    "DROPOUT = 0.5  # Dropout probability\n",
    "\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    emb_dim=EMB_DIM,\n",
    "    enc_hidden_dim=ENC_HID_DIM,\n",
    "    dec_hidden_dim=DEC_HID_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Verifying the Encoder module ---\")\n",
    "print(f\"Encoder architecture:\\n{encoder}\\n\")\n",
    "\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder(source_batch)\n",
    "\n",
    "print(f\"Shape of encoder outputs (annotations): {encoder_outputs.shape}\")\n",
    "print(\"This corresponds to [source_sequence_length, batch_size, encoder_hidden_dim * 2]\\n\")\n",
    "\n",
    "print(f\"Shape of final hidden state (for decoder input): {encoder_hidden.shape}\")\n",
    "print(\"This corresponds to [batch_size, decoder_hidden_dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8d3f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying the Attention module ---\n",
      "Attention module architecture:\n",
      "Attention(\n",
      "  (attn_in): Linear(in_features=1536, out_features=512, bias=True)\n",
      "  (v): Linear(in_features=512, out_features=1, bias=False)\n",
      ")\n",
      "\n",
      "Shape of output attention weights: torch.Size([32, 22])\n",
      "This corresponds to [batch_size, source_sequence_length]\n",
      "\n",
      "Sum of weights for the first 5 examples in the batch:\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Each sum should be very close to 1.0, confirming softmax is working correctly.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        \"\"\"\n",
    "        Initializes the Attention module.\n",
    "        :param enc_hid_dim: The dimensionality of the encoder's hidden state.\n",
    "        :param dec_hid_dim: The dimensionality of the decoder's hidden state.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        # The alignment model as described in the paper: e = V.T * tanh(W*s + U*h)\n",
    "        # Here, W*s and U*h are combined into a single linear layer for efficiency. The input to this layer will be the concatenation of the decoder hidden state and an encoder hidden state.\n",
    "        self.attn_in = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        \n",
    "\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the attention mechanism.\n",
    "        :param decoder_hidden: The previous hidden state from the decoder.\n",
    "                               Shape: [batch_size, dec_hid_dim]\n",
    "        :param encoder_outputs: The sequence of annotations from the encoder.\n",
    "                                Shape: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        :return:\n",
    "            - attention_weights: A tensor of attention weights.\n",
    "                                 Shape: [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # To calculate the alignment score for each encoder output, we need to\n",
    "        # repeat the decoder hidden state 'src_len' times.\n",
    "        # Repeat decoder hidden state src_len times\n",
    "        # decoder_hidden shape: [batch_size, dec_hid_dim]\n",
    "        # repeated_decoder_hidden shape: [batch_size, src_len, dec_hid_dim]\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # The encoder_outputs need to be reshaped to match the repeated hidden state.\n",
    "        # encoder_outputs shape: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # Permute to: [batch_size, src_len, enc_hid_dim * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenate the repeated decoder state and the encoder outputs\n",
    "        # This prepares the input for our alignment model's linear layer.\n",
    "        # concat_input shape: [batch_size, src_len, (enc_hid_dim * 2) + dec_hid_dim]\n",
    "        concat_input = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Pass the concatenated tensor through the first linear layer and tanh activation\n",
    "        # energy shape: [batch_size, src_len, dec_hid_dim]\n",
    "        energy = torch.tanh(self.attn_in(concat_input))\n",
    "        \n",
    "        # Pass the energy through the second linear layer (v) to get the alignment scores\n",
    "        # attention shape: [batch_size, src_len, 1]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        # Apply softmax to get a probability distribution of weights over the source sequence\n",
    "        # The softmax is applied to the last dimension (src_len)\n",
    "        # attention_weights shape: [batch_size, src_len]\n",
    "        attention_weights = F.softmax(attention, dim=1)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "\n",
    "attention_module = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "\n",
    "print(\"--- Verifying the Attention module ---\")\n",
    "print(f\"Attention module architecture:\\n{attention_module}\\n\")\n",
    "\n",
    "# Use the outputs from the Encoder verification step\n",
    "# encoder_outputs shape: [src_len, batch_size, enc_hid_dim * 2]\n",
    "# encoder_hidden shape: [batch_size, dec_hid_dim] (this will act as the first decoder hidden state s_0)\n",
    "attention_weights = attention_module(encoder_hidden, encoder_outputs)\n",
    "\n",
    "print(f\"Shape of output attention weights: {attention_weights.shape}\")\n",
    "print(\"This corresponds to [batch_size, source_sequence_length]\\n\")\n",
    "\n",
    "# Check if the weights sum to 1 for each example in the batch\n",
    "sum_of_weights = torch.sum(attention_weights, dim=1)\n",
    "print(f\"Sum of weights for the first 5 examples in the batch:\\n{sum_of_weights[:5]}\")\n",
    "print(\"\\nEach sum should be very close to 1.0, confirming softmax is working correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9652e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying the Decoder module ---\n",
      "Decoder architecture:\n",
      "Decoder(\n",
      "  (attention): Attention(\n",
      "    (attn_in): Linear(in_features=1536, out_features=512, bias=True)\n",
      "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (embedding): Embedding(6191, 256, padding_idx=0)\n",
      "  (rnn): GRU(1280, 512)\n",
      "  (fc_out): Linear(in_features=1792, out_features=6191, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Shape of prediction (logits): torch.Size([32, 6191])\n",
      "This corresponds to [batch_size, target_vocabulary_size]\n",
      "\n",
      "Shape of new decoder hidden state: torch.Size([32, 512])\n",
      "This corresponds to [batch_size, decoder_hidden_dim]\n",
      "\n",
      "Shape of attention weights for this step: torch.Size([32, 22])\n",
      "This corresponds to [batch_size, source_sequence_length]\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "        :param output_dim: The size of the target vocabulary.\n",
    "        :param emb_dim: The dimensionality of the word embeddings.\n",
    "        :param enc_hid_dim: The dimensionality of the encoder's hidden state.\n",
    "        :param dec_hid_dim: The dimensionality of the decoder's hidden state.\n",
    "        :param dropout: The dropout probability.\n",
    "        :param attention: The attention module instantiated in the previous step.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        # 1. Embedding Layer for target vocabulary\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # 2. GRU Layer\n",
    "        # The input to the GRU at each time step is the concatenated previous word's embedding\n",
    "        # and the context vector. So, the input dimension is emb_dim + (enc_hid_dim * 2).\n",
    "        self.rnn = nn.GRU(emb_dim + (enc_hid_dim * 2), dec_hid_dim)\n",
    "        \n",
    "        # 3. Fully Connected Layer (Readout layer)\n",
    "        # This layer generates the final prediction (logits).\n",
    "        # It takes the concatenation of the current GRU output, the context vector,\n",
    "        # and the previous word's embedding as input.\n",
    "        self.fc_out = nn.Linear(dec_hid_dim + (enc_hid_dim * 2) + emb_dim, output_dim)\n",
    "        \n",
    "        # 4. Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for a SINGLE time step of the decoder.\n",
    "        :param input: The current input token (previous word). Shape: [batch_size]\n",
    "        :param hidden: The previous hidden state from the decoder. Shape: [batch_size, dec_hid_dim]\n",
    "        :param encoder_outputs: The sequence of annotations from the encoder.\n",
    "                                Shape: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        :return:\n",
    "            - prediction: Raw logits for the next word. Shape: [batch_size, output_dim]\n",
    "            - hidden: The new decoder hidden state. Shape: [batch_size, dec_hid_dim]\n",
    "            - attention: The attention weights for this step. Shape: [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        # The input to the decoder is a single token at a time, so we need to add a sequence dimension.\n",
    "        # input shape: [batch_size] -> [1, batch_size]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # 1. Get embeddings and apply dropout\n",
    "        # embedded shape: [1, batch_size, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # 2. Get attention weights from the attention module\n",
    "        # The attention module takes the *previous* decoder hidden state and all encoder outputs.\n",
    "        # attention_weights shape: [batch_size, src_len]\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        # 3. Calculate the context vector\n",
    "        # attention_weights shape: [batch_size, src_len] -> [batch_size, 1, src_len]\n",
    "        attention_weights_unsqueezed = attention_weights.unsqueeze(1)\n",
    "        \n",
    "        # encoder_outputs shape: [src_len, batch_size, enc_hid_dim * 2] -> [batch_size, src_len, enc_hid_dim * 2]\n",
    "        encoder_outputs_permuted = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Use batch matrix multiplication (bmm) to get the weighted sum\n",
    "        # context_vector shape: [batch_size, 1, enc_hid_dim * 2]\n",
    "        context_vector = torch.bmm(attention_weights_unsqueezed, encoder_outputs_permuted)\n",
    "        \n",
    "        # Permute the context vector to match the GRU's expected input shape\n",
    "        # context_vector shape: [batch_size, 1, enc_hid_dim * 2] -> [1, batch_size, enc_hid_dim * 2]\n",
    "        context_vector = context_vector.permute(1, 0, 2)\n",
    "        \n",
    "        # 4. Prepare the input for the GRU cell\n",
    "        # Concatenate the word embedding and the context vector\n",
    "        # rnn_input shape: [1, batch_size, emb_dim + enc_hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        \n",
    "        # 5. Pass the input and previous hidden state through the GRU\n",
    "        # The hidden state needs to be of shape [num_layers, batch_size, dec_hid_dim]\n",
    "        # hidden shape: [batch_size, dec_hid_dim] -> [1, batch_size, dec_hid_dim]\n",
    "        # The GRU returns the output for this step and the new hidden state.\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # output shape: [1, batch_size, dec_hid_dim]\n",
    "        # hidden shape: [1, batch_size, dec_hid_dim]\n",
    "        \n",
    "        # Squeeze out the sequence dimension (of size 1) from all tensors\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        context_vector = context_vector.squeeze(0)\n",
    "        hidden = hidden.squeeze(0) # The returned hidden state is ready for the next time step.\n",
    "        \n",
    "        # 6. Generate the final prediction\n",
    "        # Concatenate the GRU output, context vector, and the input embedding\n",
    "        # This is the \"readout\" step, as described in the paper\n",
    "        prediction_input = torch.cat((output, context_vector, embedded), dim=1)\n",
    "        \n",
    "        # Pass through the final linear layer\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        prediction = self.fc_out(prediction_input)\n",
    "        \n",
    "        return prediction, hidden, attention_weights\n",
    "\n",
    "\n",
    "OUTPUT_DIM = len(target_vocab)\n",
    "# Other dims (EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT) are from previous steps\n",
    "\n",
    "# Instantiate the decoder\n",
    "# It requires the attention module we built earlier\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT, attention_module)\n",
    "\n",
    "print(\"--- Verifying the Decoder module ---\")\n",
    "print(f\"Decoder architecture:\\n{decoder}\\n\")\n",
    "\n",
    "# To verify, we need a sample input token for the decoder.\n",
    "# Let's take the first token from the target batch (<sos> token).\n",
    "# target_batch shape: [trg_len, batch_size]\n",
    "decoder_input_token = target_batch[0, :]\n",
    "\n",
    "# Pass the sample inputs through the decoder\n",
    "# encoder_outputs and encoder_hidden are from the Encoder verification step\n",
    "prediction, decoder_hidden, attention = decoder(decoder_input_token, encoder_hidden, encoder_outputs)\n",
    "\n",
    "print(f\"Shape of prediction (logits): {prediction.shape}\")\n",
    "print(f\"This corresponds to [batch_size, target_vocabulary_size]\\n\")\n",
    "\n",
    "print(f\"Shape of new decoder hidden state: {decoder_hidden.shape}\")\n",
    "print(f\"This corresponds to [batch_size, decoder_hidden_dim]\\n\")\n",
    "\n",
    "print(f\"Shape of attention weights for this step: {attention.shape}\")\n",
    "print(\"This corresponds to [batch_size, source_sequence_length]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9894cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Verifying the final Seq2Seq model ---\n",
      "Shape of the final predictions tensor: torch.Size([28, 32, 6191])\n",
      "This corresponds to [target_sequence_length, batch_size, target_vocabulary_size]\n",
      "\n",
      "The model is successfully assembled and produces output of the correct shape.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "        Initializes the Seq2Seq model wrapper.\n",
    "        :param encoder: The instantiated Encoder module.\n",
    "        :param decoder: The instantiated Decoder module.\n",
    "        :param device: The device (e.g., 'cuda' or 'cpu') to move tensors to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the entire sequence-to-sequence model.\n",
    "        :param src: The source sentence tensor. Shape: [src_len, batch_size]\n",
    "        :param trg: The target sentence tensor. Shape: [trg_len, batch_size]\n",
    "        :param teacher_forcing_ratio: The probability to use teacher forcing.\n",
    "                                      e.g., 0.5 means teacher forcing is used 50% of the time.\n",
    "        :return:\n",
    "            - outputs: A tensor of predictions. Shape: [trg_len, batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # Unpack dimensions from input tensors\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # 1. Pass the source sequence through the encoder\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # 2. Prepare for decoding\n",
    "        # Create a tensor to store the decoder's predictions\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # The first input to the decoder is the <sos> token\n",
    "        # trg[0,:] contains the <sos> tokens for the entire batch\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        # 3. Loop through the target sequence to generate predictions\n",
    "        # We loop from 1 to trg_len because we have already used trg[0] as the first input.\n",
    "        # The output at step t is the prediction for the (t+1)-th token.\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # Run one decoding step\n",
    "            # prediction shape: [batch_size, output_dim]\n",
    "            # hidden shape: [batch_size, dec_hid_dim]\n",
    "            prediction, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            # Store the prediction in our outputs tensor\n",
    "            outputs[t] = prediction\n",
    "            \n",
    "            # Decide whether to use teacher forcing for the next input\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token from the prediction tensor\n",
    "            # top1 shape: [batch_size]\n",
    "            top1 = prediction.argmax(1)\n",
    "            \n",
    "            # If teacher forcing, use the actual next token from the target sequence.\n",
    "            # Otherwise, use the model's own prediction.\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# --- Setup Device and Instantiate the Final Model ---\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate all components\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT, attn)\n",
    "\n",
    "# Instantiate the final Seq2Seq model and move it to the device\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verifying the final Seq2Seq model ---\")\n",
    "# Move a sample batch to the correct device\n",
    "source_batch_device = source_batch.to(device)\n",
    "target_batch_device = target_batch.to(device)\n",
    "\n",
    "# Pass the sample batch through the model\n",
    "# We use a high teacher forcing ratio just for this verification\n",
    "predictions = model(source_batch_device, target_batch_device, teacher_forcing_ratio=0.75)\n",
    "\n",
    "print(f\"Shape of the final predictions tensor: {predictions.shape}\")\n",
    "print(\"This corresponds to [target_sequence_length, batch_size, target_vocabulary_size]\")\n",
    "print(\"\\nThe model is successfully assembled and produces output of the correct shape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89497347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Setup ---\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss Function: CrossEntropyLoss (ignoring padding)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    Initializes the learnable weights of the model.\n",
    "    :param m: A module in the model.\n",
    "    \"\"\"\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        elif 'bias' in name:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"Calculates the time taken for an epoch.\"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# Apply the weight initialization\n",
    "model.apply(init_weights)\n",
    "\n",
    "# --- Define Optimizer and Loss Function ---\n",
    "\n",
    "# We use the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Our loss function is CrossEntropyLoss\n",
    "# We ignore the loss calculated on the <pad> token index.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "print(\"--- Training Setup ---\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Loss Function: CrossEntropyLoss (ignoring padding)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **7.2 The Training and Evaluation Loops**\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train()  # Set the model to training mode (enables dropout)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        # 1. Zero the gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass: get predictions\n",
    "        output = model(src, trg)  # teacher_forcing_ratio defaults to 0.5\n",
    "        \n",
    "        # To calculate the loss, we need to reshape the output and target tensors\n",
    "        # output shape: [trg_len, batch_size, output_dim]\n",
    "        # trg shape: [trg_len, batch_size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim) # Ignore <sos> token, flatten\n",
    "        trg = trg[1:].view(-1)                   # Ignore <sos> token, flatten\n",
    "        \n",
    "        # 3. Calculate the loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # 4. Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Clip gradients to prevent them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # 6. Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Performs one epoch of evaluation.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Forward pass: get predictions.\n",
    "            # We turn off teacher forcing for evaluation.\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Reshape tensors for loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9314f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run one training epoch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Run one evaluation epoch\u001b[39;00m\n\u001b[0;32m     16\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
      "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 4. Backward pass: compute gradients\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# 5. Clip gradients to prevent them from exploding\u001b[39;00m\n\u001b[0;32m     75\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Main Training Loop ---\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run one training epoch\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    \n",
    "    # Run one evaluation epoch\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save the best model found so far\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'nmt-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ac5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from nmt-model.pt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "def translate_sentence(sentence, source_vocab, target_vocab, model, device, max_len=50):\n",
    "    \"\"\"\n",
    "    Translates a single source sentence into the target language.\n",
    "    :param sentence: The raw source sentence string.\n",
    "    :param source_vocab: The source vocabulary object.\n",
    "    :param target_vocab: The target vocabulary object.\n",
    "    :param model: The trained Seq2Seq model.\n",
    "    :param device: The device to run on ('cpu' or 'cuda').\n",
    "    :param max_len: The maximum length for the output sentence.\n",
    "    :return:\n",
    "        - translated_sentence_tokens: A list of translated token strings.\n",
    "        - attention: The attention weights tensor. Shape: [trg_len, src_len]\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    # --- 1. Pre-process the source sentence ---\n",
    "    \n",
    "    # Tokenize the source sentence\n",
    "    tokens = tokenize_de(sentence) # tokenize_de also reverses the sentence\n",
    "    \n",
    "    # Add special tokens if your tokenizer doesn't. Ours doesn't for the source.\n",
    "    # tokens = [source_vocab.stoi['<sos>']] + tokens + [source_vocab.stoi['<eos>']]\n",
    "    \n",
    "    # Numericalize the tokens\n",
    "    src_indexes = source_vocab.numericalize(sentence, tokenize_de)\n",
    "    \n",
    "    # Convert to a tensor and add the batch dimension (batch_size = 1)\n",
    "    # Shape: [src_len, 1]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    # --- 2. Encoder Pass ---\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    # --- 3. Autoregressive Decoding Loop ---\n",
    "    \n",
    "    # The first input to the decoder is the <sos> token\n",
    "    trg_indexes = [target_vocab.stoi['<sos>']]\n",
    "    \n",
    "    # Create a tensor to store attention scores for each decoding step\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        # Get the last predicted token as the input for the next step\n",
    "        # Add batch dimension and move to device\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Run one decoding step\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "            \n",
    "            # Store the attention weights\n",
    "            attentions[i] = attention\n",
    "\n",
    "        # Get the index of the most likely predicted token\n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        # Append the prediction to our running list of target indices\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # If the predicted token is <eos>, we're done\n",
    "        if pred_token == target_vocab.stoi['<eos>']:\n",
    "            break\n",
    "            \n",
    "    # --- 4. Post-process the output ---\n",
    "    \n",
    "    # Convert the output indices to tokens, skipping the initial <sos>\n",
    "    translated_sentence_tokens = [target_vocab.itos[i] for i in trg_indexes[1:]]\n",
    "    \n",
    "    # Return the translated tokens and the attention matrix\n",
    "    # attentions shape: [trg_len, 1, src_len] -> [trg_len, src_len]\n",
    "    return translated_sentence_tokens, attentions[:len(trg_indexes)-1, :, :].squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Load the model and test ---\n",
    "\n",
    "# Instantiate a new model with the same architecture\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('nmt-model.pt'))\n",
    "\n",
    "print(\"Model loaded successfully from nmt-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f870f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing on a sample sentence ---\n",
      "Source Sentence      = Ein kleines Kind steht allein auf einem zerklüfteten Felsen.\n",
      "Actual Translation   = A young child is standing alone on some jagged rocks.\n",
      "Predicted Translation = A young child stands alone on a rock rock rock . <eos>\n"
     ]
    }
   ],
   "source": [
    "# Choose an example from the validation set\n",
    "example_idx = 10\n",
    "\n",
    "source_raw = val_dataset.data[example_idx][0]\n",
    "target_raw = val_dataset.data[example_idx][1]\n",
    "\n",
    "print(\"--- Testing on a sample sentence ---\")\n",
    "print(f'Source Sentence      = {source_raw}')\n",
    "print(f'Actual Translation   = {target_raw}')\n",
    "\n",
    "# Get the model's translation\n",
    "translation_tokens, attention = translate_sentence(source_raw, source_vocab, target_vocab, model, device)\n",
    "predicted_translation = ' '.join(translation_tokens)\n",
    "\n",
    "print(f'Predicted Translation = {predicted_translation}')\n",
    "\n",
    "\n",
    "# --- Visualize the Attention ---\n",
    "\n",
    "# The source tokens for display should not be reversed\n",
    "# So we tokenize again without the reversal for the plot's labels.\n",
    "source_tokens_for_display = [tok.text for tok in spacy_de.tokenizer(source_raw)]\n",
    "# The translated tokens should not include the <eos> token for the plot\n",
    "translation_tokens_for_display = translation_tokens[:-1] if translation_tokens[-1] == '<eos>' else translation_tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
