{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a4a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model Architecture:\n",
      " Encoder(\n",
      "  (embedding): Embedding(5000, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Shape of dummy input sequence: torch.Size([10, 32])\n",
      "Data type of dummy input: torch.int64\n",
      "\n",
      "Shape of final hidden state (context): torch.Size([2, 32, 512])\n",
      "Expected hidden shape: [2, 32, 512]\n",
      "\n",
      "Shape of final cell state (context): torch.Size([2, 32, 512])\n",
      "Expected cell shape: [2, 32, 512]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder module of the sequence-to-sequence model.\n",
    "    It takes a sequence of input tokens and returns a context vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The size of the input (source) vocabulary.\n",
    "            embedding_dim (int): The dimensionality of the word embeddings.\n",
    "            hidden_dim (int): The dimensionality of the hidden and cell states of the LSTM.\n",
    "            n_layers (int): The number of layers in the LSTM.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        # Calls the constructor of the parent class (nn.Module) to properly initialize the module.\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the hidden dimension and number of layers for later use.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        # This layer converts input token indices into dense vector representations (embeddings).\n",
    "        # - input_dim: The number of unique tokens in our source vocabulary.\n",
    "        # - embedding_dim: The size of the vector that will represent each token.\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        # 2. LSTM Layer\n",
    "        # This is the core of the encoder. It's a multi-layered LSTM that processes the\n",
    "        # sequence of embeddings.\n",
    "        # - input_size: The dimensionality of the input to the LSTM at each timestep.\n",
    "        #   This must match the embedding_dim from the previous layer.\n",
    "        # - hidden_size: The dimensionality of the hidden state and cell state.\n",
    "        # - num_layers: The number of stacked LSTM layers (for a deep LSTM).\n",
    "        # - dropout: Adds a dropout layer on the outputs of each LSTM layer except the\n",
    "        #   last one. This is a regularization technique to prevent overfitting.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # 3. Dropout Layer\n",
    "        # This is a regularization layer. During training, it will randomly zero out some\n",
    "        # of the elements of the input tensor with probability 'dropout'.\n",
    "        # This helps to prevent the model from becoming too reliant on any single feature.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (torch.Tensor): The input sequence of token indices.\n",
    "                                     Shape: [seq_len, batch_size]\n",
    "\n",
    "        Returns:\n",
    "            tuple(torch.Tensor, torch.Tensor): The final hidden and cell states of the LSTM,\n",
    "                                               which serve as the context vector.\n",
    "                                               - hidden: Shape [n_layers, batch_size, hidden_dim]\n",
    "                                               - cell:   Shape [n_layers, batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # input_seq shape: [seq_len, batch_size]\n",
    "        # This is a tensor of long integers, where each integer is an index\n",
    "        # corresponding to a token in our vocabulary.\n",
    "\n",
    "        # 1. Pass the input sequence through the embedding layer.\n",
    "        # The embedding layer looks up the vector for each token index.\n",
    "        # embedded shape: [seq_len, batch_size, embedding_dim]\n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        # 2. Apply dropout to the embeddings.\n",
    "        # This is done to regularize the model. Dropout is typically applied after\n",
    "        # embeddings and before they are passed into the recurrent layers.\n",
    "        # embedded_dropout shape: [seq_len, batch_size, embedding_dim]\n",
    "        embedded_dropout = self.dropout(embedded)\n",
    "\n",
    "        # 3. Pass the dropout-applied embeddings through the LSTM.\n",
    "        # The LSTM processes the sequence one timestep at a time. It does not require\n",
    "        # an initial hidden/cell state; if not provided, PyTorch defaults to a\n",
    "        # zero-initialized state.\n",
    "        # - `outputs`: Contains the hidden state from the *final* LSTM layer for *every*\n",
    "        #   timestep. We don't need this for our context vector, so we ignore it.\n",
    "        #   Shape: [seq_len, batch_size, hidden_dim]\n",
    "        # - `hidden`: Contains the final hidden state for *each* layer from the *final*\n",
    "        #   timestep. This is a crucial part of our context vector.\n",
    "        #   Shape: [n_layers, batch_size, hidden_dim]\n",
    "        # - `cell`: Contains the final cell state for *each* layer from the *final*\n",
    "        #   timestep. This is the other crucial part of our context vector.\n",
    "        #   Shape: [n_layers, batch_size, hidden_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded_dropout)\n",
    "\n",
    "        # The core idea of this Seq2Seq model is to use the final hidden and cell\n",
    "        # states of the encoder as the context vector. This context is then used to\n",
    "        # initialize the decoder.\n",
    "        return hidden, cell\n",
    "\n",
    "# --- Putting It All Together: A Small Example ---\n",
    "\n",
    "# Define the hyperparameters for our Encoder instance.\n",
    "INPUT_DIM = 5000  # Example source vocabulary size\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2  # The paper used 4, but we use 2 for a simpler example\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Instantiate the Encoder model.\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "print(\"Encoder Model Architecture:\\n\", encoder)\n",
    "\n",
    "# --- Create a dummy input to test the forward pass ---\n",
    "\n",
    "# Let's create a dummy batch of sentences.\n",
    "# batch_size = 32\n",
    "# seq_len = 10\n",
    "# The input must be a tensor of long integers (token indices).\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "dummy_input_seq = torch.randint(0, INPUT_DIM, (seq_len, batch_size))\n",
    "\n",
    "print(f\"\\nShape of dummy input sequence: {dummy_input_seq.shape}\")\n",
    "print(f\"Data type of dummy input: {dummy_input_seq.dtype}\")\n",
    "\n",
    "# Pass the dummy input through the encoder.\n",
    "# We set the model to evaluation mode to disable dropout for this test pass.\n",
    "encoder.eval()\n",
    "with torch.no_grad(): # We don't need to compute gradients for this test\n",
    "    final_hidden, final_cell = encoder(dummy_input_seq)\n",
    "\n",
    "# --- Check the outputs ---\n",
    "\n",
    "print(f\"\\nShape of final hidden state (context): {final_hidden.shape}\")\n",
    "print(f\"Expected hidden shape: [{N_LAYERS}, {batch_size}, {HIDDEN_DIM}]\")\n",
    "\n",
    "print(f\"\\nShape of final cell state (context): {final_cell.shape}\")\n",
    "print(f\"Expected cell shape: [{N_LAYERS}, {batch_size}, {HIDDEN_DIM}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc51266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Model Architecture:\n",
      " Decoder(\n",
      "  (embedding): Embedding(5500, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "  (fc_out): Linear(in_features=512, out_features=5500, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Shape of initial context (hidden): torch.Size([2, 32, 512])\n",
      "Shape of initial context (cell): torch.Size([2, 32, 512])\n",
      "\n",
      "Shape of dummy target token: torch.Size([32])\n",
      "\n",
      "Shape of prediction logits: torch.Size([32, 5500])\n",
      "Expected prediction shape: [32, 5500]\n",
      "\n",
      "Shape of next hidden state: torch.Size([2, 32, 512])\n",
      "Expected next hidden shape: [2, 32, 512]\n",
      "\n",
      "Shape of next cell state: torch.Size([2, 32, 512])\n",
      "Expected next cell shape: [2, 32, 512]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder module of the sequence-to-sequence model.\n",
    "    It takes a context vector and a target token, and returns a prediction\n",
    "    for the next token in the sequence and an updated context vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_dim (int): The size of the output (target) vocabulary.\n",
    "            embedding_dim (int): The dimensionality of the word embeddings.\n",
    "            hidden_dim (int): The dimensionality of the hidden and cell states of the LSTM.\n",
    "                              This MUST be the same as the encoder's hidden_dim.\n",
    "            n_layers (int): The number of layers in the LSTM.\n",
    "                            This MUST be the same as the encoder's n_layers.\n",
    "            dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        # Call the parent class constructor.\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the output dimension for later use in the linear layer.\n",
    "        self.output_dim = output_dim\n",
    "        # Store hidden_dim and n_layers, which must match the encoder.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        # This layer converts the input token index (from the target sequence)\n",
    "        # into a dense vector representation.\n",
    "        # - output_dim: The number of unique tokens in our target vocabulary.\n",
    "        # - embedding_dim: The size of the vector for each target token.\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "\n",
    "        # 2. LSTM Layer\n",
    "        # The decoder's LSTM. Its role is to process the input token's embedding\n",
    "        # along with the context from the previous timestep to produce an output\n",
    "        # hidden state.\n",
    "        # - The dimensions (embedding_dim, hidden_dim, n_layers) must align with\n",
    "        #   the encoder's dimensions so that the context vector can be passed seamlessly.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # 3. Fully Connected (Linear) Layer\n",
    "        # This layer is crucial for prediction. It takes the LSTM's output hidden\n",
    "        # state and transforms it into a vector of scores (logits), one for each\n",
    "        # token in our target vocabulary.\n",
    "        # - in_features: The size of the input to this layer, which is the LSTM's\n",
    "        #   hidden state dimension.\n",
    "        # - out_features: The size of the output, which must be the size of our\n",
    "        #   target vocabulary (output_dim).\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # 4. Dropout Layer\n",
    "        # A regularization layer, similar to the one in the encoder.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for a single decoding step.\n",
    "\n",
    "        Args:\n",
    "            input_token (torch.Tensor): The input token for the current timestep.\n",
    "                                        Shape: [batch_size]\n",
    "            hidden_state (torch.Tensor): The hidden state from the previous timestep.\n",
    "                                         Shape: [n_layers, batch_size, hidden_dim]\n",
    "            cell_state (torch.Tensor): The cell state from the previous timestep.\n",
    "                                       Shape: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - prediction (torch.Tensor): Raw, unnormalized scores (logits) for each\n",
    "                                             token in the output vocabulary.\n",
    "                                             Shape: [batch_size, output_dim]\n",
    "                - hidden_state (torch.Tensor): The updated hidden state.\n",
    "                                               Shape: [n_layers, batch_size, hidden_dim]\n",
    "                - cell_state (torch.Tensor): The updated cell state.\n",
    "                                             Shape: [n_layers, batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # The input_token is a 1D tensor of shape [batch_size], containing the\n",
    "        # index of the token for each sequence in the batch.\n",
    "        \n",
    "        # We need to add a sequence length dimension to the input_token tensor\n",
    "        # because the embedding and LSTM layers expect a sequence. Since we are\n",
    "        # decoding one token at a time, the sequence length is 1.\n",
    "        # input_token shape: [batch_size] -> [1, batch_size]\n",
    "        input_token = input_token.unsqueeze(0)\n",
    "\n",
    "        # 1. Pass the input token through the embedding layer and apply dropout.\n",
    "        # embedded shape: [1, batch_size, embedding_dim]\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "\n",
    "        # 2. Pass the embedded token and the previous hidden/cell states into the LSTM.\n",
    "        # The LSTM takes the embedding and the previous context (hidden, cell)\n",
    "        # and computes the next context.\n",
    "        # - `output`: The hidden state from the top LSTM layer for this timestep.\n",
    "        #   Shape: [1, batch_size, hidden_dim]\n",
    "        # - `hidden_state`, `cell_state`: The updated hidden and cell states for all layers.\n",
    "        #   Shape: [n_layers, batch_size, hidden_dim]\n",
    "        output, (hidden_state, cell_state) = self.lstm(embedded, (hidden_state, cell_state))\n",
    "\n",
    "        # The `output` tensor has a sequence length dimension of 1. We need to\n",
    "        # remove this before passing it to the linear layer, which expects a\n",
    "        # 2D tensor of [batch_size, hidden_dim].\n",
    "        # output shape: [1, batch_size, hidden_dim] -> [batch_size, hidden_dim]\n",
    "        assert output.shape[0] == 1, \"Output sequence length should be 1\"\n",
    "        output_squeezed = output.squeeze(0)\n",
    "\n",
    "        # 3. Pass the LSTM's output through the final linear layer.\n",
    "        # This generates the raw prediction scores (logits) over the vocabulary.\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        prediction = self.fc_out(output_squeezed)\n",
    "\n",
    "        # Return the prediction and the new hidden and cell states. These new states\n",
    "        # will be used as the context for the next decoding step.\n",
    "        return prediction, hidden_state, cell_state\n",
    "\n",
    "\n",
    "\n",
    "# --- Define Hyperparameters ---\n",
    "INPUT_DIM = 5000\n",
    "OUTPUT_DIM = 5500 # Example target vocabulary size\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Instantiate both models\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "print(\"Decoder Model Architecture:\\n\", decoder)\n",
    "\n",
    "# --- Create dummy inputs to test the one-step forward pass ---\n",
    "batch_size = 32\n",
    "\n",
    "# 1. Create a dummy source sequence for the encoder\n",
    "dummy_input_seq = torch.randint(0, INPUT_DIM, (10, batch_size)) # seq_len=10\n",
    "\n",
    "# 2. Get the initial context vector from the encoder\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    initial_hidden, initial_cell = encoder(dummy_input_seq)\n",
    "\n",
    "print(f\"\\nShape of initial context (hidden): {initial_hidden.shape}\")\n",
    "print(f\"Shape of initial context (cell): {initial_cell.shape}\")\n",
    "\n",
    "\n",
    "# 3. Create a dummy target token to start the decoding process\n",
    "# This would typically be the <SOS> (start-of-sequence) token index.\n",
    "# We'll just use a random token for this example.\n",
    "dummy_target_token = torch.randint(0, OUTPUT_DIM, (batch_size,)) # One token for each item in the batch\n",
    "\n",
    "print(f\"\\nShape of dummy target token: {dummy_target_token.shape}\")\n",
    "\n",
    "\n",
    "# 4. Perform one decoding step\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    prediction, next_hidden, next_cell = decoder(dummy_target_token, initial_hidden, initial_cell)\n",
    "\n",
    "# --- Check the outputs of the decoder ---\n",
    "print(f\"\\nShape of prediction logits: {prediction.shape}\")\n",
    "print(f\"Expected prediction shape: [{batch_size}, {OUTPUT_DIM}]\")\n",
    "\n",
    "print(f\"\\nShape of next hidden state: {next_hidden.shape}\")\n",
    "print(f\"Expected next hidden shape: [{N_LAYERS}, {batch_size}, {HIDDEN_DIM}]\")\n",
    "\n",
    "print(f\"\\nShape of next cell state: {next_cell.shape}\")\n",
    "print(f\"Expected next cell shape: [{N_LAYERS}, {batch_size}, {HIDDEN_DIM}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238ee4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Seq2Seq Model Architecture:\n",
      " Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(5000, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(5500, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=512, out_features=5500, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Shape of final output tensor: torch.Size([15, 16, 5500])\n",
      "Expected output shape: [15, 16, 5500]\n",
      "\n",
      "First timestep output (should be zeros):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Second timestep output (should be populated):\n",
      "tensor([-0.0694,  0.0140, -0.0068, -0.0345,  0.0522, -0.0241,  0.0562, -0.0546,\n",
      "         0.0416, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Seq2Seq model that encapsulates the Encoder and Decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "        Initializes the Seq2Seq model.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): The encoder module.\n",
    "            decoder (Decoder): The decoder module.\n",
    "            device (torch.device): The device (CPU or GPU) to which tensors will be sent.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure the encoder and decoder have the same hidden dimensions and number of layers.\n",
    "        # This is a critical architectural constraint.\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have the same number of layers!\"\n",
    "\n",
    "    def forward(self, source_seq, target_seq, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Seq2Seq model.\n",
    "\n",
    "        Args:\n",
    "            source_seq (torch.Tensor): The source sequence.\n",
    "                                       Shape: [src_len, batch_size]\n",
    "            target_seq (torch.Tensor): The target sequence.\n",
    "                                       Shape: [trg_len, batch_size]\n",
    "            teacher_forcing_ratio (float): The probability of using the ground-truth\n",
    "                                           target token as the next input, instead of\n",
    "                                           the model's own prediction.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of predictions (logits) for the entire target sequence.\n",
    "                          Shape: [trg_len, batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # Get the batch size and target sequence length from the input tensors.\n",
    "        batch_size = target_seq.shape[1]\n",
    "        trg_len = target_seq.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # 1. Create a tensor to store the decoder's outputs for each timestep.\n",
    "        # This tensor will be filled with the raw logit predictions.\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # 2. Pass the source sequence through the encoder to get the context vector.\n",
    "        # `hidden` and `cell` are the final states of the encoder's LSTM.\n",
    "        hidden, cell = self.encoder(source_seq)\n",
    "\n",
    "        # 3. The first input to the decoder is the <SOS> (start-of-sequence) token.\n",
    "        # The target_seq tensor is assumed to contain the <SOS> token at the first\n",
    "        # timestep (index 0).\n",
    "        # input_token shape: [batch_size]\n",
    "        input_token = target_seq[0,:]\n",
    "\n",
    "        # 4. Loop through the target sequence to generate predictions one token at a time.\n",
    "        # We loop from the second token (index 1) because the first token is our <SOS> input.\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # - Pass the current input token and the previous context (hidden, cell)\n",
    "            #   into the decoder.\n",
    "            # - This gives us a prediction for the next token and the updated context.\n",
    "            prediction, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "\n",
    "            # - Store the prediction in our `outputs` tensor.\n",
    "            outputs[t] = prediction\n",
    "\n",
    "            # - Decide whether to use \"teacher forcing\" for the next input.\n",
    "            #   Teacher forcing means we use the actual ground-truth token from the\n",
    "            #   target sequence as the next input.\n",
    "            #   Otherwise, we use the model's own highest-probability prediction.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # - Get the token with the highest probability from our prediction.\n",
    "            #   `prediction.argmax(1)` finds the index of the max value along dimension 1.\n",
    "            top1 = prediction.argmax(1)\n",
    "\n",
    "            # - Determine the next input token.\n",
    "            #   If teacher_force is true, use the actual next token from the target sequence.\n",
    "            #   Otherwise, use the model's own prediction.\n",
    "            input_token = target_seq[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# --- Putting It All Together: A Small Example ---\n",
    "\n",
    "# Define hyperparameters\n",
    "INPUT_DIM = 5000\n",
    "OUTPUT_DIM = 5500\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the sub-models\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# Instantiate the main Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(\"\\nSeq2Seq Model Architecture:\\n\", model)\n",
    "\n",
    "# --- Create dummy inputs to test the full forward pass ---\n",
    "batch_size = 16\n",
    "src_len = 12\n",
    "trg_len = 15 # Target can have a different length\n",
    "\n",
    "# Dummy source and target sequences\n",
    "dummy_src_seq = torch.randint(0, INPUT_DIM, (src_len, batch_size)).to(device)\n",
    "dummy_trg_seq = torch.randint(0, OUTPUT_DIM, (trg_len, batch_size)).to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "# Set the model to training mode to enable dropout\n",
    "model.train()\n",
    "outputs = model(dummy_src_seq, dummy_trg_seq) # Use default teacher_forcing_ratio=0.5\n",
    "\n",
    "# --- Check the output shape ---\n",
    "print(f\"\\nShape of final output tensor: {outputs.shape}\")\n",
    "print(f\"Expected output shape: [{trg_len}, {batch_size}, {OUTPUT_DIM}]\")\n",
    "\n",
    "# Check the first output (corresponding to <SOS> input)\n",
    "# It should be all zeros, as we initialized it that way and start the loop from t=1.\n",
    "print(\"\\nFirst timestep output (should be zeros):\")\n",
    "print(outputs[0,0,:10])\n",
    "\n",
    "# Check a later output\n",
    "print(\"\\nSecond timestep output (should be populated):\")\n",
    "print(outputs[1,0,:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
