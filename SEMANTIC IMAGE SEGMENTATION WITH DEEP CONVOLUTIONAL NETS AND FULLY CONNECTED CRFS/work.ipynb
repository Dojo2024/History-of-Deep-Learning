{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba10bdf3",
   "metadata": {},
   "source": [
    "### DeepLab-MSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10a135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # The fundamental PyTorch library for tensor operations and neural networks.\n",
    "import torch.nn as nn # Contains modules for building neural networks (e.g., Conv2d, ReLU, MaxPool2d).\n",
    "import torch.nn.functional as F # Provides functions that don't have trainable parameters (e.g., interpolation).\n",
    "import torchvision.models as models # Provides access to pre-trained models like VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab606d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBranch(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=128):\n",
    "        # Call the constructor of the parent class (nn.Module).\n",
    "        # This is essential for PyTorch modules to properly initialize themselves,\n",
    "        # register parameters, and manage internal states.\n",
    "        super(MLPBranch, self).__init__()\n",
    "\n",
    "        # First layer: A 2D Convolutional layer with a 3x3 kernel.\n",
    "        # This is the first part of the \"two-layer MLP\" as described.\n",
    "        # It processes the input feature map (from a VGG pooling layer) locally.\n",
    "        # in_channels: The number of input feature channels from the VGG pooling layer (e.g., 64 for pool1, 128 for pool2, etc.).\n",
    "        # out_channels: The number of output feature channels, fixed to 128 as specified in the paper.\n",
    "        # kernel_size=3: The filter size is 3x3, allowing it to capture local spatial patterns.\n",
    "        # padding=1: 'same' padding is used to ensure the output feature map has the same spatial dimensions as the input.\n",
    "        #           For a 3x3 kernel, padding=1 maintains spatial resolution.\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # ReLU activation function for non-linearity after the first convolution.\n",
    "        # This introduces non-linearity, allowing the network to learn more complex patterns.\n",
    "        self.relu1 = nn.ReLU(inplace=True) # inplace=True saves memory by modifying input directly.\n",
    "\n",
    "        # Second layer: A 2D Convolutional layer with a 1x1 kernel.\n",
    "        # This is the second part of the \"two-layer MLP\".\n",
    "        # in_channels: The output channels from the previous conv1 layer (which is out_channels, 128).\n",
    "        # out_channels: The final output channels for this branch, also 128.\n",
    "        # kernel_size=1: A 1x1 kernel operates on each spatial location independently,\n",
    "        #               performing a linear combination across the feature channels.\n",
    "        #               It's like a fully connected layer operating channel-wise across the feature map.\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        # ReLU activation after the second convolution.\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass: how data flows through the layers.\n",
    "        # x: Input tensor to the MLP branch (a feature map from a VGG pooling layer).\n",
    "\n",
    "        # Apply the first convolution and ReLU.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Apply the second convolution and ReLU.\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Return the processed feature map from this branch.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbbf205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabMSc(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        # Call the constructor of the parent class (nn.Module).\n",
    "        super(DeepLabMSc, self).__init__()\n",
    "\n",
    "        # --- 3.1. VGG-16 Backbone Initialization and Modification ---\n",
    "        # Load a pre-trained VGG-16 model from torchvision.\n",
    "        # pretrained=True: Loads weights pre-trained on the ImageNet dataset.\n",
    "        #                  This is crucial for transfer learning, as these weights are good\n",
    "        #                  general feature extractors and help with convergence.\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        \n",
    "        # Access the 'features' module of VGG-16. This is the convolutional part of VGG.\n",
    "        # VGG-16 is composed of a 'features' module (conv layers, pooling)\n",
    "        # and a 'classifier' module (fully connected layers).\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # Identify key layers for feature extraction for multi-scale branches.\n",
    "        # These are the indices of the MaxPool2d layers within the vgg16.features Sequential module.\n",
    "        # We need their outputs for our MLP branches.\n",
    "        self.pool1_idx = 4  # nn.MaxPool2d at index 4 (after conv1_2)\n",
    "        self.pool2_idx = 9  # nn.MaxPool2d at index 9 (after conv2_2)\n",
    "        self.pool3_idx = 16 # nn.MaxPool2d at index 16 (after conv3_3)\n",
    "        self.pool4_idx = 23 # nn.MaxPool2d at index 23 (after conv4_3)\n",
    "\n",
    "        # --- DeepLab-specific modifications for VGG-16 backbone ---\n",
    "        # The paper states: \"skip subsampling after the last two max-pooling layers\"\n",
    "        # This refers to VGG's original pool4 and pool5.\n",
    "        # In PyTorch's VGG-16 features module:\n",
    "        # features[23] is MaxPool2d (pool4)\n",
    "        # features[30] is MaxPool2d (pool5)\n",
    "\n",
    "        # 1. Modify stride of pool4 and pool5 to 1.\n",
    "        # This prevents resolution reduction at these stages, maintaining feature map density.\n",
    "        # Originally, these layers would halve spatial dimensions. Setting stride=1 means\n",
    "        # they now only apply a max operation without downsampling.\n",
    "        # This keeps the effective output stride after pool5 at 1/8 of input.\n",
    "        self.features[self.pool4_idx].stride = 1 # Change pool4 stride from 2 to 1\n",
    "        self.features[self.pool5_idx].stride = 1 # Change pool5 stride from 2 to 1\n",
    "\n",
    "        # 2. Apply atrous (dilated) convolution to subsequent layers.\n",
    "        # This compensates for the reduced effective stride and expands the receptive field,\n",
    "        # allowing the network to see a larger context without further downsampling.\n",
    "        # The paper specifies: \"2x in the last three convolutional layers and 4x in the first fully connected layer\"\n",
    "        # 'Last three convolutional layers' refer to conv5_1, conv5_2, conv5_3 (in VGG features module).\n",
    "        # 'First fully connected layer' refers to the one converted from vgg.classifier[0] (fc6).\n",
    "\n",
    "        # For conv5_1 (features[24]), conv5_2 (features[26]), conv5_3 (features[28]):\n",
    "        # Set dilation to 2. To maintain same output size, padding must be adjusted.\n",
    "        # For kernel_size=3, dilation=2, new padding = dilation * (kernel_size - 1) // 2 = 2 * (3 - 1) // 2 = 2.\n",
    "        for i in [24, 26, 28]: # Indices for conv5_1, conv5_2, conv5_3\n",
    "            if isinstance(self.features[i], nn.Conv2d): # Ensure it's a Conv2d layer\n",
    "                self.features[i].dilation = (2, 2) # Set dilation to 2 in both dimensions\n",
    "                self.features[i].padding = (2, 2)  # Adjust padding accordingly\n",
    "\n",
    "        # --- 3.2. Converting Fully Connected Layers to Convolutional Layers ---\n",
    "        # VGG-16's original classifier has three nn.Linear layers: fc6, fc7, fc8.\n",
    "        # For dense prediction (segmentation), these must be converted to nn.Conv2d layers.\n",
    "        # The input to fc6 comes from pool5 (which is 512 channels, 7x7 spatial size if original 224x224 input).\n",
    "        # With our modifications (pool5 stride=1), the input to conv6 (converted fc6)\n",
    "        # will have spatial dimensions that are 1/8th of the original input.\n",
    "\n",
    "        # Store the original classifier weights to initialize the new convolutional layers.\n",
    "        # This preserves the learned knowledge from ImageNet classification.\n",
    "        # clone() is important to ensure we don't modify the original tensor in-place before copying.\n",
    "        # detach() removes the tensor from the current computation graph, so no gradients are computed for this copy.\n",
    "        classifier = vgg16.classifier.clone().detach()\n",
    "\n",
    "        # fc6 (original classifier[0]) -> new conv6\n",
    "        # Input channels: 512 (from conv5_3 output)\n",
    "        # Output channels: 4096 (original fc6 output size)\n",
    "        # Kernel size: 7x7 (original spatial size for 224x224 input / 32)\n",
    "        # Dilation: The paper specifies \"4x in the first fully connected layer\".\n",
    "        #           So, dilation=4. Padding = dilation * (kernel_size - 1) // 2 = 4 * (7 - 1) // 2 = 12.\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, padding=12, dilation=4)\n",
    "        # Copy weights from fc6. We need to reshape the linear layer weights (4096, 512*7*7)\n",
    "        # to convolutional weights (out_channels, in_channels, kernel_h, kernel_w).\n",
    "        # We divide by (7*7) to get the average weight for each channel per kernel output location.\n",
    "        # The VGG fc6 weights are often considered as 7x7 filters replicated for all input locations.\n",
    "        self.conv6.weight.data = classifier[0].weight.data.view(4096, 512, 7, 7)\n",
    "        self.conv6.bias.data = classifier[0].bias.data\n",
    "\n",
    "        # fc7 (original classifier[3]) -> new conv7\n",
    "        # Input channels: 4096 (from conv6 output)\n",
    "        # Output channels: 4096 (original fc7 output size)\n",
    "        # Kernel size: 1x1 (standard for fully connected layer conversion when spatial dim is 1x1, or for channel mixing)\n",
    "        # Dilation: Not explicitly specified as dilated, so usually 1x1.\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "        # Copy weights. For a 1x1 conv, reshape from (4096, 4096) to (4096, 4096, 1, 1).\n",
    "        self.conv7.weight.data = classifier[3].weight.data.view(4096, 4096, 1, 1)\n",
    "        self.conv7.bias.data = classifier[3].bias.data\n",
    "\n",
    "        # fc8 (original classifier[6]) -> final_conv\n",
    "        # This is the classification head predicting scores for each class.\n",
    "        # Input channels: 4096 (from conv7 output)\n",
    "        # Output channels: num_classes (e.g., 21 for PASCAL VOC)\n",
    "        # Kernel size: 1x1\n",
    "        # Dilation: Not explicitly specified as dilated.\n",
    "        self.final_conv_main = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        # Copy weights. Reshape from (num_classes, 4096) to (num_classes, 4096, 1, 1).\n",
    "        self.final_conv_main.weight.data = classifier[6].weight.data.view(num_classes, 4096, 1, 1)\n",
    "        self.final_conv_main.bias.data = classifier[6].bias.data\n",
    "\n",
    "        # ReLU activations for the converted FC layers.\n",
    "        # VGG's classifier has ReLU after fc6 and fc7.\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # --- 3.3. Multi-Scale Branch Initialization ---\n",
    "        # Instantiate MLPBranch for each pooling layer.\n",
    "        # The in_channels correspond to the output channels of the respective VGG pooling layer.\n",
    "        self.branch1 = MLPBranch(in_channels=64)  # pool1 outputs 64 channels\n",
    "        self.branch2 = MLPBranch(in_channels=128) # pool2 outputs 128 channels\n",
    "        self.branch3 = MLPBranch(in_channels=256) # pool3 outputs 256 channels\n",
    "        self.branch4 = MLPBranch(in_channels=512) # pool4 (modified stride=1) outputs 512 channels\n",
    "\n",
    "        # --- 3.4. Final Classification Head for Concatenated Features ---\n",
    "        # The paper states: \"The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels.\"\n",
    "        # This implies that the main network's final feature map (before its own classification head)\n",
    "        # is also processed to 128 channels, or there's a different setup.\n",
    "        # However, the general interpretation is: main_network_features + 4 * branch_features.\n",
    "        # If main network's features (before final_conv_main) are 4096 channels, and each branch outputs 128,\n",
    "        # then total input channels to the *final* classification head for concatenated features would be:\n",
    "        # 4096 (from conv7) + 4 * 128 (from 4 branches) = 4096 + 512 = 4608 channels.\n",
    "        # The paper's text \"5 * 128 = 640 channels\" might imply they changed conv7 output to 128 channels\n",
    "        # or it refers to a different specific variant.\n",
    "        # For DeepLab-MSc implementation, the common approach is to use the conv5_3 (main) output for concat,\n",
    "        # or have another small layer that reduces conv7 output to 128 channels.\n",
    "        # Let's stick to the interpretation that the final classification head takes all concatenated features.\n",
    "        # We assume the main network's final features (e.g. from conv7) are 4096 channels, as per VGG.\n",
    "        # Total concatenated channels = 4096 (from main path) + 4 * 128 (from branches) = 4608 channels.\n",
    "        self.final_classifier_msc = nn.Conv2d(4096 + (4 * 128), num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input dimensions for final upsampling.\n",
    "        input_H, input_W = x.shape[2], x.shape[3]\n",
    "\n",
    "        # Containers to store outputs of pooling layers and branch outputs.\n",
    "        pool_outputs = {}\n",
    "        branch_features = []\n",
    "\n",
    "        # --- 3.5. Forward Pass through VGG Features (with DeepLab modifications) ---\n",
    "        # Iterate through the VGG features module to get intermediate outputs.\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x) # Pass input through the current layer\n",
    "\n",
    "            # Store outputs at specific pooling layer indices.\n",
    "            if i == self.pool1_idx:\n",
    "                pool_outputs['pool1'] = x # (N, 64, H/2, W/2)\n",
    "            elif i == self.pool2_idx:\n",
    "                pool_outputs['pool2'] = x # (N, 128, H/4, W/4)\n",
    "            elif i == self.pool3_idx:\n",
    "                pool_outputs['pool3'] = x # (N, 256, H/8, W/8)\n",
    "            elif i == self.pool4_idx:\n",
    "                pool_outputs['pool4'] = x # (N, 512, H/8, W/8) (due to stride=1 modification)\n",
    "        \n",
    "        # x at this point is the output of the modified VGG features (after pool5, also H/8, W/8).\n",
    "        # This is the feature map that would normally go into the classifier.\n",
    "\n",
    "        # --- 3.6. Forward Pass through Converted FC Layers (Main Path) ---\n",
    "        # These are the former fully-connected layers, now acting as convolutional layers.\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu7(x)\n",
    "        # x is now the deep, high-level feature map (N, 4096, H/8, W/8).\n",
    "\n",
    "        # --- 3.7. Forward Pass through Multi-Scale Branches ---\n",
    "        # Process each stored pooling output through its respective MLPBranch.\n",
    "        # Also, upsample their outputs to the target resolution (H/8, W/8) for concatenation.\n",
    "        # Note: pool3 and pool4 are already at H/8, W/8, so no upsampling is needed for them.\n",
    "        \n",
    "        # Branch for pool1 (H/2, W/2) -> Needs 4x upsampling\n",
    "        branch_features.append(\n",
    "            F.interpolate(self.branch1(pool_outputs['pool1']), size=(input_H // 8, input_W // 8),\n",
    "                          mode='bilinear', align_corners=False)\n",
    "        )\n",
    "        # Explanation: F.interpolate is used for upsampling.\n",
    "        # size=(H//8, W//8): Specifies the target spatial dimensions (1/8th of input resolution).\n",
    "        # mode='bilinear': Uses bilinear interpolation, a common method for smooth resizing.\n",
    "        #                  It's a weighted average of the 4 nearest pixels.\n",
    "        # align_corners=False: This is crucial for consistency in deep learning frameworks.\n",
    "        #                      It avoids off-by-one pixel issues when upsampling/downsampling multiple times.\n",
    "        #                      True means the corner pixels of the input and output tensors are aligned.\n",
    "        #                      False means the pixel centers are aligned, which is generally preferred for feature maps.\n",
    "\n",
    "        # Branch for pool2 (H/4, W/4) -> Needs 2x upsampling\n",
    "        branch_features.append(\n",
    "            F.interpolate(self.branch2(pool_outputs['pool2']), size=(input_H // 8, input_W // 8),\n",
    "                          mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "        # Branch for pool3 (H/8, W/8) -> Already at target resolution, but still pass through MLPBranch\n",
    "        branch_features.append(self.branch3(pool_outputs['pool3']))\n",
    "\n",
    "        # Branch for pool4 (H/8, W/8, due to stride=1) -> Already at target resolution, but still pass through MLPBranch\n",
    "        branch_features.append(self.branch4(pool_outputs['pool4']))\n",
    "\n",
    "        # --- 3.8. Concatenate All Features ---\n",
    "        # Concatenate the deep main path features (x) with all processed multi-scale branch features.\n",
    "        # dim=1: Specifies concatenation along the channel dimension (axis=1 in PyTorch tensors).\n",
    "        # This creates a combined feature map with (4096 + 4*128) channels,\n",
    "        # providing both high-level semantic context and fine-grained spatial details.\n",
    "        x_concat = torch.cat([x] + branch_features, dim=1)\n",
    "\n",
    "        # --- 3.9. Final Classification from Concatenated Features ---\n",
    "        # Pass the concatenated feature map through the final classification head.\n",
    "        # This is a 1x1 convolution that projects the combined features to per-pixel class logits.\n",
    "        logits = self.final_classifier_msc(x_concat) # (N, num_classes, H/8, W/8)\n",
    "\n",
    "        # --- 3.10. Final Upsampling to Original Input Resolution ---\n",
    "        # The logits are currently at 1/8th resolution. For pixel-level segmentation output\n",
    "        # (which serves as unary potential for CRF), they need to be upsampled to the original input size.\n",
    "        # This is the final output of the DCNN part of DeepLab-MSc-CRF, ready for CRF.\n",
    "        output = F.interpolate(logits, size=(input_H, input_W), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Define the number of semantic classes (e.g., 21 for PASCAL VOC).\n",
    "    num_classes = 21\n",
    "\n",
    "    # Instantiate the DeepLabMSc model.\n",
    "    print(f\"Initializing DeepLabMSc model with {num_classes} classes...\")\n",
    "    model = DeepLabMSc(num_classes=num_classes)\n",
    "    # Move the model to GPU if available for faster computation.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Set the model to evaluation mode. This disables dropout and batch normalization updates,\n",
    "    # important for consistent inference.\n",
    "    model.eval()\n",
    "\n",
    "    # Create a dummy input image tensor.\n",
    "    # Batch size = 1, Channels = 3 (RGB), Height = 224, Width = 224 (typical VGG input size).\n",
    "    # This input will be processed by the model.\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "    # Perform a forward pass (inference).\n",
    "    print(\"Performing forward pass...\")\n",
    "    with torch.no_grad(): # Disable gradient calculations for inference, saves memory and speeds up.\n",
    "        output_logits = model(dummy_input)\n",
    "\n",
    "    # Print the shape of the output tensor.\n",
    "    # It should be (Batch_Size, Num_Classes, Original_Height, Original_Width).\n",
    "    print(f\"Output logits shape: {output_logits.shape}\")\n",
    "\n",
    "    # Proof of Concept: Output Resolution\n",
    "    expected_output_shape = (1, num_classes, 224, 224)\n",
    "    assert output_logits.shape == expected_output_shape, \\\n",
    "        f\"Output shape mismatch! Expected {expected_output_shape}, got {output_logits.shape}\"\n",
    "    print(\"Output shape matches expected full resolution.\")\n",
    "\n",
    "    # --- Illustrative Example of Feature Map Sizes at Each Stage ---\n",
    "    print(\"\\n--- Illustrative Feature Map Sizes (Conceptual Flow) ---\")\n",
    "    # This is a conceptual walkthrough to show the impact of DeepLab modifications on resolution.\n",
    "    # Actual sizes will vary with input image size.\n",
    "    \n",
    "    # Original VGG stages for a 224x224 input:\n",
    "    # After conv1_x (no pool): 224x224\n",
    "    # After pool1: 112x112 (1/2) - used for branch1\n",
    "    # After pool2: 56x56 (1/4) - used for branch2\n",
    "    # After pool3: 28x28 (1/8) - used for branch3\n",
    "    # After conv4_x (no pool4 due to stride=1): 28x28 (1/8)\n",
    "    # After pool4 (stride=1): 28x28 (1/8) - used for branch4\n",
    "    # After conv5_x (no pool5 due to stride=1, with dilation=2): 28x28 (1/8)\n",
    "    # After pool5 (stride=1): 28x28 (1/8)\n",
    "    # After conv6 (with dilation=4): 28x28 (1/8)\n",
    "    # After conv7 (1x1 conv): 28x28 (1/8)\n",
    "    # Concatenated features: 28x28 (1/8)\n",
    "    # Final logits: 28x28 (1/8) before final upsampling\n",
    "\n",
    "    print(f\"Input: {dummy_input.shape[2]}x{dummy_input.shape[3]}\")\n",
    "    print(f\"pool1 output (raw): {dummy_input.shape[2]//2}x{dummy_input.shape[3]//2}\")\n",
    "    print(f\"pool2 output (raw): {dummy_input.shape[2]//4}x{dummy_input.shape[3]//4}\")\n",
    "    print(f\"pool3 output (raw): {dummy_input.shape[2]//8}x{dummy_input.shape[3]//8}\")\n",
    "    print(f\"pool4 output (modified VGG): {dummy_input.shape[2]//8}x{dummy_input.shape[3]//8} (stride 1)\")\n",
    "    print(f\"Main path features (after conv7): {dummy_input.shape[2]//8}x{dummy_input.shape[3]//8}\")\n",
    "    \n",
    "    print(f\"Branch1 output (after MLPBranch, before upsample): {dummy_input.shape[2]//2}x{dummy_input.shape[3]//2}\")\n",
    "    print(f\"Branch1 output (after upsample for concat): {dummy_input.shape[2]//8}x{dummy_input.shape[3]//8}\")\n",
    "    # Similar for other branches.\n",
    "\n",
    "    print(f\"Final concatenated features: {output_logits.shape[2]//(224//dummy_input.shape[2])}x{output_logits.shape[3]//(224//dummy_input.shape[3])}\") # Correct for arbitrary input size for 1/8 output.\n",
    "    print(f\"Final output logits (after final upsample): {output_logits.shape[2]}x{output_logits.shape[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
