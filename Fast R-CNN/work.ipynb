{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f014b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2 # OpenCV for image loading/resizing (example dependency)\n",
    "import os # For file path handling (example dependency)\n",
    "# Assume external libraries for XML/JSON parsing, proposal loading, IoU calculation, NMS\n",
    "\n",
    "# --- Utility Functions (Conceptual - Needs Implementation) ---\n",
    "\n",
    "def calculate_iou(box_a, boxes_b):\n",
    "    \"\"\"Calculates IoU between box_a and multiple boxes_b.\n",
    "    Args:\n",
    "        box_a (tensor): Single box [x1, y1, x2, y2]\n",
    "        boxes_b (tensor): Multiple boxes [N, 4]\n",
    "    Returns:\n",
    "        tensor: IoU values [N,]\n",
    "    \"\"\"\n",
    "    # Placeholder - Requires implementation based on box format\n",
    "    # (e.g., using torchvision.ops.box_iou if boxes are [N, 4])\n",
    "    # Ensure correct format and device handling\n",
    "    # Example logic:\n",
    "    x_a = torch.max(box_a[0], boxes_b[:, 0])\n",
    "    y_a = torch.max(box_a[1], boxes_b[:, 1])\n",
    "    x_b = torch.min(box_a[2], boxes_b[:, 2])\n",
    "    y_b = torch.min(box_a[3], boxes_b[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(x_b - x_a + 1e-6, min=0) * torch.clamp(y_b - y_a + 1e-6, min=0)\n",
    "\n",
    "    box_a_area = (box_a[2] - box_a[0] + 1e-6) * (box_a[3] - box_a[1] + 1e-6)\n",
    "    box_b_area = (boxes_b[:, 2] - boxes_b[:, 0] + 1e-6) * (boxes_b[:, 3] - boxes_b[:, 1] + 1e-6)\n",
    "\n",
    "    iou = inter_area / (box_a_area + box_b_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def calculate_regression_targets(proposal, gt_box):\n",
    "    \"\"\"Calculates ground truth regression targets (v_x, v_y, v_w, v_h).\n",
    "    Args:\n",
    "        proposal (tensor): Proposal box [x1, y1, x2, y2]\n",
    "        gt_box (tensor): Ground truth box [x1, y1, x2, y2]\n",
    "    Returns:\n",
    "        tensor: Targets [4,] (tx, ty, tw, th)\n",
    "    \"\"\"\n",
    "    # Placeholder - Requires implementation matching paper's parameterization\n",
    "    # Assumes input boxes are [x1, y1, x2, y2]\n",
    "    # Convert to Px, Py, Pw, Ph and Gx, Gy, Gw, Gh if needed\n",
    "    prop_w = proposal[2] - proposal[0] + 1e-6\n",
    "    prop_h = proposal[3] - proposal[1] + 1e-6\n",
    "    prop_cx = proposal[0] + 0.5 * prop_w\n",
    "    prop_cy = proposal[1] + 0.5 * prop_h\n",
    "\n",
    "    gt_w = gt_box[2] - gt_box[0] + 1e-6\n",
    "    gt_h = gt_box[3] - gt_box[1] + 1e-6\n",
    "    gt_cx = gt_box[0] + 0.5 * gt_w\n",
    "    gt_cy = gt_box[1] + 0.5 * gt_h\n",
    "\n",
    "    vx = (gt_cx - prop_cx) / prop_w\n",
    "    vy = (gt_cy - prop_cy) / prop_h\n",
    "    vw = torch.log(gt_w / prop_w)\n",
    "    vh = torch.log(gt_h / prop_h)\n",
    "    return torch.tensor([vx, vy, vw, vh], dtype=torch.float32)\n",
    "\n",
    "def apply_regression_offsets(proposal, offsets):\n",
    "    \"\"\"Applies predicted offsets to a proposal box.\n",
    "    Args:\n",
    "        proposal (tensor): Proposal box [x1, y1, x2, y2]\n",
    "        offsets (tensor): Predicted offsets [4,] (tx, ty, tw, th)\n",
    "    Returns:\n",
    "        tensor: Refined box [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    # Placeholder - Inverse of calculate_regression_targets\n",
    "    prop_w = proposal[2] - proposal[0] + 1e-6\n",
    "    prop_h = proposal[3] - proposal[1] + 1e-6\n",
    "    prop_cx = proposal[0] + 0.5 * prop_w\n",
    "    prop_cy = proposal[1] + 0.5 * prop_h\n",
    "\n",
    "    tx, ty, tw, th = offsets\n",
    "\n",
    "    pred_cx = prop_w * tx + prop_cx\n",
    "    pred_cy = prop_h * ty + prop_cy\n",
    "    pred_w = prop_w * torch.exp(tw)\n",
    "    pred_h = prop_h * torch.exp(th)\n",
    "\n",
    "    pred_x1 = pred_cx - 0.5 * pred_w\n",
    "    pred_y1 = pred_cy - 0.5 * pred_h\n",
    "    pred_x2 = pred_cx + 0.5 * pred_w\n",
    "    pred_y2 = pred_cy + 0.5 * pred_h\n",
    "    return torch.stack([pred_x1, pred_y1, pred_x2, pred_y2])\n",
    "\n",
    "\n",
    "def non_maximum_suppression(boxes, scores, iou_threshold):\n",
    "    \"\"\"Performs Non-Maximum Suppression.\n",
    "    Args:\n",
    "        boxes (tensor): Boxes [N, 4] for a specific class.\n",
    "        scores (tensor): Scores [N,] for the boxes.\n",
    "        iou_threshold (float): IoU threshold for suppression.\n",
    "    Returns:\n",
    "        tensor: Indices of boxes to keep.\n",
    "    \"\"\"\n",
    "    # Placeholder - Requires implementation\n",
    "    # Can use torchvision.ops.nms\n",
    "    # Example: keep_indices = torchvision.ops.nms(boxes, scores, iou_threshold)\n",
    "    # return keep_indices\n",
    "    # Simplified greedy NMS logic:\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1e-6) * (y2 - y1 + 1e-6)\n",
    "    order = scores.argsort(descending=True)\n",
    "\n",
    "    keep = []\n",
    "    while order.numel() > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        if order.numel() == 1:\n",
    "            break\n",
    "\n",
    "        # Calculate IoU of the current box with remaining boxes\n",
    "        xx1 = torch.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = torch.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = torch.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = torch.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = torch.clamp(xx2 - xx1 + 1e-6, min=0.0)\n",
    "        h = torch.clamp(yy2 - yy1 + 1e-6, min=0.0)\n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        # Keep boxes with IoU below the threshold\n",
    "        inds = torch.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1] # +1 because we compare with order[1:]\n",
    "\n",
    "    return torch.tensor(keep, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"Conceptual Dataset for loading images, annotations, and proposals.\"\"\"\n",
    "    def __init__(self, image_dir, annotation_dir, proposal_dir, # Basic directories required\n",
    "                 num_classes, class_map, # Class variables\n",
    "                 target_scale=600, max_scale=1000, # Image process variable\n",
    "                 use_random_scale=False, scales=(480, 576, 688, 864, 1200), # To use random scale or not.\n",
    "                 use_flip=True): #To use flip or not\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.proposal_dir = proposal_dir # Assume proposals are pre-computed\n",
    "        self.num_classes = num_classes\n",
    "        self.class_map = class_map # e.g., {'background': 0, 'car': 1, ...}\n",
    "        self.target_scale = target_scale # For single-scale processing\n",
    "        self.max_scale = max_scale\n",
    "        self.use_random_scale = use_random_scale # For multi-scale augmentation\n",
    "        self.scales = scales\n",
    "        self.use_flip = use_flip\n",
    "\n",
    "        self.image_ids = self._load_image_ids() # Function to list image identifiers\n",
    "\n",
    "        # Standard ImageNet normalization\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # --- IMPORTANT: Add BBox Target Normalization ---\n",
    "        # self.bbox_means, self.bbox_stds = self._calculate_bbox_stats() # Needs separate pass over dataset\n",
    "        # Dummy values for now - REPLACE WITH ACTUAL STATS\n",
    "        self.bbox_means = torch.tensor([0.0] * 4)\n",
    "        self.bbox_stds = torch.tensor([1.0] * 4) # Using 1.0 means no normalization initially\n",
    "\n",
    "\n",
    "    def _load_image_ids(self):\n",
    "        # Placeholder: Scan annotation_dir or use a predefined list\n",
    "        # Returns a list of unique identifiers (e.g., filenames without extension)\n",
    "        # Example: return sorted([f.split('.')[0] for f in os.listdir(self.annotation_dir)])\n",
    "        return [\"image_001\", \"image_002\"] # Dummy IDs\n",
    "\n",
    "    def _load_annotation(self, image_id):\n",
    "        # Placeholder: Load and parse annotation file (e.g., XML for VOC)\n",
    "        # Should return:\n",
    "        #   - gt_boxes (tensor): [N_gt, 4] ground truth boxes [x1, y1, x2, y2]\n",
    "        #   - gt_labels (tensor): [N_gt,] ground truth class labels (integer indices)\n",
    "        # Example:\n",
    "        # xml_path = os.path.join(self.annotation_dir, f\"{image_id}.xml\")\n",
    "        # boxes, labels = parse_voc_xml(xml_path, self.class_map)\n",
    "        # return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "        # Dummy annotation:\n",
    "        gt_boxes = torch.tensor([[50, 50, 250, 250], [300, 100, 400, 200]], dtype=torch.float32)\n",
    "        gt_labels = torch.tensor([1, 2], dtype=torch.long) # Assume class 1 and 2 exist\n",
    "        return gt_boxes, gt_labels\n",
    "\n",
    "    def _load_proposals(self, image_id):\n",
    "        # Placeholder: Load pre-computed proposals (e.g., from a .mat or .txt file)\n",
    "        # Should return:\n",
    "        #   - proposals (tensor): [N_prop, 4] proposal boxes [x1, y1, x2, y2]\n",
    "        # Example:\n",
    "        # prop_path = os.path.join(self.proposal_dir, f\"{image_id}_proposals.txt\")\n",
    "        # proposals = load_proposals_from_file(prop_path)\n",
    "        # return torch.tensor(proposals, dtype=torch.float32)\n",
    "        # Dummy proposals:\n",
    "        proposals = torch.randint(0, 300, (2000, 4)).float() # ~2000 proposals\n",
    "        proposals[:, 2] += proposals[:, 0] + 50 # Ensure x2 > x1\n",
    "        proposals[:, 3] += proposals[:, 1] + 50 # Ensure y2 > y1\n",
    "        return proposals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "\n",
    "        # 1. Load Image\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.jpg\") # Assuming jpg\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "        H_orig, W_orig, _ = img.shape\n",
    "\n",
    "        # 2. Load Annotations and Proposals\n",
    "        gt_boxes, gt_labels = self._load_annotation(image_id)\n",
    "        proposals = self._load_proposals(image_id)\n",
    "\n",
    "        # 3. Data Augmentation: Flipping\n",
    "        apply_flip = False\n",
    "        if self.use_flip and torch.rand(1) < 0.5:\n",
    "            apply_flip = True\n",
    "            img = cv2.flip(img, 1) # Horizontal flip\n",
    "            # Flip boxes (x1' = W - x2, x2' = W - x1)\n",
    "            proposals_orig_x1 = proposals[:, 0].clone()\n",
    "            proposals[:, 0] = W_orig - proposals[:, 2]\n",
    "            proposals[:, 2] = W_orig - proposals_orig_x1\n",
    "            if gt_boxes.shape[0] > 0:\n",
    "                gt_boxes_orig_x1 = gt_boxes[:, 0].clone()\n",
    "                gt_boxes[:, 0] = W_orig - gt_boxes[:, 2]\n",
    "                gt_boxes[:, 2] = W_orig - gt_boxes_orig_x1\n",
    "\n",
    "        # 4. Data Augmentation / Preprocessing: Rescaling\n",
    "        scale = self.target_scale\n",
    "        if self.use_random_scale: # Choose a random scale if training augmentation is enabled\n",
    "             scale = np.random.choice(self.scales)\n",
    "\n",
    "        min_size = scale\n",
    "        max_size = self.max_scale\n",
    "        im_size_min = min(H_orig, W_orig)\n",
    "        im_size_max = max(H_orig, W_orig)\n",
    "        im_scale = float(min_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > max_size:\n",
    "            im_scale = float(max_size) / float(im_size_max)\n",
    "\n",
    "        # Resize image using OpenCV (can use PIL or torchvision.transforms too)\n",
    "        new_H = int(np.round(H_orig * im_scale))\n",
    "        new_W = int(np.round(W_orig * im_scale))\n",
    "        img_resized = cv2.resize(img, (new_W, new_H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Apply normalization and ToTensor transform\n",
    "        img_tensor = self.image_transform(img_resized)\n",
    "\n",
    "        # Rescale boxes (proposals and ground truth)\n",
    "        proposals_resized = proposals * im_scale\n",
    "        gt_boxes_resized = gt_boxes * im_scale\n",
    "\n",
    "        # 5. Assign Labels and Regression Targets to Proposals\n",
    "        # This is a complex step matching R-CNN/Fast R-CNN logic\n",
    "\n",
    "        if gt_boxes_resized.shape[0] > 0:\n",
    "            # Calculate IoU between all proposals and all GT boxes\n",
    "            ious = torch.zeros((proposals_resized.shape[0], gt_boxes_resized.shape[0]))\n",
    "            for i in range(proposals_resized.shape[0]):\n",
    "                 ious[i, :] = calculate_iou(proposals_resized[i], gt_boxes_resized)\n",
    "\n",
    "            # Find max IoU for each proposal and the corresponding GT box index\n",
    "            max_ious, gt_assignment = ious.max(dim=1)\n",
    "        else: # Handle images with no ground truth objects\n",
    "            max_ious = torch.zeros(proposals_resized.shape[0])\n",
    "            gt_assignment = torch.zeros(proposals_resized.shape[0], dtype=torch.long) -1 # Invalid assignment\n",
    "\n",
    "        # Assign labels based on IoU thresholds (as per paper section 2.3)\n",
    "        labels = torch.zeros(proposals_resized.shape[0], dtype=torch.long) # Default to background (0)\n",
    "        # Foreground: >= 0.5 IoU\n",
    "        fg_inds = torch.where(max_ious >= 0.5)[0]\n",
    "        if fg_inds.numel() > 0:\n",
    "            labels[fg_inds] = gt_labels[gt_assignment[fg_inds]]\n",
    "\n",
    "        # Background: [0.1, 0.5) IoU\n",
    "        bg_inds = torch.where((max_ious >= 0.1) & (max_ious < 0.5))[0]\n",
    "        labels[bg_inds] = 0 # Explicitly set to background\n",
    "\n",
    "        # Calculate regression targets ONLY for foreground proposals\n",
    "        bbox_targets = torch.zeros((proposals_resized.shape[0], 4), dtype=torch.float32)\n",
    "        if fg_inds.numel() > 0:\n",
    "            assigned_gt_boxes = gt_boxes_resized[gt_assignment[fg_inds]]\n",
    "            foreground_proposals = proposals_resized[fg_inds]\n",
    "            # Calculate targets\n",
    "            targets_raw = torch.zeros_like(foreground_proposals)\n",
    "            for i in range(foreground_proposals.shape[0]):\n",
    "                 targets_raw[i,:] = calculate_regression_targets(foreground_proposals[i], assigned_gt_boxes[i])\n",
    "\n",
    "            # --- Normalize targets ---\n",
    "            targets_normalized = (targets_raw - self.bbox_means.to(targets_raw.device)) / self.bbox_stds.to(targets_raw.device)\n",
    "            bbox_targets[fg_inds, :] = targets_normalized\n",
    "\n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'proposals': proposals_resized, # Keep resized proposals for potential use\n",
    "            'labels': labels,\n",
    "            'bbox_targets': bbox_targets,\n",
    "            'image_id': image_id # Keep track for debugging/evaluation\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn_fast_rcnn(batch, R=128, N=2, num_classes=20):\n",
    "    \"\"\"Custom collate function for hierarchical sampling.\"\"\"\n",
    "    # batch: A list of dictionaries, where each dict is the output of __getitem__\n",
    "\n",
    "    # 1. Select N images for the batch\n",
    "    num_images_in_batch = min(N, len(batch)) # Handle cases where batch size < N\n",
    "    selected_indices = np.random.choice(len(batch), num_images_in_batch, replace=False)\n",
    "    selected_batch = [batch[i] for i in selected_indices]\n",
    "\n",
    "    images_list = []\n",
    "    rois_list = []\n",
    "    labels_list = []\n",
    "    bbox_targets_list = []\n",
    "\n",
    "    rois_per_image = R // num_images_in_batch\n",
    "    fg_rois_per_image = int(np.round(0.25 * rois_per_image)) # 25% foreground\n",
    "    bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "\n",
    "    for i in range(num_images_in_batch):\n",
    "        data = selected_batch[i]\n",
    "        images_list.append(data['image'])\n",
    "\n",
    "        proposals = data['proposals']\n",
    "        labels = data['labels']\n",
    "        bbox_targets = data['bbox_targets']\n",
    "\n",
    "        # Separate foreground and background proposals based on labels\n",
    "        fg_inds = torch.where(labels > 0)[0]\n",
    "        bg_inds = torch.where(labels == 0)[0]\n",
    "\n",
    "        # Sample foreground RoIs\n",
    "        if fg_inds.numel() > fg_rois_per_image:\n",
    "            fg_inds_sampled = np.random.choice(fg_inds.numpy(), size=fg_rois_per_image, replace=False)\n",
    "        elif fg_inds.numel() > 0: # Sample with replacement if not enough\n",
    "             fg_inds_sampled = np.random.choice(fg_inds.numpy(), size=fg_rois_per_image, replace=True)\n",
    "        else: # Handle no foreground objects\n",
    "             fg_inds_sampled = np.array([], dtype=np.int64)\n",
    "\n",
    "        # Sample background RoIs\n",
    "        num_bg_needed = rois_per_image - fg_inds_sampled.shape[0] # Adjust if fewer fg found\n",
    "        if bg_inds.numel() > num_bg_needed:\n",
    "            bg_inds_sampled = np.random.choice(bg_inds.numpy(), size=num_bg_needed, replace=False)\n",
    "        elif bg_inds.numel() > 0: # Sample with replacement\n",
    "            bg_inds_sampled = np.random.choice(bg_inds.numpy(), size=num_bg_needed, replace=True)\n",
    "        else: # Handle no background objects (rare)\n",
    "             bg_inds_sampled = np.array([], dtype=np.int64)\n",
    "\n",
    "        # Combine sampled indices\n",
    "        keep_inds = np.concatenate([fg_inds_sampled, bg_inds_sampled])\n",
    "        if keep_inds.size == 0: continue # Skip image if no RoIs sampled\n",
    "\n",
    "        # Select corresponding proposals, labels, targets\n",
    "        sampled_proposals = proposals[keep_inds]\n",
    "        sampled_labels = labels[keep_inds]\n",
    "        sampled_bbox_targets = bbox_targets[keep_inds]\n",
    "\n",
    "        # Create RoI tensor with batch index (format: batch_idx, x1, y1, x2, y2)\n",
    "        batch_idx_tensor = torch.full((sampled_proposals.shape[0], 1), i, dtype=torch.float32)\n",
    "        rois_for_image = torch.cat([batch_idx_tensor, sampled_proposals], dim=1)\n",
    "\n",
    "        rois_list.append(rois_for_image)\n",
    "        labels_list.append(sampled_labels)\n",
    "        bbox_targets_list.append(sampled_bbox_targets)\n",
    "\n",
    "    # Stack images and concatenate RoIs, labels, targets\n",
    "    images = torch.stack(images_list, 0)\n",
    "    if not rois_list: # Handle cases where no valid RoIs were sampled across batch\n",
    "        # Return dummy tensors or raise error, depends on desired behavior\n",
    "        return images, torch.empty((0,5)), torch.empty((0,)), torch.empty((0,4))\n",
    "\n",
    "    rois = torch.cat(rois_list, 0)\n",
    "    labels = torch.cat(labels_list, 0)\n",
    "    bbox_targets = torch.cat(bbox_targets_list, 0)\n",
    "\n",
    "    # --- Map RoIs to Feature Map Scale ---\n",
    "    # This depends on the backbone's total stride S (e.g., 16 for VGG/ResNet default)\n",
    "    # stride = 16.0\n",
    "    # rois[:, 1:] /= stride # Divide x1, y1, x2, y2 by stride\n",
    "    # Note: This mapping should technically happen *inside* the model or just before\n",
    "    # RoI pooling if the stride is fixed. Passing image-scale RoIs might be cleaner\n",
    "    # and mapping happens relative to the computed feature map size dynamically.\n",
    "    # Let's assume image-scale RoIs are passed and mapping happens later.\n",
    "\n",
    "    # --- Prepare bbox_targets for Smooth L1 Loss ---\n",
    "    # The loss function needs targets expanded per class.\n",
    "    # We only have targets for the GT class. We need a tensor [N_roi, num_classes * 4]\n",
    "    # where targets are placed at indices corresponding to labels[roi]\n",
    "    # and the rest are ignored (e.g., zeroed, masked out in loss).\n",
    "    # This part is complex and often handled within the loss calculation itself\n",
    "    # by selecting the correct predicted offsets based on the true label 'u'.\n",
    "    # For simplicity here, we pass the [N_roi, 4] targets and the labels,\n",
    "    # assuming the loss function handles the selection.\n",
    "\n",
    "    return images, rois, labels, bbox_targets\n",
    "\n",
    "\n",
    "# --- Backbone Network Definitions (VGG16 and ResNet50 as before) ---\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = None\n",
    "        self.out_channels = None\n",
    "        self.output_size = None\n",
    "        self.stride = 16.0 # Default stride assumption\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class VGG16(Backbone):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        vgg16 = models.vgg16(pretrained=pretrained)\n",
    "        # Remove final max pool layer from features to get output of conv5_3\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children())[:-1])\n",
    "        self.out_channels = 512\n",
    "        self.output_size = (7, 7)\n",
    "        self.stride = 16.0\n",
    "\n",
    "        # Keep original classifier structure up to fc7 (or equivalent)\n",
    "        # Remove avgpool and the final classifier layer (fc8)\n",
    "        self.roi_head_feature_extractor = nn.Sequential(\n",
    "            nn.Linear(self.out_channels * self.output_size[0] * self.output_size[1], 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        # Initialize new FC layers properly (example)\n",
    "        for layer in self.roi_head_feature_extractor:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.features(x)\n",
    "        # Note: We don't run the fc layers here, that happens *after* RoI pooling\n",
    "        return feature_map # Only return the feature map\n",
    "\n",
    "\n",
    "class ResNet50(Backbone):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Use layers up to the end of the last conv block (e.g., layer4)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
    "            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "        )\n",
    "        self.out_channels = 2048 # Output channels of layer4\n",
    "        self.output_size = (7, 7) # Often use 7x7 RoI pool for ResNet heads too\n",
    "        self.stride = 16.0 # Note: Stride calculation might be more complex depending on exact ResNet variant\n",
    "\n",
    "        # Define the RoI head feature extractor (replaces avgpool and fc)\n",
    "        # Often uses AdaptiveAvgPool + FC layer(s)\n",
    "        self.roi_head_feature_extractor = nn.Sequential(\n",
    "             nn.AdaptiveAvgPool2d(self.output_size), # Pool to desired size first? Alternative is AdaptiveAvgPool(1) then FC\n",
    "             nn.Flatten(), # Flatten before FC\n",
    "             nn.Linear(self.out_channels * self.output_size[0] * self.output_size[1], 1024), # Example intermediate size\n",
    "             nn.ReLU(True),\n",
    "             # Can add more FC layers here if needed, e.g., mapping to 4096 like VGG\n",
    "             nn.Linear(1024, 4096), # Map to 4096 for consistency with sibling heads\n",
    "             nn.ReLU(True)\n",
    "             # No dropout needed if only used internally before final heads? Check common practice.\n",
    "        )\n",
    "        # Initialize new layers\n",
    "        for layer in self.roi_head_feature_extractor:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') # He initialization common for ResNet\n",
    "                 nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.features(x)\n",
    "        # The RoI head feature extractor will be applied *after* RoI pooling\n",
    "        return feature_map\n",
    "\n",
    "\n",
    "# --- RoI Pooling Layer (Simplified - using torchvision.ops) ---\n",
    "# Using torchvision's RoIPool is much more robust and efficient\n",
    "from torchvision.ops import RoIPool as TorchVisionRoIPool\n",
    "\n",
    "class RoIPoolWrapper(nn.Module):\n",
    "    def __init__(self, output_size, spatial_scale):\n",
    "        super().__init__()\n",
    "        # Note: torchvision RoIPool expects output_size as (h, w) tuple\n",
    "        # spatial_scale is 1.0 / stride\n",
    "        self.roi_pool = TorchVisionRoIPool(output_size, spatial_scale)\n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        # rois need format [batch_idx, x1, y1, x2, y2] matching feature map scale\n",
    "        # The collate_fn provided rois with batch_idx and *image scale* coordinates\n",
    "        # We need to adjust here based on the feature map scale\n",
    "        # Assuming rois[:, 0] is batch index and rois[:, 1:] are IMAGE scale coords\n",
    "\n",
    "        # Correct application requires knowing feature map size relative to image\n",
    "        # This spatial_scale handles it.\n",
    "        return self.roi_pool(features, rois)\n",
    "\n",
    "\n",
    "# --- Fast R-CNN Model Definition (Revised) ---\n",
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, num_classes, backbone_name=\"vgg16\", pretrained=True, roi_output_size=(7, 7)):\n",
    "        super(FastRCNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Instantiate selected backbone\n",
    "        if backbone_name.lower() == \"vgg16\":\n",
    "            self.backbone = VGG16(pretrained=pretrained)\n",
    "        elif backbone_name.lower() == \"resnet50\":\n",
    "            self.backbone = ResNet50(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid backbone name.\")\n",
    "\n",
    "        # RoI Pooling Layer (using torchvision)\n",
    "        spatial_scale = 1.0 / self.backbone.stride\n",
    "        self.roi_pool = RoIPoolWrapper(output_size=roi_output_size, spatial_scale=spatial_scale)\n",
    "\n",
    "        # RoI Head Feature Extractor (from backbone definition)\n",
    "        self.roi_head_feature_extractor = self.backbone.roi_head_feature_extractor\n",
    "\n",
    "        # Sibling Output Layers\n",
    "        # Input dimension depends on the output of roi_head_feature_extractor\n",
    "        # Assuming it consistently outputs 4096 features for this example\n",
    "        feature_dim = 4096\n",
    "        self.cls_score_head = nn.Linear(feature_dim, num_classes + 1)\n",
    "        self.bbox_pred_head = nn.Linear(feature_dim, num_classes * 4)\n",
    "\n",
    "        # --- Initialize sibling layers ---\n",
    "        nn.init.normal_(self.cls_score_head.weight, std=0.01)\n",
    "        nn.init.constant_(self.cls_score_head.bias, 0)\n",
    "        nn.init.normal_(self.bbox_pred_head.weight, std=0.001)\n",
    "        nn.init.constant_(self.bbox_pred_head.bias, 0)\n",
    "\n",
    "    def forward(self, images, rois):\n",
    "        # images: (B, 3, H_img, W_img)\n",
    "        # rois: (N_roi, 5) [batch_idx, x1_img, y1_img, x2_img, y2_img] (IMAGE scale)\n",
    "\n",
    "        # 1. Get feature map from backbone\n",
    "        feature_map = self.backbone(images)\n",
    "\n",
    "        # 2. Perform RoI Pooling\n",
    "        # RoIPoolWrapper handles the spatial scaling internally\n",
    "        pooled_features = self.roi_pool(feature_map, rois) # Output: (N_roi, C, H_pool, W_pool)\n",
    "\n",
    "        # 3. Pass through RoI head feature extractor\n",
    "        # Reshape pooled features if necessary before FC layers\n",
    "        # The exact reshaping depends on roi_head_feature_extractor structure\n",
    "        # If it starts with FC layers (like VGG):\n",
    "        pooled_features_flat = torch.flatten(pooled_features, start_dim=1)\n",
    "        shared_roi_features = self.roi_head_feature_extractor(pooled_features_flat)\n",
    "        # If it starts with AvgPool (like ResNet example):\n",
    "        # shared_roi_features = self.roi_head_feature_extractor(pooled_features) # Assumes extractor handles input shape\n",
    "\n",
    "        # 4. Get final predictions from sibling heads\n",
    "        cls_score = self.cls_score_head(shared_roi_features)\n",
    "        bbox_pred_offsets = self.bbox_pred_head(shared_roi_features)\n",
    "\n",
    "        return cls_score, bbox_pred_offsets\n",
    "\n",
    "# --- Loss Function Definitions (Mostly Unchanged) ---\n",
    "def fast_rcnn_loss(cls_score, bbox_pred, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights, lambda_loc=1.0):\n",
    "    \"\"\"Calculates Fast R-CNN loss.\n",
    "    Args:\n",
    "        cls_score (tensor): [N_roi, K+1] class scores.\n",
    "        bbox_pred (tensor): [N_roi, K*4] predicted offsets.\n",
    "        labels (tensor): [N_roi] integer class labels (0 for bg).\n",
    "        bbox_targets (tensor): [N_roi, K*4] GT offsets (only relevant entries filled).\n",
    "        bbox_inside_weights (tensor): [N_roi, K*4] Mask for L_loc components (1 if relevant, 0 otherwise).\n",
    "        bbox_outside_weights (tensor): [N_roi, K*4] Balancing weight for L_loc (typically 1/N_roi for relevant).\n",
    "        lambda_loc (float): Balancing factor.\n",
    "    Returns:\n",
    "        tensor: Total loss.\n",
    "        tensor: Classification loss.\n",
    "        tensor: Localization loss.\n",
    "    \"\"\"\n",
    "    # Classification Loss (Cross Entropy)\n",
    "    loss_cls = F.cross_entropy(cls_score, labels)\n",
    "\n",
    "    # Localization Loss (Smooth L1)\n",
    "    # Select predictions and targets for foreground objects only\n",
    "    # This selection is now often done using the weights instead of direct indexing\n",
    "\n",
    "    # Calculate element-wise Smooth L1\n",
    "    loss_box_all = F.smooth_l1_loss(\n",
    "        bbox_pred * bbox_inside_weights, # Apply mask to predictions\n",
    "        bbox_targets * bbox_inside_weights, # Apply mask to targets\n",
    "        reduction='none' # Get element-wise loss\n",
    "    )\n",
    "\n",
    "    # Sum the loss across the 4 coordinates, weighted\n",
    "    loss_box = torch.sum(bbox_outside_weights * loss_box_all)\n",
    "\n",
    "    # Combine losses\n",
    "    loss = loss_cls + lambda_loc * loss_box\n",
    "\n",
    "    return loss, loss_cls, loss_box\n",
    "\n",
    "\n",
    "# --- Usage Instructions / Inference ---\n",
    "\n",
    "def detect_objects(model, image_path, proposal_loader, device,\n",
    "                   conf_threshold=0.7, nms_threshold=0.3,\n",
    "                   target_scale=600, max_scale=1000,\n",
    "                   bbox_means=None, bbox_stds=None, # For de-normalization\n",
    "                   num_classes=20, class_map_inv=None): # To map indices back to names\n",
    "    \"\"\"Performs object detection on a single image.\"\"\"\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    # 1. Load and Preprocess Image\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    H_orig, W_orig, _ = img_rgb.shape\n",
    "\n",
    "    # Rescale image (same logic as dataset __getitem__)\n",
    "    min_size = target_scale\n",
    "    max_size = max_scale\n",
    "    im_size_min = min(H_orig, W_orig)\n",
    "    im_size_max = max(H_orig, W_orig)\n",
    "    im_scale = float(min_size) / float(im_size_min)\n",
    "    if np.round(im_scale * im_size_max) > max_size:\n",
    "        im_scale = float(max_size) / float(im_size_max)\n",
    "    new_H = int(np.round(H_orig * im_scale))\n",
    "    new_W = int(np.round(W_orig * im_scale))\n",
    "    img_resized = cv2.resize(img_rgb, (new_W, new_H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Apply normalization\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = img_transform(img_resized).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "    # 2. Load/Generate Proposals\n",
    "    # proposals_img_scale: (N_prop, 4) [x1, y1, x2, y2] in original image coordinates\n",
    "    proposals_img_scale = proposal_loader(image_path) # Conceptual function\n",
    "    if proposals_img_scale is None or proposals_img_scale.shape[0] == 0:\n",
    "        print(\"No proposals found for image.\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Add batch index (0 for single image inference)\n",
    "    batch_idx_tensor = torch.zeros((proposals_img_scale.shape[0], 1), device=device)\n",
    "    rois_img_scale = torch.cat([batch_idx_tensor, torch.tensor(proposals_img_scale, device=device)], dim=1)\n",
    "\n",
    "    # 3. Run Model Forward Pass\n",
    "    with torch.no_grad():\n",
    "        cls_score, bbox_pred_offsets = model(img_tensor, rois_img_scale)\n",
    "\n",
    "    # 4. Post-process Outputs\n",
    "    probs = F.softmax(cls_score, dim=1) # Get probabilities (N_roi, K+1)\n",
    "    proposals_resized_scale = rois_img_scale[:, 1:] * im_scale # Rescale proposals to match resized image\n",
    "\n",
    "    # Use means/stds for de-normalization if provided\n",
    "    if bbox_means is None: bbox_means = torch.zeros(4, device=device)\n",
    "    if bbox_stds is None: bbox_stds = torch.ones(4, device=device)\n",
    "\n",
    "    final_boxes = []\n",
    "    final_scores = []\n",
    "    final_labels = []\n",
    "\n",
    "    # Iterate through each foreground class (skip background class 0)\n",
    "    for class_idx in range(1, num_classes + 1):\n",
    "        # Get scores for this class\n",
    "        class_scores = probs[:, class_idx]\n",
    "\n",
    "        # Get predicted offsets for this class\n",
    "        # Indices for class `j` are `j*4` to `j*4+3` (if bg is 0, adjust if bg is last class)\n",
    "        # Assuming K classes means indices 1 to K map to output offsets 0*4.. to (K-1)*4... Needs care!\n",
    "        # Let's assume class_idx 1 corresponds to first block of 4 offsets, etc.\n",
    "        offset_indices = slice((class_idx - 1) * 4, class_idx * 4) # Adjust if class indices differ\n",
    "        class_bbox_offsets = bbox_pred_offsets[:, offset_indices]\n",
    "\n",
    "        # Filter proposals by confidence threshold\n",
    "        keep_inds = torch.where(class_scores >= conf_threshold)[0]\n",
    "        if keep_inds.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        filtered_proposals = proposals_resized_scale[keep_inds]\n",
    "        filtered_scores = class_scores[keep_inds]\n",
    "        filtered_offsets = class_bbox_offsets[keep_inds]\n",
    "\n",
    "        # De-normalize offsets\n",
    "        filtered_offsets_denorm = (filtered_offsets * bbox_stds) + bbox_means\n",
    "\n",
    "        # Apply offsets to proposals to get refined boxes\n",
    "        refined_boxes = torch.zeros_like(filtered_proposals)\n",
    "        for i in range(filtered_proposals.shape[0]):\n",
    "            refined_boxes[i] = apply_regression_offsets(filtered_proposals[i], filtered_offsets_denorm[i])\n",
    "\n",
    "        # Clip boxes to image boundaries (of the *resized* image)\n",
    "        refined_boxes[:, 0::2].clamp_(min=0, max=new_W - 1) # x1, x2\n",
    "        refined_boxes[:, 1::2].clamp_(min=0, max=new_H - 1) # y1, y2\n",
    "\n",
    "        # Perform Non-Maximum Suppression (NMS)\n",
    "        keep_nms_inds = non_maximum_suppression(refined_boxes, filtered_scores, nms_threshold)\n",
    "        if keep_nms_inds.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Scale final boxes back to original image size\n",
    "        final_class_boxes = refined_boxes[keep_nms_inds] / im_scale\n",
    "        final_class_scores = filtered_scores[keep_nms_inds]\n",
    "        final_class_labels = torch.full_like(final_class_scores, class_idx, dtype=torch.long)\n",
    "\n",
    "        final_boxes.append(final_class_boxes)\n",
    "        final_scores.append(final_class_scores)\n",
    "        final_labels.append(final_class_labels)\n",
    "\n",
    "    if not final_boxes:\n",
    "        return [], [], []\n",
    "\n",
    "    # Concatenate results across all classes\n",
    "    final_boxes = torch.cat(final_boxes, dim=0)\n",
    "    final_scores = torch.cat(final_scores, dim=0)\n",
    "    final_labels = torch.cat(final_labels, dim=0)\n",
    "\n",
    "    # Optional: Map labels back to names\n",
    "    if class_map_inv:\n",
    "        final_label_names = [class_map_inv[label.item()] for label in final_labels]\n",
    "        return final_boxes.cpu().numpy(), final_scores.cpu().numpy(), final_label_names\n",
    "    else:\n",
    "        return final_boxes.cpu().numpy(), final_scores.cpu().numpy(), final_labels.cpu().numpy()\n",
    "\n",
    "\n",
    "# --- Example Usage Outline ---\n",
    "\n",
    "# 1. Setup (Paths, Classes, etc.)\n",
    "# IMAGE_DIR = \"path/to/your/images\"\n",
    "# ANNOTATION_DIR = \"path/to/your/annotations\"\n",
    "# PROPOSAL_DIR = \"path/to/your/proposals\"\n",
    "# NUM_CLASSES = 20 # e.g., PASCAL VOC\n",
    "# CLASS_MAP = {'background': 0, 'aeroplane': 1, ...} # Map class names to integers\n",
    "# CLASS_MAP_INV = {v: k for k, v in CLASS_MAP.items()} # For inference output\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Create Dataset and DataLoader (for Training)\n",
    "# train_dataset = ObjectDetectionDataset(IMAGE_DIR, ANNOTATION_DIR, PROPOSAL_DIR, NUM_CLASSES, CLASS_MAP, use_flip=True, use_random_scale=True)\n",
    "# # Note: Need robust collate_fn implementation\n",
    "# train_loader = DataLoader(train_dataset, batch_size=None, # Batch size handled by collate_fn logic implicitly via N\n",
    "#                           num_workers=4, collate_fn=lambda batch: collate_fn_fast_rcnn(batch, R=128, N=2, num_classes=NUM_CLASSES))\n",
    "\n",
    "# 3. Initialize Model and Optimizer (for Training)\n",
    "# model = FastRCNN(num_classes=NUM_CLASSES, backbone_name=\"vgg16\", pretrained=True).to(DEVICE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30000, gamma=0.1) # Example scheduler\n",
    "\n",
    "# 4. Training Loop (Conceptual)\n",
    "# num_iterations = 40000 # e.g., 30k + 10k\n",
    "# model.train()\n",
    "# for i, batch_data in enumerate(train_loader):\n",
    "#     if i >= num_iterations: break\n",
    "#     images, rois, labels, bbox_targets = batch_data\n",
    "#     images = images.to(DEVICE)\n",
    "#     rois = rois.to(DEVICE)\n",
    "#     labels = labels.to(DEVICE)\n",
    "#     bbox_targets = bbox_targets.to(DEVICE) # Needs proper expansion/masking based on labels for loss calculation\n",
    "\n",
    "     # --- Calculate loss weights (conceptual) ---\n",
    "#     bbox_inside_weights = torch.zeros_like(bbox_targets.repeat(1, NUM_CLASSES)) # Placeholder for masking L_loc\n",
    "#     bbox_outside_weights = torch.zeros_like(bbox_targets.repeat(1, NUM_CLASSES))# Placeholder for balancing L_loc\n",
    "     # Populate weights based on labels - complex logic needed here\n",
    "     # Correct calculation of these weights is crucial for the loss function.\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     cls_score, bbox_pred = model(images, rois)\n",
    "\n",
    "#     loss, loss_cls, loss_loc = fast_rcnn_loss(\n",
    "#         cls_score, bbox_pred, labels,\n",
    "#         bbox_targets, # Pass appropriately structured/expanded targets\n",
    "#         bbox_inside_weights, bbox_outside_weights, # Pass weights\n",
    "#         lambda_loc=1.0\n",
    "#     )\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "#     if i % 100 == 0: # Logging example\n",
    "#         print(f\"Iter: {i}, Loss: {loss.item():.4f}, Loss Cls: {loss_cls.item():.4f}, Loss Loc: {loss_loc.item():.4f}\")\n",
    "\n",
    "     # Add validation, checkpointing etc.\n",
    "\n",
    "# 5. Inference / Detection (Example)\n",
    "# Load trained model weights\n",
    "# model.load_state_dict(torch.load(\"path/to/trained_model.pth\"))\n",
    "# model.eval()\n",
    "# model.to(DEVICE)\n",
    "\n",
    "# Define a conceptual proposal loader function\n",
    "# def my_proposal_loader(image_path):\n",
    "#    # Load proposals for this image_path (e.g., from Selective Search output)\n",
    "#    # Return numpy array [N_prop, 4]\n",
    "#    return np.random.rand(500, 4) * 200 # Dummy\n",
    "\n",
    "# Perform detection\n",
    "# try:\n",
    "#     boxes, scores, labels = detect_objects(\n",
    "#         model, \"path/to/test_image.jpg\", my_proposal_loader, DEVICE,\n",
    "#         conf_threshold=0.8, nms_threshold=0.3,\n",
    "#         # Pass actual bbox means/stds if normalization was used\n",
    "#         num_classes=NUM_CLASSES, class_map_inv=CLASS_MAP_INV\n",
    "#     )\n",
    "#     print(\"Detections:\")\n",
    "#     for box, score, label in zip(boxes, scores, labels):\n",
    "#         print(f\"  Label: {label}, Score: {score:.3f}, Box: {box}\")\n",
    "#     # Add visualization code here (draw boxes on image)\n",
    "# except FileNotFoundError as e:\n",
    "#     print(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
