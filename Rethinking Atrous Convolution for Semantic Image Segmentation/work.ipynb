{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a341b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5134bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, name='resnet101', pretrained=True, output_stride=8, freeze_bn=True):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        # Load the specified ResNet model from torchvision\n",
    "        if name == 'resnet101':\n",
    "            self.resnet = models.resnet101(pretrained=pretrained)\n",
    "        elif name == 'resnet50':\n",
    "            self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet backbone name.\")\n",
    "\n",
    "        self.output_stride = output_stride\n",
    "        self.freeze_bn = freeze_bn\n",
    "\n",
    "        # DeepLabv3 modification:\n",
    "        # Change strides and dilations for specific layers to control output_stride\n",
    "\n",
    "        # --- Layer 4 Modification (for output_stride = 16) ---\n",
    "        # The 'Bottleneck' block in torchvision's ResNet has the stride applied to conv2.\n",
    "        # Its 'downsample' branch also gets the same stride.\n",
    "        # To change output_stride from 32 to 16, we need to modify the first block of layer4.\n",
    "        # Original: layer4[0].conv2.stride = (2,2), layer4[0].downsample[0].stride = (2,2)\n",
    "        # New: layer4[0].conv2.stride = (1,1), layer4[0].downsample[0].stride = (1,1)\n",
    "        # Then, apply dilation to all convs in layer4 to compensate for the removed stride.\n",
    "\n",
    "        if self.output_stride == 16:\n",
    "            # Set stride of the first Bottleneck in layer4 to 1\n",
    "            # This makes the feature map size from layer3 (1/16) to layer4 (1/16)\n",
    "            self.resnet.layer4[0].conv2.stride = (1, 1)\n",
    "            self.resnet.layer4[0].downsample[0].stride = (1, 1)\n",
    "\n",
    "            # Apply dilation to all convolutional layers within layer4\n",
    "            # (conv2 and conv3 in each Bottleneck block).\n",
    "            # The original blocks typically have a padding of 1 for 3x3 convs (rate 1).\n",
    "            # For dilation r, padding should be r to maintain output size.\n",
    "            for i, block in enumerate(self.resnet.layer4):\n",
    "                # conv2 is the 3x3 convolution\n",
    "                block.conv2.dilation = (2, 2)\n",
    "                block.conv2.padding = (2, 2) # Padding = dilation rate for 3x3 kernel\n",
    "\n",
    "                # For the third convolution (conv3, 1x1), no dilation needed usually.\n",
    "                # However, if there are subsequent 3x3 convs within the block (not typical in Bottleneck)\n",
    "                # or if a different block structure, apply dilation where appropriate.\n",
    "                # In ResNet Bottleneck, conv3 is 1x1, so dilation doesn't apply.\n",
    "                # No change to block.conv1 (1x1) as it's typically just channel transform.\n",
    "\n",
    "        # --- Layer 3 & 4 Modification (for output_stride = 8) ---\n",
    "        # To change output_stride from 32 to 8, we need to modify layer3 as well.\n",
    "        # This will be more memory intensive.\n",
    "        elif self.output_stride == 8:\n",
    "            # Modify layer3 first\n",
    "            self.resnet.layer3[0].conv2.stride = (1, 1)\n",
    "            self.resnet.layer3[0].downsample[0].stride = (1, 1)\n",
    "            for i, block in enumerate(self.resnet.layer3):\n",
    "                block.conv2.dilation = (2, 2)\n",
    "                block.conv2.padding = (2, 2)\n",
    "\n",
    "            # Then modify layer4 (relative to original network, it's now 'twice' dilated)\n",
    "            self.resnet.layer4[0].conv2.stride = (1, 1)\n",
    "            self.resnet.layer4[0].downsample[0].stride = (1, 1)\n",
    "            for i, block in enumerate(self.resnet.layer4):\n",
    "                block.conv2.dilation = (4, 4) # Rate of 4 (2*2 from layer3+layer4)\n",
    "                block.conv2.padding = (4, 4)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported output_stride. Must be 8 or 16.\")\n",
    "\n",
    "        # Freeze Batch Normalization layers if specified\n",
    "        if self.freeze_bn:\n",
    "            self._freeze_bn()\n",
    "\n",
    "    def _freeze_bn(self):\n",
    "        # Freeze Batch Normalization layers\n",
    "        # This sets all BN layers to evaluation mode and stops updating their running stats.\n",
    "        # Crucially, it also freezes their learnable gamma and beta parameters.\n",
    "        # However, for DeepLab, we want to freeze running_mean/var but allow gamma/beta to update.\n",
    "        # The correct way to do this in PyTorch is to set track_running_stats=False\n",
    "        # and keep BN layers in training mode if their parameters should be updated.\n",
    "\n",
    "        for m in self.resnet.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval() # Set to eval mode to use running_mean/var, not batch stats\n",
    "                # Optionally, if gamma/beta *should* be updated, you would need to\n",
    "                # ensure m.weight.requires_grad = True and m.bias.requires_grad = True.\n",
    "                # By default, in m.eval(), these are still trainable, but their updates\n",
    "                # won't rely on *batch* statistics for normalization.\n",
    "                # The typical DeepLab implementation usually means BN is frozen completely\n",
    "                # for the backbone to avoid issues with small batch sizes.\n",
    "                # So m.eval() is the standard behavior in practice for this setting.\n",
    "                # Alternatively:\n",
    "                # m.track_running_stats = False # Use fixed running stats from pre-training\n",
    "                # m.requires_grad_(False) # Freeze all parameters (gamma, beta too)\n",
    "                # But the paper implies updating gamma/beta. This is a subtle point.\n",
    "                # For simplicity and common practice, m.eval() is often used for backbone BN.\n",
    "                # If we want to strictly follow \"update gamma/beta but freeze stats\",\n",
    "                # it's m.track_running_stats = False and m.momentum = 0.0 (for no moving average update)\n",
    "                # then ensure m.weight.requires_grad and m.bias.requires_grad are True.\n",
    "                # For this implementation, we will use m.eval() for simplicity which implies\n",
    "                # using the pre-computed means/vars and *not updating* gamma/beta for backbone BNs.\n",
    "                # This aligns with common DeepLab implementations for stability.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet layers\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x) # Output stride 4\n",
    "        x = self.resnet.layer2(x) # Output stride 8 (for original) or 8 (for modified)\n",
    "        low_level_features = x # Potentially useful for DeepLabV3+ decoder\n",
    "\n",
    "        # Depending on output_stride, layer3 and layer4 are modified\n",
    "        x = self.resnet.layer3(x) # Output stride 16 (for original) or 8 (for modified)\n",
    "        x = self.resnet.layer4(x) # Output stride 32 (for original) or 16/8 (for modified)\n",
    "\n",
    "        return x, low_level_features # Return both high-level and low-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d58a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super(_ASPPConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Explanation of parameters:\n",
    "        # in_channels: Number of input feature channels from the previous layer (e.g., ResNet's layer4 output channels).\n",
    "        # out_channels: Number of output feature channels for this specific ASPP branch.\n",
    "        # kernel_size=3: Standard 3x3 kernel size for feature extraction.\n",
    "        # padding=dilation: Crucial for maintaining the output spatial dimensions when stride=1.\n",
    "        #                   For a 3x3 kernel with dilation 'd', padding 'd' ensures output_size = input_size.\n",
    "        # dilation=dilation: This is the atrous rate. It controls how sparsely the input is sampled.\n",
    "        # bias=False: Typically set to False when BatchNorm2d is used immediately after, as BatchNorm\n",
    "        #             introduces its own learnable bias (beta), making the Conv2d's bias redundant.\n",
    "        # nn.BatchNorm2d(out_channels): Applies Batch Normalization.\n",
    "        # nn.ReLU(): Applies Rectified Linear Unit activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels_per_branch=256, atrous_rates=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        # Ensure atrous_rates are provided and valid\n",
    "        if not isinstance(atrous_rates, (list, tuple)):\n",
    "            raise TypeError(\"atrous_rates must be a list or tuple of integers.\")\n",
    "        \n",
    "        # 1. 1x1 Convolution branch\n",
    "        # This branch acts as a baseline, capturing local point-wise features and reducing channels.\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels_per_branch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels_per_branch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 2. Atrous Convolution branches with different dilation rates\n",
    "        # These capture multi-scale contextual information.\n",
    "        self.atrous_convs = nn.ModuleList() # Use ModuleList to hold multiple modules\n",
    "        for rate in atrous_rates:\n",
    "            self.atrous_convs.append(_ASPPConv(in_channels, out_channels_per_branch, dilation=rate))\n",
    "\n",
    "        # 3. Image-level features branch (Global Average Pooling)\n",
    "        # This captures global context.\n",
    "        self.image_pooling = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # Global Average Pooling: pools spatial dimensions to 1x1\n",
    "            nn.Conv2d(in_channels, out_channels_per_branch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels_per_branch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 4. Final 1x1 Convolution to fuse all branch outputs\n",
    "        # It reduces the concatenated channels back to a unified 'out_channels_per_branch' size.\n",
    "        # Total input channels for this final conv will be:\n",
    "        # (1x1 conv branch) + (num_atrous_convs * out_channels_per_branch) + (image_pooling branch)\n",
    "        # e.g., 256 + (3 * 256) + 256 = 256 * 5 = 1280 channels\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels_per_branch * (len(atrous_rates) + 2), out_channels_per_branch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels_per_branch),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5) # Dropout is often used for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original input dimensions for upsampling the image_pooling branch later\n",
    "        input_size = x.size()[2:] # Get H, W from (N, C, H, W)\n",
    "\n",
    "        # 1. Process through 1x1 convolution branch\n",
    "        # Input: (N, in_channels, H, W) -> Output: (N, out_channels_per_branch, H, W)\n",
    "        x1 = self.conv1x1(x)\n",
    "\n",
    "        # 2. Process through Atrous Convolution branches\n",
    "        # Each branch: Input: (N, in_channels, H, W) -> Output: (N, out_channels_per_branch, H, W)\n",
    "        atrous_outputs = []\n",
    "        for atrous_conv_layer in self.atrous_convs:\n",
    "            atrous_outputs.append(atrous_conv_layer(x))\n",
    "\n",
    "        # 3. Process through Image-level features branch\n",
    "        # Input: (N, in_channels, H, W)\n",
    "        # a. Global Average Pooling: Output (N, in_channels, 1, 1)\n",
    "        x_pool = self.image_pooling(x)\n",
    "        \n",
    "        # b. Bilinear Upsampling: Resize pooled feature from 1x1 to original input_size (H, W)\n",
    "        # This broadcasts the global context across all spatial locations.\n",
    "        # Output: (N, out_channels_per_branch, H, W)\n",
    "        x_pool_upsampled = F.interpolate(x_pool, size=input_size, mode='bilinear', align_corners=True)\n",
    "        # align_corners=True is standard practice for bilinear upsampling in segmentation\n",
    "        # to ensure pixel alignment.\n",
    "\n",
    "        # 4. Concatenate all branch outputs\n",
    "        # Stack all tensors along the channel dimension (dim=1).\n",
    "        # List of tensors to concatenate: [x1] + atrous_outputs + [x_pool_upsampled]\n",
    "        # All tensors should have the same spatial dimensions (H, W) and out_channels_per_branch.\n",
    "        # The total channels will be out_channels_per_branch * (1 + len(atrous_rates) + 1)\n",
    "        # e.g., 256 * (1 + 3 + 1) = 256 * 5 = 1280\n",
    "        all_outputs = [x1] + atrous_outputs + [x_pool_upsampled]\n",
    "        x_concat = torch.cat(all_outputs, dim=1)\n",
    "\n",
    "        # 5. Final 1x1 Convolution to fuse concatenated features\n",
    "        # Input: (N, 1280, H, W) -> Output: (N, out_channels_per_branch, H, W)\n",
    "        output = self.final_conv(x_concat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchvision import models # Already imported in Step 1\n",
    "# Assuming ResNetBackbone and ASPP classes from Step 1 and 2 are defined above this.\n",
    "\n",
    "class DeepLabV3(nn.Module):\n",
    "    def __init__(self, num_classes, backbone_name='resnet101', pretrained_backbone=True, output_stride=16, freeze_bn_backbone=True):\n",
    "        super(DeepLabV3, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.output_stride = output_stride\n",
    "\n",
    "        # 1. Initialize the Backbone (Modified ResNet)\n",
    "        # The ResNetBackbone will handle loading the pre-trained model and applying atrous modifications.\n",
    "        # It typically returns two feature maps: high_level_features (from layer4) and low_level_features (from layer2).\n",
    "        self.backbone = ResNetBackbone(name=backbone_name,\n",
    "                                       pretrained=pretrained_backbone,\n",
    "                                       output_stride=output_stride,\n",
    "                                       freeze_bn=freeze_bn_backbone)\n",
    "\n",
    "        # Determine input channels for ASPP based on the chosen backbone and output_stride.\n",
    "        # For ResNet-50/101/152, layer4 typically outputs 2048 channels.\n",
    "        # If output_stride=8, layer3 also has atrous convs, so input to layer4 is denser.\n",
    "        # However, the *number of channels* from layer4 remains 2048 regardless of stride/dilation.\n",
    "        aspp_in_channels = 2048 # ResNet's layer4 output channels\n",
    "\n",
    "        # Define atrous rates for ASPP based on output_stride\n",
    "        # These rates are carefully chosen to cover different scales and avoid gridding.\n",
    "        # A common set of rates for output_stride=16 are [6, 12, 18].\n",
    "        # For output_stride=8, the rates are doubled: [12, 24, 36].\n",
    "        if output_stride == 16:\n",
    "            aspp_atrous_rates = [6, 12, 18]\n",
    "        elif output_stride == 8:\n",
    "            aspp_atrous_rates = [12, 24, 36]\n",
    "        else:\n",
    "            raise ValueError(\"ASPP atrous rates are only defined for output_stride 8 or 16.\")\n",
    "\n",
    "        # 2. Initialize the Atrous Spatial Pyramid Pooling (ASPP) module\n",
    "        # It takes the high-level features from the backbone.\n",
    "        # The output channels of ASPP are typically 256 (out_channels_per_branch in ASPP.__init__).\n",
    "        self.aspp = ASPP(in_channels=aspp_in_channels,\n",
    "                         out_channels_per_branch=256, # Common choice for ASPP output channels\n",
    "                         atrous_rates=aspp_atrous_rates)\n",
    "\n",
    "        # 3. Final Classification Head\n",
    "        # This is a 1x1 convolution that maps the ASPP output channels (e.g., 256)\n",
    "        # to the number of desired semantic classes (num_classes).\n",
    "        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "        # Initialize new layers (ASPP and classifier) with common practices.\n",
    "        # Pre-trained layers in backbone are already initialized.\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize convolutional layers with He initialization (Kaiming normal)\n",
    "        # and Batch Normalization layers with default (mean=0, var=1, gamma=1, beta=0).\n",
    "        # This only applies to *newly added* layers (ASPP and classifier),\n",
    "        # as backbone is pre-trained.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # He initialization is suitable for ReLU activations\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # BatchNorm default initialization (weights=1, biases=0) is usually fine\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original input dimensions for final upsampling\n",
    "        input_size = x.size()[2:] # Get H, W from (N, C, H, W)\n",
    "\n",
    "        # 1. Forward pass through the backbone\n",
    "        # backbone_output is the high-level feature map from layer4 (e.g., N, 2048, H/16, W/16)\n",
    "        # low_level_features is from layer2 (e.g., N, 512, H/4, W/4) -- Not directly used in pure DeepLabV3,\n",
    "        # but kept as output of backbone for potential DeepLabV3+ extension.\n",
    "        backbone_output, low_level_features = self.backbone(x)\n",
    "\n",
    "        # 2. Forward pass through the ASPP module\n",
    "        # aspp_output: (N, 256, H/16, W/16) or (N, 256, H/8, W/8)\n",
    "        aspp_output = self.aspp(backbone_output)\n",
    "\n",
    "        # 3. Forward pass through the final classification head\n",
    "        # logits: (N, num_classes, H/16, W/16) or (N, num_classes, H/8, W/8)\n",
    "        logits = self.classifier(aspp_output)\n",
    "\n",
    "        # 4. Upsample logits to original input image resolution\n",
    "        # This is crucial for per-pixel classification.\n",
    "        # Output: (N, num_classes, H, W)\n",
    "        output = F.interpolate(logits, size=input_size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b666f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image # For loading images\n",
    "import os # For path manipulation\n",
    "import numpy as np # For potential mask operations (e.g., handling ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ba7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_dir, mask_dir, transform=None, ignore_index=255):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Base directory for the dataset (e.g., 'path/to/voc2012').\n",
    "            image_dir (str): Subdirectory containing images (e.g., 'JPEGImages').\n",
    "            mask_dir (str): Subdirectory containing segmentation masks (e.g., 'SegmentationClassAug').\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            ignore_index (int): Label value to ignore in loss calculation, typically 255.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, image_dir)\n",
    "        self.mask_dir = os.path.join(root_dir, mask_dir)\n",
    "        self.transform = transform\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        # List all image files. Assuming mask filenames correspond to image filenames.\n",
    "        # Example: image_dir/2007_000033.jpg, mask_dir/2007_000033.png\n",
    "        self.image_filenames = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))])\n",
    "        # For simplicity, we assume mask filenames are the same as image filenames but with .png extension\n",
    "        self.mask_filenames = sorted([f.replace('.jpg', '.png') if f.endswith('.jpg') else f for f in self.image_filenames])\n",
    "\n",
    "        # Basic check to ensure masks exist for all images (optional but good for robustness)\n",
    "        if len(self.image_filenames) != len(self.mask_filenames):\n",
    "            # This is a simplification; in real datasets, you might need a list of image IDs\n",
    "            # and map them to their respective mask files.\n",
    "            print(\"Warning: Number of images and masks do not match. Ensure proper filename logic.\")\n",
    "        \n",
    "        # Verify all mask files exist\n",
    "        # for mask_fn in self.mask_filenames:\n",
    "        #     if not os.path.exists(os.path.join(self.mask_dir, mask_fn)):\n",
    "        #         raise FileNotFoundError(f\"Mask file not found: {os.path.join(self.mask_dir, mask_fn)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns one sample (image, mask) from the dataset.\n",
    "        Args:\n",
    "            idx (int): Index of the sample to load.\n",
    "        Returns:\n",
    "            tuple: (image, mask)\n",
    "        \"\"\"\n",
    "        # Construct full paths to the image and mask files\n",
    "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_name = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "\n",
    "        # Load image and mask using PIL\n",
    "        # Ensure image is RGB for consistency\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        # Segmentation masks are typically single channel (grayscale)\n",
    "        # Note: 'L' mode is for grayscale. If masks are RGB with specific colors,\n",
    "        # you might need to convert RGB colors to class IDs. For Pascal VOC,\n",
    "        # masks are single channel with class IDs.\n",
    "        mask = Image.open(mask_name).convert('L') \n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            # IMPORTANT: Transforms must be applied identically to both image and mask.\n",
    "            # This requires a custom transform that operates on both.\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        # Convert mask to long tensor (required for CrossEntropyLoss)\n",
    "        # Handle ignore_index if specified (e.g., map to actual class ID 255 for loss calculation)\n",
    "        # Ensure mask is of type torch.long for nn.CrossEntropyLoss\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.long))\n",
    "        mask[mask == self.ignore_index] = self.ignore_index # Keep ignore_index as is, or map to a specific value\n",
    "\n",
    "        return image, mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
