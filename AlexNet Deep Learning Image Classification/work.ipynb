{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Define the AlexNet Model\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4), # conv1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2), # lrn1 - Optional, can be removed for modern implementations\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # pool1\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # conv2 (padding=2 to keep size same with stride 1)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2), # lrn2 - Optional\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # pool2\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # conv3 (padding=1 to keep size same with stride 1)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # conv4 (padding=1 to keep size same with stride 1)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # conv5 (padding=1 to keep size same with stride 1)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # pool3\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) # Adaptive pooling to ensure consistent input to FC layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5), # dropout layer fc6\n",
    "            nn.Linear(256 * 6 * 6, 4096), # fc6\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5), # dropout layer fc7\n",
    "            nn.Linear(4096, 4096), # fc7\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes), # fc8\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 2. Data Loading and Preprocessing (Example - using a dummy dataset)\n",
    "\n",
    "class DummyImageDataset(Dataset): # Replace with your actual ImageNet or dataset loading\n",
    "    def __init__(self, root_dir, transform=None, num_samples=1000, num_classes=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.image_paths = [os.path.join(root_dir, f'image_{i}.jpg') for i in range(num_samples)]\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        for path in self.image_paths:\n",
    "            if not os.path.exists(path):\n",
    "                Image.new('RGB', (256, 256), color = 'red').save(path) # Create dummy image if not exist\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = np.random.randint(0, self.num_classes) # Dummy random label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Data Augmentation Transforms (as described in AlexNet paper)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), # Random crop to 224x224 from resized (implicitly 256x256 in AlexNet)\n",
    "    transforms.RandomHorizontalFlip(), # Horizontal flip\n",
    "    transforms.ToTensor(), # Convert to tensor, scales to [0, 1]\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization - if needed for real ImageNet\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256), # Resize to 256x256\n",
    "    transforms.CenterCrop(224), # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization - if needed for real ImageNet\n",
    "])\n",
    "\n",
    "\n",
    "# Create Dummy Dataset and DataLoaders (Replace with your actual data)\n",
    "num_classes = 1000 # For ImageNet\n",
    "train_dataset = DummyImageDataset('dummy_train_images', transform=train_transform, num_samples=1000, num_classes=num_classes)\n",
    "val_dataset = DummyImageDataset('dummy_val_images', transform=val_transform, num_samples=200, num_classes=num_classes)\n",
    "\n",
    "batch_size = 128 # As mentioned in the paper\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) # num_workers for parallel data loading\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# 3. Set up Training Parameters, Optimizer, and Learning Rate Scheduler\n",
    "\n",
    "model = AlexNet(num_classes=num_classes)\n",
    "\n",
    "# Check if CUDA is available and use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Move model to GPU if available\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # SGD with momentum and weight decay\n",
    "\n",
    "\n",
    "# Learning Rate Scheduling (Manual decay as in paper)\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr=0.01):\n",
    "    \"\"\"Decay learning rate by a factor of 10 when validation error plateaus.\"\"\"\n",
    "    lr = initial_lr\n",
    "    if epoch >= 30: # Example epochs for decay - adjust based on validation performance\n",
    "        lr /= 10\n",
    "    if epoch >= 60:\n",
    "        lr /= 10\n",
    "    if epoch >= 80:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "# 4. Training Loop\n",
    "\n",
    "num_epochs = 90 # As mentioned in the paper (roughly)\n",
    "initial_lr = 0.01 # Initial learning rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch, initial_lr) # Adjust learning rate per epoch\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) # Move data to GPU if available\n",
    "\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimizer step (update weights)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9: # Print every 10 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f} LR: {current_lr}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validation after each epoch (optional, but good practice)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation during validation\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Validation Loss: {val_loss/len(val_loader):.3f}, Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
