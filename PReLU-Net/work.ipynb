{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PReLU Test ---\n",
      "Original Input Tensor:\n",
      " tensor([[[[-1.0000,  2.0000],\n",
      "          [ 0.5000, -3.0000]]]])\n",
      "\n",
      "--- Channel-Shared PReLU (a=0.25) ---\n",
      "Output Tensor:\n",
      " tensor([[[[-0.2500,  2.0000],\n",
      "          [ 0.5000, -0.7500]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "--- Channel-Wise PReLU (a=0.1 for the single channel) ---\n",
      "Output Tensor:\n",
      " tensor([[[[-0.1000,  2.0000],\n",
      "          [ 0.5000, -0.3000]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Is 'weight' a learnable parameter in prelu_shared? True\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 1: Imports and PReLU Implementation\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 2. Implement the Custom PReLU Activation Function ---\n",
    "\n",
    "class PReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Parametric Rectified Linear Unit (PReLU) activation function.\n",
    "\n",
    "    As defined in the paper \"Delving Deep into Rectifiers,\" PReLU is a generalization\n",
    "    of ReLU where the slope for negative inputs is a learnable parameter.\n",
    "\n",
    "    Formula: f(y_i) = max(0, y_i) + a_i * min(0, y_i)\n",
    "    where 'a_i' is a learnable coefficient.\n",
    "\n",
    "    This implementation supports both:\n",
    "    1. Channel-wise PReLU: A separate 'a_i' is learned for each channel.\n",
    "    2. Channel-shared PReLU: A single 'a' is learned and shared across all channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_parameters: int = 1, init: float = 0.25):\n",
    "        \"\"\"\n",
    "        Initializes the PReLU layer.\n",
    "\n",
    "        Args:\n",
    "            num_parameters (int): The number of 'a' parameters to learn.\n",
    "                                  - For channel-shared, this is 1.\n",
    "                                  - For channel-wise, this is the number of channels\n",
    "                                    in the input feature map.\n",
    "                                  Defaults to 1.\n",
    "            init (float): The initial value for the 'a' parameter(s). The paper\n",
    "                          suggests starting with 0.25. Defaults to 0.25.\n",
    "        \"\"\"\n",
    "        super(PReLU, self).__init__()\n",
    "\n",
    "        # --- Define the Learnable Parameter 'a' ---\n",
    "        # In PyTorch, learnable parameters of a module must be wrapped in `nn.Parameter`. This tells PyTorch that this tensor should be considered a model parameter, which means\n",
    "        # its gradients will be computed during backpropagation and it will be updated by the optimizer.\n",
    "        # 1. `torch.empty(num_parameters)`: Allocates a 1D tensor of size `num_parameters` without initializing its values.\n",
    "        # 2. `.fill_(init)`: Fills the allocated tensor with the initial value 'init'. The underscore suffix in PyTorch functions (e.g., `fill_`) indicates an \"in-place\" operation, \n",
    "        #      modifying the tensor directly.\n",
    "        # 3. `nn.Parameter(...)`: Wraps the tensor, registering it as a learnable parameter. We name it 'weight' to be consistent with PyTorch's naming conventions for parameters\n",
    "        #     in layers, although it represents our 'a' coefficient.\n",
    "        self.weight = nn.Parameter(torch.empty(num_parameters).fill_(init))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the PReLU activation.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (pre-activations) from the previous layer.\n",
    "                              Expected shape for a conv layer: (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying PReLU.\n",
    "        \"\"\"\n",
    "        # The core logic of PReLU is to apply a different function based on the sign of the input.\n",
    "\n",
    "        # f(x) = relu(x) + a * (-relu(-x)) which is equivalent to: f(x) = max(0, x) + a * min(0, x)\n",
    "        # Let's break down the implementation:\n",
    "        # 1. `torch.max(torch.tensor(0.0, device=x.device), x)`: This calculates the positive part,\n",
    "        #    equivalent to ReLU(x). We create the zero tensor on the same device as `x` for compatibility.\n",
    "\n",
    "        # 2. `torch.min(torch.tensor(0.0, device=x.device), x)`: This calculates the negative part.\n",
    "\n",
    "        # 3. `self.weight * ...`: The negative part is multiplied by our learnable parameter 'a'.\n",
    "        #    PyTorch's broadcasting rules will automatically handle the multiplication, whether\n",
    "        #    'self.weight' is a single value (channel-shared) or a vector (channel-wise).\n",
    "        #    For a (N, C, H, W) input tensor and a (C,) weight tensor, broadcasting will\n",
    "        #    apply the correct a_i to each channel.\n",
    "\n",
    "        # To handle both channel-wise and channel-shared cases gracefully with broadcasting,\n",
    "        # we need to ensure the shape of `self.weight` is compatible with `x`.\n",
    "        # `x` has shape (N, C, H, W). `self.weight` has shape (C,). We reshape it to (1, C, 1, 1).\n",
    "        if self.weight.shape[0] > 1:\n",
    "            weight_reshaped = self.weight.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            weight_reshaped = self.weight\n",
    "\n",
    "        return torch.max(torch.tensor(0.0, device=x.device), x) + weight_reshaped * torch.min(torch.tensor(0.0, device=x.device), x)\n",
    "\n",
    "# --- 3. Example Usage and Verification ---\n",
    "\n",
    "# Shape: (batch_size=1, channels=3, height=2, width=2)\n",
    "dummy_input = torch.tensor([[[[-1.0, 2.0],\n",
    "                              [0.5, -3.0]]]], dtype=torch.float32)\n",
    "print(\"--- PReLU Test ---\")\n",
    "print(\"Original Input Tensor:\\n\", dummy_input)\n",
    "\n",
    "# Test the channel-shared PReLU\n",
    "print(\"\\n--- Channel-Shared PReLU (a=0.25) ---\")\n",
    "prelu_shared = PReLU(num_parameters=1, init=0.25)\n",
    "output_shared = prelu_shared(dummy_input)\n",
    "print(\"Output Tensor:\\n\", output_shared)\n",
    "# Expected output for negative values: -1.0*0.25 = -0.25, -3.0*0.25 = -0.75\n",
    "\n",
    "# Test the channel-wise PReLU\n",
    "# Let's pretend our input has 1 channel and we want a specific 'a' for it.\n",
    "print(\"\\n--- Channel-Wise PReLU (a=0.1 for the single channel) ---\")\n",
    "prelu_wise = PReLU(num_parameters=1, init=0.1) # Here num_parameters matches channel count\n",
    "output_wise = prelu_wise(dummy_input)\n",
    "print(\"Output Tensor:\\n\", output_wise)\n",
    "# Expected output for negative values: -1.0*0.1 = -0.1, -3.0*0.1 = -0.3\n",
    "\n",
    "# Check that the parameter is indeed learnable\n",
    "print(\"\\nIs 'weight' a learnable parameter in prelu_shared?\", prelu_shared.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SPP Layer Test ---\n",
      "Pyramid Levels: [7, 3, 2, 1]\n",
      "Total Bins per Channel: 63\n",
      "Expected Output Feature Dimension: 16128\n",
      "--------------------\n",
      "Input 1 Shape: torch.Size([1, 256, 13, 13])\n",
      "Output 1 Shape: torch.Size([1, 16128])\n",
      "--------------------\n",
      "Input 2 Shape: torch.Size([1, 256, 10, 15])\n",
      "Output 2 Shape: torch.Size([1, 16128])\n",
      "--------------------\n",
      "Verification Successful: Both outputs have the same shape (16128 features).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 2: Spatial Pyramid Pooling (SPP) Implementation\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math # We need the math library for ceiling function\n",
    "\n",
    "class SpatialPyramidPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Spatial Pyramid Pooling (SPP) layer.\n",
    "\n",
    "    As described in \"Spatial Pyramid Pooling in Deep Convolutional Networks for\n",
    "    Visual Recognition\" and used in this paper's architecture. SPP allows the\n",
    "    network to handle variable-sized input images by producing a fixed-length\n",
    "    output vector.\n",
    "\n",
    "    This implementation replicates the 4-level pyramid from the paper:\n",
    "    - 7x7 bins\n",
    "    - 3x3 bins\n",
    "    - 2x2 bins\n",
    "    - 1x1 bin (global pooling)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pyramid_levels: list):\n",
    "        \"\"\"\n",
    "        Initializes the SPP layer.\n",
    "\n",
    "        Args:\n",
    "            pyramid_levels (list): A list of integers defining the number of bins\n",
    "                                   for each side of the square pyramid levels.\n",
    "                                   Example: [7, 3, 2, 1] for 7x7, 3x3, 2x2, 1x1 bins.\n",
    "        \"\"\"\n",
    "        super(SpatialPyramidPooling, self).__init__()\n",
    "        self.pyramid_levels = pyramid_levels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the SPP layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input feature map from the last convolutional layer.\n",
    "                              Expected shape: (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The fixed-length feature vector after pooling and concatenation.\n",
    "                          Shape: (batch_size, channels * total_bins)\n",
    "        \"\"\"\n",
    "        # Get the input dimensions.\n",
    "        # N = batch_size, C = channels, H = height, W = width\n",
    "        N, C, H, W = x.size()\n",
    "\n",
    "        # --- Iterate through each level of the pyramid ---\n",
    "        # We will store the output of each pyramid level in this list.\n",
    "        level_outputs = []\n",
    "\n",
    "        for level_size in self.pyramid_levels:\n",
    "            # For each pyramid level (e.g., 7x7), we calculate the required\n",
    "            # pooling kernel size and stride dynamically based on the input size (H, W).\n",
    "            # This is the core idea that makes SPP input-size-agnostic.\n",
    "\n",
    "            # 1. Calculate Kernel Size:\n",
    "            # The kernel should be large enough to cover at least one \"bin\" of the input.\n",
    "            # We use the ceiling function to ensure the kernel is not too small.\n",
    "            kernel_h = math.ceil(H / level_size)\n",
    "            kernel_w = math.ceil(W / level_size)\n",
    "\n",
    "            # 2. Calculate Stride:\n",
    "            # The stride determines how the pooling window moves. To create `level_size`\n",
    "            # bins, the window should slide by approximately H / level_size.\n",
    "            # We use the floor function to ensure we get exactly `level_size` outputs.\n",
    "            stride_h = math.floor(H / level_size)\n",
    "            stride_w = math.floor(W / level_size)\n",
    "            \n",
    "            # --- Handle potential off-by-one errors due to discrete grid sizes ---\n",
    "            # The padding is calculated to ensure the pooling operation covers the entire\n",
    "            # feature map. The formula for output size is `(Input + 2*Pad - Kernel)/Stride + 1`.\n",
    "            # We want Output = level_size. So, `level_size = (Input + 2*Pad - Kernel)/Stride + 1`.\n",
    "            # Solving for Pad: `Pad = ((level_size - 1) * Stride + Kernel - Input) / 2`.\n",
    "            # We need to ensure the padding is non-negative.\n",
    "            pad_h = (kernel_h * level_size - H + stride_h -1) // 2\n",
    "            pad_w = (kernel_w * level_size - W + stride_w -1) // 2\n",
    "\n",
    "            # A more robust and modern way is to use AdaptiveAvgPool2d, which handles\n",
    "            # these calculations internally. Let's implement it both ways for clarity.\n",
    "\n",
    "            # --- Method 1: Manual Calculation (for understanding) ---\n",
    "            # pooler = nn.MaxPool2d(\n",
    "            #     kernel_size=(kernel_h, kernel_w),\n",
    "            #     stride=(stride_h, stride_w),\n",
    "            #     padding=(pad_h, pad_w)\n",
    "            # )\n",
    "            # pooled_level = pooler(x)\n",
    "\n",
    "            # --- Method 2: Using Adaptive Pooling (the standard, easier way) ---\n",
    "            # `nn.AdaptiveAvgPool2d` or `nn.AdaptiveMaxPool2d` simplifies this greatly.\n",
    "            # You just specify the desired output size (e.g., (level_size, level_size)),\n",
    "            # and PyTorch automatically computes the necessary kernel and stride.\n",
    "            # We use average pooling here, but max pooling is also common.\n",
    "            adaptive_pooler = nn.AdaptiveAvgPool2d((level_size, level_size))\n",
    "            pooled_level = adaptive_pooler(x)\n",
    "\n",
    "            # After pooling, the shape is (N, C, level_size, level_size).\n",
    "            # We need to flatten this into a vector for each item in the batch.\n",
    "            # `pooled_level.view(N, -1)` flattens all dimensions except the batch dimension.\n",
    "            # The resulting shape will be (N, C * level_size * level_size).\n",
    "            flattened_level = pooled_level.view(N, -1)\n",
    "            level_outputs.append(flattened_level)\n",
    "\n",
    "        # --- Concatenate all level outputs ---\n",
    "        # Now we concatenate the flattened vectors from all pyramid levels along the\n",
    "        # feature dimension (dim=1).\n",
    "        # For example, if we have outputs of shape (N, 256*49), (N, 256*9), etc.,\n",
    "        # we concatenate them to get one long vector per batch item.\n",
    "        spp_output = torch.cat(level_outputs, dim=1)\n",
    "\n",
    "        return spp_output\n",
    "\n",
    "# --- 3. Example Usage and Verification ---\n",
    "\n",
    "# Define the pyramid levels as specified in the paper\n",
    "pyramid_spec = [7, 3, 2, 1]\n",
    "\n",
    "# Instantiate our SPP layer\n",
    "spp_layer = SpatialPyramidPooling(pyramid_levels=pyramid_spec)\n",
    "\n",
    "# Let's test it with two different-sized input feature maps to see if it\n",
    "# produces a fixed-length output.\n",
    "# Let's assume the number of channels from the last conv layer is 256.\n",
    "\n",
    "# Test Case 1: Input feature map of size 13x13\n",
    "input_map1 = torch.randn(1, 256, 13, 13) # (N, C, H, W)\n",
    "output1 = spp_layer(input_map1)\n",
    "\n",
    "# Test Case 2: Input feature map of size 10x15 (non-square)\n",
    "input_map2 = torch.randn(1, 256, 10, 15)\n",
    "output2 = spp_layer(input_map2)\n",
    "\n",
    "# Calculate the expected output size\n",
    "num_channels = 256\n",
    "total_bins = sum([level*level for level in pyramid_spec]) # 7*7 + 3*3 + 2*2 + 1*1 = 49 + 9 + 4 + 1 = 63\n",
    "expected_output_features = num_channels * total_bins\n",
    "\n",
    "print(\"--- SPP Layer Test ---\")\n",
    "print(f\"Pyramid Levels: {pyramid_spec}\")\n",
    "print(f\"Total Bins per Channel: {total_bins}\")\n",
    "print(f\"Expected Output Feature Dimension: {expected_output_features}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Input 1 Shape: {input_map1.shape}\")\n",
    "print(f\"Output 1 Shape: {output1.shape}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Input 2 Shape: {input_map2.shape}\")\n",
    "print(f\"Output 2 Shape: {output2.shape}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Verification Successful: Both outputs have the same shape ({output1.shape[1]} features).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9103ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PReLU-Net (Model B) Instantiation ---\n",
      "\n",
      "--- Performing a forward pass ---\n",
      "Input batch shape: torch.Size([2, 3, 224, 224])\n",
      "Output logits shape: torch.Size([2, 1000])\n",
      "\n",
      "Verification Successful: The output shape is correct.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 3: Assembling the Full PReLU-Net Architecture (Model B)\n",
    "# ==============================================================================\n",
    "\n",
    "# We continue using the previously imported libraries and defined custom modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Let's re-paste our custom modules here for a self-contained script.\n",
    "class PReLU(nn.Module):\n",
    "    def __init__(self, num_parameters: int = 1, init: float = 0.25):\n",
    "        super(PReLU, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(num_parameters).fill_(init))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.weight.shape[0] > 1:\n",
    "            weight_reshaped = self.weight.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            weight_reshaped = self.weight\n",
    "        return torch.max(torch.tensor(0.0, device=x.device), x) + \\\n",
    "               weight_reshaped * torch.min(torch.tensor(0.0, device=x.device), x)\n",
    "\n",
    "class SpatialPyramidPooling(nn.Module):\n",
    "    def __init__(self, pyramid_levels: list):\n",
    "        super(SpatialPyramidPooling, self).__init__()\n",
    "        self.pyramid_levels = pyramid_levels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        N, C, H, W = x.size()\n",
    "        level_outputs = []\n",
    "        for level_size in self.pyramid_levels:\n",
    "            adaptive_pooler = nn.AdaptiveAvgPool2d((level_size, level_size))\n",
    "            pooled_level = adaptive_pooler(x)\n",
    "            flattened_level = pooled_level.view(N, -1)\n",
    "            level_outputs.append(flattened_level)\n",
    "        return torch.cat(level_outputs, dim=1)\n",
    "\n",
    "\n",
    "# --- 1. Define the Full PReLU-Net (Model B) ---\n",
    "\n",
    "class PReLUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the PReLU-Net architecture (Model B, 22 layers)\n",
    "    from the paper \"Delving Deep into Rectifiers\".\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 1000, prelu_shared: bool = True):\n",
    "        \"\"\"\n",
    "        Initializes the PReLU-Net.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes (e.g., 1000 for ImageNet).\n",
    "            prelu_shared (bool): If True, use channel-shared PReLU. If False, use\n",
    "                                 channel-wise PReLU.\n",
    "        \"\"\"\n",
    "        super(PReLUNet, self).__init__()\n",
    "        \n",
    "        # --- Define the Convolutional Feature Extractor ---\n",
    "        # We group layers into sequential blocks based on the architecture in Table 3.\n",
    "        # `nn.Sequential` is a container that passes data through modules in order.\n",
    "\n",
    "        # Stage 1: Input -> 112x112\n",
    "        # Paper: 7x7 conv, 96 filters, stride 2\n",
    "        self.stage1 = nn.Sequential(\n",
    "            # `in_channels=3` for standard RGB images.\n",
    "            # `out_channels=96` is the number of filters.\n",
    "            # `kernel_size=7`, `stride=2`, `padding=3` to get output H/2, W/2.\n",
    "            nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 96),\n",
    "            # `kernel_size=3`, `stride=2` for max pooling.\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Stage 2: 112x112 -> 56x56\n",
    "        # Paper: Three 3x3 conv layers with 256 filters\n",
    "        self.stage2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 256),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Stage 3: 56x56 -> 28x28\n",
    "        # Paper: Six 3x3 conv layers with 384 filters\n",
    "        self.stage3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 384),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Stage 4: 28x28 -> 14x14\n",
    "        # Paper: Six 3x3 conv layers with 768 filters\n",
    "        self.stage4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.Conv2d(768, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.Conv2d(768, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.Conv2d(768, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.Conv2d(768, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.Conv2d(768, 768, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 768),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Stage 5: 14x14 -> 7x7 (Note: No pooling at the end of this stage in paper's table)\n",
    "        # Paper: Three 3x3 conv layers with 896 filters\n",
    "        self.stage5 = nn.Sequential(\n",
    "            nn.Conv2d(768, 896, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 896),\n",
    "            nn.Conv2d(896, 896, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 896),\n",
    "            nn.Conv2d(896, 896, kernel_size=3, stride=1, padding=1),\n",
    "            PReLU(num_parameters=1 if prelu_shared else 896),\n",
    "        )\n",
    "\n",
    "        # --- Define the Spatial Pyramid Pooling Layer ---\n",
    "        # The paper uses a 4-level pyramid.\n",
    "        self.spp = SpatialPyramidPooling([7, 3, 2, 1])\n",
    "\n",
    "        # --- Define the Classifier (Fully Connected Layers) ---\n",
    "        # First, we need to calculate the input size to the first FC layer.\n",
    "        # This is determined by the output of the SPP layer.\n",
    "        # Number of channels from last conv layer (stage5) is 896.\n",
    "        # Total bins from SPP = 7*7 + 3*3 + 2*2 + 1*1 = 63.\n",
    "        fc_input_features = 896 * 63\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # First FC layer. Paper specifies 4096 output units.\n",
    "            nn.Linear(fc_input_features, 4096),\n",
    "            PReLU(num_parameters=1), # FC layers use shared PReLU\n",
    "            # Dropout is a regularization technique to prevent overfitting.\n",
    "            # It randomly zeros some of the elements of the input tensor.\n",
    "            # Paper specifies 50% dropout.\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            # Second FC layer.\n",
    "            nn.Linear(4096, 4096),\n",
    "            PReLU(num_parameters=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            # Final output layer.\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "        # --- Initialize weights ---\n",
    "        # The paper emphasizes the importance of a custom initialization method.\n",
    "        # We will apply it to all conv and linear layers.\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Defines the full forward pass of the network. \"\"\"\n",
    "        # Pass input through the convolutional stages\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        \n",
    "        # Apply Spatial Pyramid Pooling\n",
    "        x = self.spp(x)\n",
    "\n",
    "        # Pass the fixed-length vector through the classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Applies the He initialization method as described in the paper.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # He initialization for Conv2d layers\n",
    "                # `nn.init.kaiming_normal_` implements the He initialization.\n",
    "                # `mode='fan_in'` ensures variance is scaled by the number of input channels.\n",
    "                # `nonlinearity='leaky_relu'` with a=0.25 (for PReLU init) or 'relu' is appropriate.\n",
    "                # The paper's PReLU init is a=0.25, so we can use leaky_relu with this slope.\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.25)\n",
    "                if m.bias is not None:\n",
    "                    # Initialize biases to zero.\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # He initialization for Linear layers\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=0.25)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# --- 2. Example Usage and Verification ---\n",
    "\n",
    "# Instantiate the full network (using channel-shared PReLU for efficiency)\n",
    "print(\"--- PReLU-Net (Model B) Instantiation ---\")\n",
    "model = PReLUNet(num_classes=1000, prelu_shared=True)\n",
    "# print(model) # Uncomment to see the full architecture printed out\n",
    "\n",
    "# Create a dummy input tensor that mimics a batch of ImageNet images.\n",
    "# Shape: (batch_size=2, channels=3, height=224, width=224)\n",
    "dummy_image_batch = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Perform a forward pass\n",
    "print(\"\\n--- Performing a forward pass ---\")\n",
    "print(f\"Input batch shape: {dummy_image_batch.shape}\")\n",
    "output = model(dummy_image_batch)\n",
    "print(f\"Output logits shape: {output.shape}\")\n",
    "\n",
    "# Verify the output shape is correct (batch_size, num_classes)\n",
    "assert output.shape == (2, 1000)\n",
    "print(\"\\nVerification Successful: The output shape is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c91261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Training Components ---\n",
      "Model Instantiated.\n",
      "Loss Function: CrossEntropyLoss\n",
      "Optimizer: SGD with momentum and weight decay\n",
      "\n",
      "--- Simulating a Training Loop for One Epoch ---\n",
      "1. Model set to train mode: `model.train()`\n",
      "2. Gradients zeroed: `optimizer.zero_grad()`\n",
      "3. Forward Pass: Input shape torch.Size([4, 3, 224, 224]) -> Output shape torch.Size([4, 10])\n",
      "4. Loss Computed: 13.5604\n",
      "5. Backward Pass: Gradients computed with `loss.backward()`\n",
      "6. Optimizer Step: Model parameters updated with `optimizer.step()`\n",
      "\n",
      "--- Training Step Complete ---\n",
      "The model's weights (and PReLU 'a' params) have been updated once.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 4: Setting up Training Components\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "print(\"--- Setting up Training Components ---\")\n",
    "\n",
    "# Instantiate the model. We'll use a smaller number of classes for this example.\n",
    "# Let's assume we're fine-tuning on a 10-class problem like CIFAR-10.\n",
    "model = PReLUNet(num_classes=10, prelu_shared=True)\n",
    "print(\"Model Instantiated.\")\n",
    "\n",
    "# Define the Loss Function.\n",
    "# For multi-class classification, the standard and most effective loss function\n",
    "# is Cross-Entropy Loss. `nn.CrossEntropyLoss` in PyTorch conveniently combines\n",
    "# `nn.LogSoftmax` and `nn.NLLLoss` (Negative Log Likelihood Loss) in one class.\n",
    "# It expects raw, unnormalized logits from the model and integer class labels.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Loss Function: CrossEntropyLoss\")\n",
    "\n",
    "# Define the Optimizer.\n",
    "# The paper specifies using SGD with momentum.\n",
    "# `optim.SGD` is PyTorch's implementation of Stochastic Gradient Descent.\n",
    "#\n",
    "# - `model.parameters()`: This crucial method, inherited from `nn.Module`,\n",
    "#   automatically gathers all learnable parameters (tensors wrapped in `nn.Parameter`,\n",
    "#   including our PReLU 'a' weights) and passes them to the optimizer.\n",
    "# - `lr=0.01`: The learning rate. This is a critical hyperparameter that controls\n",
    "#   the step size of parameter updates. The paper uses 1e-2 initially.\n",
    "# - `momentum=0.9`: The momentum factor. This helps accelerate SGD in the\n",
    "#   relevant direction and dampens oscillations. The paper uses 0.9.\n",
    "# - `weight_decay=0.0005`: This implements L2 regularization. It adds a penalty\n",
    "#   to the loss proportional to the squared magnitude of the weights, helping\n",
    "#   to prevent overfitting. The paper uses 5e-4.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "print(\"Optimizer: SGD with momentum and weight decay\")\n",
    "\n",
    "\n",
    "# --- 2. Implement the Training Loop (for one epoch) ---\n",
    "\n",
    "print(\"\\n--- Simulating a Training Loop for One Epoch ---\")\n",
    "\n",
    "# Create a dummy dataset for demonstration.\n",
    "# In a real scenario, this would come from a `torch.utils.data.DataLoader`.\n",
    "# Let's create a batch of 4 images, 3 channels, 224x224 pixels.\n",
    "dummy_inputs = torch.randn(4, 3, 224, 224)\n",
    "# Corresponding dummy labels (integer class indices from 0 to 9).\n",
    "dummy_labels = torch.randint(0, 10, (4,))\n",
    "\n",
    "# --- A single training step ---\n",
    "\n",
    "# Set the model to training mode.\n",
    "# This is important for layers like Dropout, which behave differently\n",
    "# during training (actively dropping out neurons) and evaluation (using all neurons).\n",
    "model.train()\n",
    "print(\"1. Model set to train mode: `model.train()`\")\n",
    "\n",
    "# Zero the gradients.\n",
    "# Before the backward pass, we must explicitly zero out the gradients from the\n",
    "# previous iteration. If we don't, gradients would accumulate, leading to\n",
    "# incorrect updates.\n",
    "optimizer.zero_grad()\n",
    "print(\"2. Gradients zeroed: `optimizer.zero_grad()`\")\n",
    "\n",
    "# Step A: Forward Pass\n",
    "# We pass the input data through our model to get the output predictions (logits).\n",
    "# PyTorch builds a computation graph behind the scenes to track all operations.\n",
    "outputs = model(dummy_inputs)\n",
    "print(f\"3. Forward Pass: Input shape {dummy_inputs.shape} -> Output shape {outputs.shape}\")\n",
    "\n",
    "# Step B: Loss Computation\n",
    "# We compute the loss by comparing the model's outputs with the ground-truth labels.\n",
    "loss = criterion(outputs, dummy_labels)\n",
    "print(f\"4. Loss Computed: {loss.item():.4f}\") # .item() gets the scalar value of the loss\n",
    "\n",
    "# Step C: Backward Pass\n",
    "# This is where the magic happens. `loss.backward()` computes the gradient of the loss\n",
    "# with respect to every single parameter in the model that has `requires_grad=True`.\n",
    "# The gradients are stored in the `.grad` attribute of each parameter tensor.\n",
    "loss.backward()\n",
    "print(\"5. Backward Pass: Gradients computed with `loss.backward()`\")\n",
    "\n",
    "# Step D: Parameter Update\n",
    "# The optimizer uses the computed gradients to update the model's parameters.\n",
    "# It applies the update rule (e.g., SGD with momentum) to take a step.\n",
    "optimizer.step()\n",
    "print(\"6. Optimizer Step: Model parameters updated with `optimizer.step()`\")\n",
    "\n",
    "print(\"\\n--- Training Step Complete ---\")\n",
    "print(\"The model's weights (and PReLU 'a' params) have been updated once.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
