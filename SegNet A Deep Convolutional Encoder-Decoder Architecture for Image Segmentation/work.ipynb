{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f4fa4b",
   "metadata": {},
   "source": [
    "### SegNet-Basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e160d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single encoder block for \n",
    "    Consists of two convolutional layers, each followed by Batch Normalization and ReLU,\n",
    "    and concludes with a Max-Pooling operation that returns indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Constructor for the EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input feature map channels.\n",
    "            out_channels (int): Number of output feature map channels (after the convolutions).\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__() # Calls the constructor of the parent class (nn.Module).\n",
    "                                            # This is mandatory for all nn.Module subclasses.\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        # nn.Conv2d: Applies a 2D convolution over an input signal composed of several input planes.\n",
    "        # in_channels: Number of channels in the input image/feature map.\n",
    "        # out_channels: Number of channels produced by the convolution (i.e., number of filters).\n",
    "        # kernel_size=3: The size of the convolutional kernel (filter) is 3x3.\n",
    "        #                This is a common choice for capturing local patterns efficiently.\n",
    "        # padding=1: Adds 1 pixel of zero-padding to all four sides of the input.\n",
    "        #            This is crucial to ensure that the output feature map has the same\n",
    "        #            spatial dimensions (height and width) as the input feature map *before* pooling.\n",
    "        # bias=False: Whether to add a learnable bias to the output.\n",
    "        #             It's common to set bias=False when followed by nn.BatchNorm2d,\n",
    "        #             as BatchNorm's beta parameter effectively handles the bias.\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # First Batch Normalization Layer\n",
    "        # nn.BatchNorm2d: Applies Batch Normalization over a 4D input (N, C, H, W).\n",
    "        # out_channels: Number of channels in the input to BatchNorm, which is the output of conv1.\n",
    "        #               BatchNorm normalizes independently for each channel.\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        # Similar to conv1, but input channels are now 'out_channels' (from the previous conv/bn/relu).\n",
    "        # It maintains the 'out_channels' count, effectively adding more feature extraction capacity.\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # Second Batch Normalization Layer\n",
    "        # Again, normalizing the output of conv2.\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Max-Pooling Layer\n",
    "        # nn.MaxPool2d: Applies a 2D max-pooling operation.\n",
    "        # kernel_size=2: The pooling window size is 2x2.\n",
    "        # stride=2: The step size for the pooling window is 2.\n",
    "        #           This means the window moves 2 pixels at a time, resulting in a 2x spatial downsampling.\n",
    "        #           E.g., a 10x10 input becomes a 5x5 output.\n",
    "        # return_indices=True: THIS IS CRUCIAL FOR SEGNET.\n",
    "        #                      It makes the MaxPool2d layer return not only the pooled output tensor\n",
    "        #                      but also the indices of the maximum values in each pooling region.\n",
    "        #                      These indices are passed to the corresponding decoder's MaxUnpool2d layer.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor for the block (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - pooled_x (torch.Tensor): The output tensor after max-pooling.\n",
    "                - indices (torch.Tensor): The indices of max values from the pooling operation.\n",
    "        \"\"\"\n",
    "        # First Conv -> BN -> ReLU\n",
    "        # F.relu: The Rectified Linear Unit activation function (max(0, x)).\n",
    "        #         It introduces non-linearity, allowing the network to learn complex patterns.\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # Apply conv1, then bn1, then relu. The output is fed to conv2.\n",
    "\n",
    "        # Second Conv -> BN -> ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x))) # Apply conv2, then bn2, then relu. The output is fed to pooling.\n",
    "\n",
    "        # Max-Pooling\n",
    "        # The pool layer returns two tensors: the pooled output and the indices.\n",
    "        pooled_x, indices = self.pool(x)\n",
    "\n",
    "        return pooled_x, indices # Return both for the main SegNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e81174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder block for SegNet-Basic.\n",
    "    Consists of Max-Unpooling (using indices), followed by Batch Normalization and\n",
    "    two convolutional layers (each followed by BatchNorm).\n",
    "    Note: Decoder convolutions in SegNet-Basic have no bias and no ReLU, and use a 7x7 kernel.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Constructor for the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input feature map channels (from the previous decoder layer).\n",
    "            out_channels (int): Number of output feature map channels (for the reconstructed features).\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Max-Unpooling Layer\n",
    "        # nn.MaxUnpool2d: Performs unpooling. It uses the output and indices from nn.MaxPool2d\n",
    "        #                 to reconstruct the input tensor.\n",
    "        # kernel_size=2: Must match the kernel_size used in the corresponding MaxPool2d layer.\n",
    "        # stride=2: Must match the stride used in the corresponding MaxPool2d layer.\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        # in_channels: The input to this conv layer comes from the unpooled map.\n",
    "        #              This unpooled map retains the channel depth from the previous decoder layer.\n",
    "        # out_channels: The number of channels for the reconstructed features at this stage.\n",
    "        # kernel_size=7: THIS IS A SPECIFIC CONSTRAINT OF SEGNET-BASIC DECODER.\n",
    "        #                A larger kernel allows the convolution to have a wider receptive field,\n",
    "        #                which helps in \"filling in\" the sparse unpooled map and providing\n",
    "        #                more context for smooth labeling.\n",
    "        # padding=3: For a 7x7 kernel, padding=3 ensures the spatial dimensions (H, W)\n",
    "        #            remain the same as the input to this convolution (the unpooled map).\n",
    "        # bias=False: THIS IS A SPECIFIC CONSTRAINT OF SEGNET-BASIC DECODER.\n",
    "        #             As discussed previously, when BatchNorm follows, biases are often redundant.\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "        # First Batch Normalization Layer\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        # Input channels are now 'out_channels' (from the previous conv/bn).\n",
    "        # It helps further refine the reconstructed features.\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "        # Second Batch Normalization Layer\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, indices, output_size):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor for the block (N, C, H_low, W_low)\n",
    "                              from the previous decoder layer.\n",
    "            indices (torch.Tensor): Max-pooling indices obtained from the\n",
    "                                    corresponding encoder block.\n",
    "            output_size (tuple): The desired spatial size (H, W) of the output tensor\n",
    "                                 after unpooling. This corresponds to the size of the\n",
    "                                 feature map *before* pooling in the corresponding encoder block.\n",
    "                                 It is crucial for MaxUnpool2d to correctly infer the output shape.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reconstructed dense feature map.\n",
    "        \"\"\"\n",
    "        # Unpooling using stored indices\n",
    "        # The unpool layer takes the low-res tensor 'x', the 'indices', and the 'output_size'.\n",
    "        # It places values from 'x' at the positions indicated by 'indices' in the larger 'output_size' grid,\n",
    "        # filling other positions with zeros. This creates a sparse feature map.\n",
    "        x = self.unpool(x, indices, output_size=output_size)\n",
    "\n",
    "        # First Conv -> BN (No ReLU here per SegNet-Basic specification)\n",
    "        x = self.bn1(self.conv1(x)) # Apply conv1, then bn1.\n",
    "\n",
    "        # Second Conv -> BN (No ReLU here per SegNet-Basic specification)\n",
    "        x = self.bn2(self.conv2(x)) # Apply conv2, then bn2.\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10df99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNetBasic(nn.Module):\n",
    "    \"\"\"\n",
    "    The full SegNet-Basic architecture for semantic segmentation.\n",
    "    Comprises 4 encoder blocks and 4 decoder blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=11, in_channels=3):\n",
    "        \"\"\"\n",
    "        Constructor for SegNetBasic.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of semantic classes for segmentation (e.g., 11 for CamVid).\n",
    "            in_channels (int): Number of input channels for the image (e.g., 3 for RGB).\n",
    "        \"\"\"\n",
    "        super(SegNetBasic, self).__init__()\n",
    "\n",
    "        # Define channel progression for Encoder and Decoder\n",
    "        # Encoder channels: 3 (input) -> 64 -> 128 -> 256 -> 512\n",
    "        # Decoder channels: 512 -> 256 -> 128 -> 64 -> num_classes (output)\n",
    "        self.encoder_channels = [in_channels, 64, 128, 256, 512]\n",
    "        self.decoder_channels = [512, 256, 128, 64] # Output of decoder blocks\n",
    "\n",
    "        # --- Encoder Pathway ---\n",
    "        # Each encoder block outputs the pooled tensor and its indices.\n",
    "        # We need to store these indices to pass to the corresponding decoder block.\n",
    "        self.enc1 = EncoderBlock(self.encoder_channels[0], self.encoder_channels[1]) # 3 -> 64 channels\n",
    "        self.enc2 = EncoderBlock(self.encoder_channels[1], self.encoder_channels[2]) # 64 -> 128 channels\n",
    "        self.enc3 = EncoderBlock(self.encoder_channels[2], self.encoder_channels[3]) # 128 -> 256 channels\n",
    "        self.enc4 = EncoderBlock(self.encoder_channels[3], self.encoder_channels[4]) # 256 -> 512 channels\n",
    "\n",
    "        # --- Decoder Pathway ---\n",
    "        # Each decoder block takes the upsampled tensor from the previous decoder,\n",
    "        # the indices from its corresponding encoder, and the original size before pooling.\n",
    "        self.dec1 = DecoderBlock(self.decoder_channels[0], self.decoder_channels[1]) # 512 -> 256 channels\n",
    "        self.dec2 = DecoderBlock(self.decoder_channels[1], self.decoder_channels[2]) # 256 -> 128 channels\n",
    "        self.dec3 = DecoderBlock(self.decoder_channels[2], self.decoder_channels[3]) # 128 -> 64 channels\n",
    "        # The last decoder block reduces to the number of classes.\n",
    "        self.dec4 = DecoderBlock(self.decoder_channels[3], num_classes) # 64 -> num_classes channels\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        # No additional classification layer is typically needed if the last decoder block\n",
    "        # outputs directly to num_classes. However, a 1x1 convolution can be used as a\n",
    "        # pixel-wise classifier to refine the final feature map into logit scores.\n",
    "        # In SegNet's original description, the output of the final decoder is fed to a softmax classifier.\n",
    "        # Here, the last DecoderBlock already outputs num_classes.\n",
    "        # We will apply log_softmax in the forward pass for numerical stability with NLLLoss.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for SegNetBasic.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (N, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log-probabilities for each pixel belonging to a class (N, num_classes, H, W).\n",
    "        \"\"\"\n",
    "        # Store original input size for the final unpooling step in the decoder\n",
    "        # This is needed because MaxUnpool2d sometimes requires the exact output size\n",
    "        # if the input dimensions are not perfectly divisible by stride.\n",
    "        input_size = x.size() # Example: (N, 3, 360, 480)\n",
    "\n",
    "        # --- Encoder Forward Pass ---\n",
    "        # Each step stores the output of the block *before* pooling (for decoder's output_size)\n",
    "        # and the indices from the pooling layer.\n",
    "        enc1, indices1 = self.enc1(x) # x: (N, 3, H, W) -> enc1: (N, 64, H/2, W/2)\n",
    "        size1 = x.size() # Store size *before* pooling for decoder unpooling later (N, 3, H, W)\n",
    "\n",
    "        enc2, indices2 = self.enc2(enc1) # enc1: (N, 64, H/2, W/2) -> enc2: (N, 128, H/4, W/4)\n",
    "        size2 = enc1.size() # (N, 64, H/2, W/2)\n",
    "\n",
    "        enc3, indices3 = self.enc3(enc2) # enc2: (N, 128, H/4, W/4) -> enc3: (N, 256, H/8, W/8)\n",
    "        size3 = enc2.size() # (N, 128, H/4, W/4)\n",
    "\n",
    "        # The deepest encoder output, which will be the input to the deepest decoder.\n",
    "        # This is the feature map with the lowest spatial resolution and highest semantic abstraction.\n",
    "        enc4, indices4 = self.enc4(enc3) # enc3: (N, 256, H/8, W/8) -> enc4: (N, 512, H/16, W/16)\n",
    "        size4 = enc3.size() # (N, 256, H/8, W/8)\n",
    "\n",
    "        # --- Decoder Forward Pass ---\n",
    "        # The deepest encoder output (enc4) is the input to the first (deepest) decoder block.\n",
    "        # Each decoder block takes:\n",
    "        # 1. The input tensor from the previous decoder block (or enc4 for the first one).\n",
    "        # 2. The max-pooling indices from the *corresponding* encoder block.\n",
    "        # 3. The 'output_size' which is the size of the feature map *before* pooling in the corresponding encoder.\n",
    "\n",
    "        # dec1 takes enc4 (512 channels) and indices4. Output is 256 channels.\n",
    "        dec1 = self.dec1(enc4, indices4, output_size=size4) # enc4: (N, 512, H/16, W/16) -> dec1: (N, 256, H/8, W/8)\n",
    "\n",
    "        # dec2 takes dec1 (256 channels) and indices3. Output is 128 channels.\n",
    "        dec2 = self.dec2(dec1, indices3, output_size=size3) # dec1: (N, 256, H/8, W/8) -> dec2: (N, 128, H/4, W/4)\n",
    "\n",
    "        # dec3 takes dec2 (128 channels) and indices2. Output is 64 channels.\n",
    "        dec3 = self.dec3(dec2, indices2, output_size=size2) # dec2: (N, 128, H/4, W/4) -> dec3: (N, 64, H/2, W/2)\n",
    "\n",
    "        # dec4 takes dec3 (64 channels) and indices1. Output is num_classes channels.\n",
    "        # Its output_size is the original input image size (before first encoder pooling).\n",
    "        output = self.dec4(dec3, indices1, output_size=size1) # dec3: (N, 64, H/2, W/2) -> output: (N, num_classes, H, W)\n",
    "\n",
    "        # For pixel-wise classification, we typically apply a Softmax layer\n",
    "        # to get probabilities for each class at each pixel.\n",
    "        # F.log_softmax is often used with nn.NLLLoss for numerical stability,\n",
    "        # or F.softmax if using a custom CrossEntropyLoss that expects probabilities.\n",
    "        # nn.CrossEntropyLoss in PyTorch already combines log_softmax and NLLLoss,\n",
    "        # so for training, you'd usually pass the raw 'output' directly to CrossEntropyLoss.\n",
    "        # For inference or visualizing probabilities, you might explicitly apply softmax.\n",
    "        return F.log_softmax(output, dim=1) # Apply log_softmax across the channel dimension (dim=1)\n",
    "                                            # to get log-probabilities for each class per pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d081c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Dataset Class (you'd replace this with a real dataset like CamVidDataset)\n",
    "class DummySegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=100, img_height=360, img_width=480, num_classes=11):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random dummy image (RGB)\n",
    "        image = torch.randn(3, self.img_height, self.img_width)\n",
    "        # Generate random dummy segmentation mask (class labels per pixel)\n",
    "        # Masks should be LongTensor and values from 0 to num_classes-1\n",
    "        mask = torch.randint(0, self.num_classes, (self.img_height, self.img_width), dtype=torch.long)\n",
    "        return image, mask\n",
    "\n",
    "# Instantiate the dataset and DataLoader\n",
    "# You would replace this with actual data loading logic\n",
    "# For example, using torchvision.datasets for standard datasets, or a custom dataset for CamVid.\n",
    "# E.g., for CamVid: image and mask transformations (resize, normalize, ToTensor) would be applied.\n",
    "train_dataset = DummySegmentationDataset()\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# For a realistic example, consider an image preprocessing step:\n",
    "# from torchvision import transforms\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize((360, 480)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# # For masks: only ToTensor and then convert to Long (no normalization)\n",
    "# mask_preprocess = transforms.Compose([\n",
    "#     transforms.Resize((360, 480)),\n",
    "#     transforms.ToTensor(),\n",
    "#     lambda x: x.squeeze().long() # Masks are typically 1-channel, ensure long type\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f50208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_classes = 11 # For example, CamVid dataset\n",
    "segnet_model = SegNetBasic(num_classes=num_classes, in_channels=3)\n",
    "\n",
    "# Define the loss function\n",
    "# nn.CrossEntropyLoss expects raw scores (logits) from the network,\n",
    "# and integer labels for the ground truth.\n",
    "# The network's final output is F.log_softmax(output, dim=1), so it gives log-probabilities.\n",
    "# Therefore, we should use nn.NLLLoss (Negative Log Likelihood Loss) if we apply log_softmax manually.\n",
    "# If we omit F.log_softmax in the model's forward and return 'output' directly, then we use nn.CrossEntropyLoss.\n",
    "# Let's adjust the SegNetBasic forward to return raw logits for direct use with nn.CrossEntropyLoss:\n",
    "# (Inside SegNetBasic.forward, change: `return F.log_softmax(output, dim=1)` to `return output`)\n",
    "\n",
    "# For this example, let's keep F.log_softmax and use NLLLoss.\n",
    "criterion = nn.NLLLoss() # This is suitable because our model outputs log-probabilities.\n",
    "                         # If the model outputted raw logits, nn.CrossEntropyLoss would be used.\n",
    "                         # nn.CrossEntropyLoss is more common in practice as it's an all-in-one.\n",
    "                         # For a practical SegNet, you'd likely remove the F.log_softmax from forward\n",
    "                         # and use nn.CrossEntropyLoss for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15676bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "learning_rate = 0.01 # A common starting learning rate\n",
    "momentum = 0.9       # As used in the paper for SGD\n",
    "# optimizer = torch.optim.SGD(segnet_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = torch.optim.Adam(segnet_model.parameters(), lr=learning_rate) # Adam is often easier to tune initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f38004",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # For demonstration, typically hundreds of epochs for real training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "segnet_model.to(device) # Move model to the selected device\n",
    "\n",
    "print(f\"Training SegNet-Basic on {device}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    segnet_model.train() # Set model to training mode.\n",
    "                         # This enables Dropout and Batch Normalization's training behavior.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        # 1. Move data to device\n",
    "        images = images.to(device) # Transfer input images to GPU (if cuda)\n",
    "        masks = masks.to(device)   # Transfer ground truth masks to GPU\n",
    "\n",
    "        # 2. Zero the gradients\n",
    "        # Before each new batch, gradients from the previous batch must be cleared.\n",
    "        # Otherwise, gradients would accumulate across batches.\n",
    "        optimizer.zero_grad() # Sets gradients of all optimized torch.Tensor to zero.\n",
    "\n",
    "        # 3. Forward pass\n",
    "        # model(images) calls the forward method of SegNetBasic.\n",
    "        outputs = segnet_model(images) # Shape: (N, num_classes, H, W). Log-probabilities.\n",
    "\n",
    "        # 4. Calculate loss\n",
    "        # The loss function compares the model's predictions (outputs) with the true labels (masks).\n",
    "        loss = criterion(outputs, masks) # NLLLoss expects log-probabilities and class indices.\n",
    "\n",
    "        # 5. Backward pass\n",
    "        # Computes gradients of the loss with respect to all model parameters that require gradients.\n",
    "        # These gradients are then stored in the .grad attribute of each parameter tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # 6. Optimizer step\n",
    "        # Updates the model's weights using the computed gradients and the optimizer's algorithm (e.g., Adam).\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() # .item() retrieves the Python number from a single-element tensor.\n",
    "\n",
    "        # Optional: Print loss periodically\n",
    "        if (batch_idx + 1) % 50 == 0: # Print every 50 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss / (batch_idx + 1):.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Optional: Evaluation on a validation set after each epoch (highly recommended for real training)\n",
    "    # segnet_model.eval() # Set model to evaluation mode (disables Dropout, uses population stats for BatchNorm)\n",
    "    # with torch.no_grad(): # Disable gradient calculations for evaluation (saves memory and speeds up)\n",
    "    #     val_loss = 0.0\n",
    "    #     for images, masks in val_loader:\n",
    "    #         images = images.to(device)\n",
    "    #         masks = masks.to(device)\n",
    "    #         outputs = segnet_model(images)\n",
    "    #         loss = criterion(outputs, masks)\n",
    "    #         val_loss += loss.item()\n",
    "    #     print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Re-define our EncoderBlock and DecoderBlock for clarity within this example context\n",
    "class EncoderBlockTrace(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlockTrace, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "    def forward(self, x):\n",
    "        print(f\"  Encoder Input shape: {x.shape}\")\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        pre_pool_size = x.size() # Store size *before* pooling\n",
    "        pooled_x, indices = self.pool(x)\n",
    "        print(f\"  Encoder Post-Pool shape: {pooled_x.shape}, Pre-Pool size for decoder: {pre_pool_size}\")\n",
    "        return pooled_x, indices, pre_pool_size\n",
    "\n",
    "class DecoderBlockTrace(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlockTrace, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=7, padding=3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x, indices, output_size):\n",
    "        print(f\"  Decoder Input shape: {x.shape}, Target Unpool Size: {output_size}, Indices shape: {indices.shape}\")\n",
    "        x = self.unpool(x, indices, output_size=output_size)\n",
    "        print(f\"  Decoder Post-Unpool (Sparse) shape: {x.shape}\")\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        print(f\"  Decoder Post-Conv (Dense) shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class SegNetBasicTrace(nn.Module):\n",
    "    def __init__(self, num_classes=2, in_channels=3):\n",
    "        super(SegNetBasicTrace, self).__init__()\n",
    "        self.encoder_channels = [in_channels, 64, 128, 256, 512]\n",
    "        self.decoder_channels = [512, 256, 128, 64]\n",
    "\n",
    "        self.enc1 = EncoderBlockTrace(self.encoder_channels[0], self.encoder_channels[1])\n",
    "        self.enc2 = EncoderBlockTrace(self.encoder_channels[1], self.encoder_channels[2])\n",
    "        self.enc3 = EncoderBlockTrace(self.encoder_channels[2], self.encoder_channels[3])\n",
    "        self.enc4 = EncoderBlockTrace(self.encoder_channels[3], self.encoder_channels[4])\n",
    "\n",
    "        self.dec1 = DecoderBlockTrace(self.decoder_channels[0], self.decoder_channels[1])\n",
    "        self.dec2 = DecoderBlockTrace(self.decoder_channels[1], self.decoder_channels[2])\n",
    "        self.dec3 = DecoderBlockTrace(self.decoder_channels[2], self.decoder_channels[3])\n",
    "        self.dec4 = DecoderBlockTrace(self.decoder_channels[3], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\n--- Encoder Pathway ---\")\n",
    "        enc1, indices1, size1 = self.enc1(x)\n",
    "        enc2, indices2, size2 = self.enc2(enc1)\n",
    "        enc3, indices3, size3 = self.enc3(enc2)\n",
    "        enc4, indices4, size4 = self.enc4(enc3)\n",
    "\n",
    "        print(\"\\n--- Decoder Pathway ---\")\n",
    "        dec1 = self.dec1(enc4, indices4, output_size=size4)\n",
    "        dec2 = self.dec2(dec1, indices3, output_size=size3)\n",
    "        dec3 = self.dec3(dec2, indices2, output_size=size2)\n",
    "        output = self.dec4(dec3, indices1, output_size=size1)\n",
    "\n",
    "        print(f\"\\n--- Final Output ---\")\n",
    "        print(f\"Final output shape (logits): {output.shape}\")\n",
    "        return F.log_softmax(output, dim=1) # Applying log_softmax for the final output\n",
    "\n",
    "# Instantiate and run a dummy forward pass\n",
    "dummy_input = torch.randn(1, 3, 16, 16) # N=1, C=3, H=16, W=16\n",
    "segnet_trace_model = SegNetBasicTrace(num_classes=2, in_channels=3)\n",
    "segnet_trace_model.eval() # Set to eval mode for consistent BN behavior (though not trained)\n",
    "\n",
    "print(f\"Starting SegNet-Basic Trace with input shape: {dummy_input.shape}\")\n",
    "traced_output = segnet_trace_model(dummy_input)\n",
    "print(f\"Log-softmax output shape: {traced_output.shape}\")\n",
    "\n",
    "# Expected output shapes:\n",
    "# Encoder:\n",
    "# (1, 3, 16, 16) -> (1, 64, 8, 8)\n",
    "# (1, 64, 8, 8) -> (1, 128, 4, 4)\n",
    "# (1, 128, 4, 4) -> (1, 256, 2, 2)\n",
    "# (1, 256, 2, 2) -> (1, 512, 1, 1) # Deepest encoder output\n",
    "\n",
    "# Decoder (Input to decoder, indices from corresponding encoder, output_size for unpooling)\n",
    "# (1, 512, 1, 1) + indices4 + size4 (1,256,2,2) -> (1, 256, 2, 2)\n",
    "# (1, 256, 2, 2) + indices3 + size3 (1,128,4,4) -> (1, 128, 4, 4)\n",
    "# (1, 128, 4, 4) + indices2 + size2 (1,64,8,8) -> (1, 64, 8, 8)\n",
    "# (1, 64, 8, 8) + indices1 + size1 (1,3,16,16) -> (1, 2, 16, 16) # Final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c748f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
