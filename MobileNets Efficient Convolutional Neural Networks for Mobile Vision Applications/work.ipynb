{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import math # For potential rounding/floor operations if needed, though int() conversion often suffices\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model Hyperparameters (Adjust as needed)\n",
    "WIDTH_MULTIPLIER = 1.0 # Alpha (typical values: 1.0, 0.75, 0.5, 0.25)\n",
    "NUM_CLASSES = 10        # For CIFAR-10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20 # Keep low for demonstration; increase for real training\n",
    "\n",
    "# --- Building Blocks ---\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNet V1 Depthwise Separable Convolution block.\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        stride (int): Stride for the depthwise convolution. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "\n",
    "        # --- Depthwise Convolution ---\n",
    "        # Applies spatial filtering independently for each input channel.\n",
    "        # groups=in_channels makes it depthwise.\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels, # Output channels = Input channels for depthwise\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,     # Preserves spatial dimensions for stride 1\n",
    "            groups=in_channels,\n",
    "            bias=False     # BatchNorm has bias, so conv bias is redundant\n",
    "        )\n",
    "        self.bn_depthwise = nn.BatchNorm2d(in_channels)\n",
    "        self.relu_depthwise = nn.ReLU(inplace=True)\n",
    "\n",
    "        # --- Pointwise Convolution ---\n",
    "        # 1x1 convolution to combine channel information.\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False     # BatchNorm has bias\n",
    "        )\n",
    "        self.bn_pointwise = nn.BatchNorm2d(out_channels)\n",
    "        self.relu_pointwise = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.bn_depthwise(x)\n",
    "        x = self.relu_depthwise(x)\n",
    "\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn_pointwise(x)\n",
    "        x = self.relu_pointwise(x)\n",
    "        return x\n",
    "\n",
    "# --- MobileNet V1 Architecture ---\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNet V1 implementation with Width Multiplier (alpha).\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Width multiplier, controls the number of channels.\n",
    "                       alpha=1.0 is the baseline MobileNet.\n",
    "        num_classes (int): Number of output classes for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, num_classes=1000):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Helper function to apply width multiplier\n",
    "        def apply_alpha(channels):\n",
    "            return max(1, int(channels * self.alpha)) # Ensure at least 1 channel\n",
    "\n",
    "        # --- Stem ---\n",
    "        # Initial standard convolution layer\n",
    "        # Input: 224x224x3 (assumed ImageNet size) -> Output: 112x112x(32*alpha)\n",
    "        initial_channels = apply_alpha(32)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, initial_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(initial_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        current_channels = initial_channels\n",
    "\n",
    "        # --- Body: Stacked Depthwise Separable Convolutions ---\n",
    "        # Layer specifications based on Table 1, modified by alpha\n",
    "        # Format: (output_baseline_channels, stride)\n",
    "        layer_configs = [\n",
    "            (64, 1),\n",
    "            (128, 2),\n",
    "            (128, 1),\n",
    "            (256, 2),\n",
    "            (256, 1),\n",
    "            (512, 2),\n",
    "            # Repeat 5 times\n",
    "            (512, 1), (512, 1), (512, 1), (512, 1), (512, 1),\n",
    "            (1024, 2),\n",
    "            # Note: Stride for the last 1024 block is debated.\n",
    "            # Paper Table 1 implies s2, but output size suggests s1.\n",
    "            # Common implementations often use s1 here to get 7x7 feature map before pooling.\n",
    "            # We use s1 here. If s2 is used, the AdaptiveAvgPool2d handles it.\n",
    "            (1024, 1)\n",
    "        ]\n",
    "\n",
    "        body_layers = []\n",
    "        for out_baseline_channels, stride in layer_configs:\n",
    "            out_channels_alpha = apply_alpha(out_baseline_channels)\n",
    "            body_layers.append(\n",
    "                DepthwiseSeparableConv(current_channels, out_channels_alpha, stride=stride)\n",
    "            )\n",
    "            current_channels = out_channels_alpha # Update channels for next layer input\n",
    "\n",
    "        self.body = nn.Sequential(*body_layers)\n",
    "\n",
    "        # --- Classifier Head ---\n",
    "        # Global Average Pooling and Fully Connected Layer\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1)) # Output: B x C x 1 x 1\n",
    "        self.flatten = nn.Flatten()             # Output: B x C\n",
    "        self.fc = nn.Linear(current_channels, self.num_classes) # No activation/BN needed before loss\n",
    "\n",
    "        # --- Weight Initialization (Optional but Recommended) ---\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.body(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x) # Logits output\n",
    "        return x\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "# Normalization for CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Note: MobileNet expects 224x224 input typically.\n",
    "# For CIFAR-10 (32x32), we either need to resize or accept that the\n",
    "# effective receptive field and downsampling might behave differently.\n",
    "# For simplicity here, we use CIFAR-10 directly.\n",
    "# For real use, add transforms.Resize(224) if using ImageNet-based model directly.\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- Model, Loss, Optimizer ---\n",
    "\n",
    "print(f\"Initializing MobileNetV1 with alpha={WIDTH_MULTIPLIER}...\")\n",
    "model = MobileNetV1(alpha=WIDTH_MULTIPLIER, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# --- Sanity Check: Print Model Summary and Parameter Count ---\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized. Parameter count: {count_parameters(model):,}\")\n",
    "# You can optionally print the model structure: print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - Paper uses RMSprop, Adam is also common\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, eps=1e-08, weight_decay=0.00004, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) # Adam often works well out-of-the-box\n",
    "\n",
    "# Learning rate scheduler (optional, but often helpful)\n",
    "# Example: Step decay\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i + 1) % 100 == 0: # Print every 100 mini-batches\n",
    "             print(f'[Epoch {epoch + 1}/{NUM_EPOCHS}, Batch {i + 1}/{len(trainloader)}] '\n",
    "                   f'Loss: {running_loss / 100:.3f} | '\n",
    "                   f'Acc: {100 * correct / total:.2f}%')\n",
    "             running_loss = 0.0 # Reset loss average\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch + 1} Training finished. Accuracy: {epoch_acc:.2f}%, Time: {epoch_time:.2f}s')\n",
    "    return epoch_acc\n",
    "\n",
    "# --- Validation Loop ---\n",
    "\n",
    "def validate():\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients during validation\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0) # Accumulate weighted loss\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_time = time.time() - start_time\n",
    "    val_acc = 100 * correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    print(f'Validation Accuracy: {val_acc:.2f}%, Avg Loss: {avg_loss:.4f}, Time: {val_time:.2f}s')\n",
    "    print(\"-\" * 30)\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "# --- Main Training Execution ---\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_acc = validate()\n",
    "\n",
    "    # Optional: Update learning rate scheduler\n",
    "    # if scheduler:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    # Optional: Save the model checkpoint if validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        print(f\"Validation accuracy improved ({best_val_acc:.2f}% -> {val_acc:.2f}%). Saving model...\")\n",
    "        torch.save(model.state_dict(), f'mobilenetv1_alpha{WIDTH_MULTIPLIER}_cifar10_best.pth')\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "print('Finished Training')\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# --- Optional: Load best model and evaluate again ---\n",
    "# print(\"Loading best model for final evaluation...\")\n",
    "# model.load_state_dict(torch.load(f'mobilenetv1_alpha{WIDTH_MULTIPLIER}_cifar10_best.pth'))\n",
    "# validate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
