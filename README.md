# History-of-Deep-Learning

A curated and hands-on collection of seminal deep learning architectures and research papers that shaped the field of Computer Vision and Natural Language Processing. This repository provides organized code, notes, and implementations for key breakthroughs, intended for deep learning learners, researchers, and enthusiasts.


| Paper / Architecture                                                         | Year | Contribution                                                                    |
| ---------------------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------- |
| **AlexNet**                                                                  | 2012 | First deep CNN to win ImageNet, reignited interest in deep learning             |
| **VGGNet** (*Very Deep Convolutional Networks*)                              | 2014 | Demonstrated that stacking small 3Ã—3 filters is highly effective                |
| **GoogLeNet / Inception**                                                    | 2014 | Introduced Inception modules; deeper and computationally efficient              |
| **ResNet** (*Deep Residual Networks*)                                        | 2015 | Solved vanishing gradients using identity-based skip connections                |
| **MobileNet**                                                                | 2017 | Designed for mobile and embedded devices using depthwise separable convolutions |
| **DenseNet** (*Densely Connected Convolutional Networks*)                    | 2017 | Each layer receives input from all previous layers                              |
| **OverFeat**                                                                 | 2013 | Unified framework for classification, localization, and detection               |
| **Spatial Pyramid Pooling (SPP-Net)**                                        | 2014 | Enabled CNNs to handle arbitrary image sizes using spatial pyramids             |
| **Fast R-CNN**                                                               | 2015 | Fast object detection architecture built on region proposals                    |
| **PReLU (Delving Deep into Rectifiers)**                                     | 2015 | Proposed Parametric ReLU activation for deeper network training                 |
| **Rethinking the Inception Architecture**                                    | 2015 | Inception-v3 with batch normalization, factorized convolutions                  |
| **Rich Feature Hierarchies (R-CNN)**                                         | 2014 | Pioneered region-based CNN object detection                                     |
| **Efficient Estimation of Word Representations**                             | 2013 | Introduced Word2Vec (Skip-gram and CBOW) for NLP embeddings                     |
| **Distributed Representations of Words and Phrases**                         | 2013 | Improved phrase embeddings using negative sampling                              |
| **Attention is All You Need (Transformer)**                                  | 2017 | Revolutionized NLP and vision with self-attention mechanism                     |
| **Seq2Seq with LSTMs**                                                       | 2014 | Sequence-to-sequence model using encoder-decoder LSTMs                          |
| **Encoder-Decoder with Atrous Convolution (DeepLab)**                        | 2017 | Atrous (dilated) convolutions for segmentation tasks                            |
| **Xception**                                                                 | 2017 | Depthwise separable convolutions taken to an extreme                            |
| **High-Resolution Network (HRNet)**                                          | 2019 | Maintains high-resolution representations through the network                   |
| **Segmentation Transformer (SETR)**                                          | 2021 | First pure-transformer model for semantic segmentation                          |
| **Dual Attention Network**                                                   | 2019 | Introduced spatial and channel attention mechanisms                             |
| **Rethinking Atrous Convolution**                                            | 2021 | New design for semantic segmentation with atrous spatial pyramid pooling        |
| **U-Net**                                                                    | 2015 | Popular architecture for biomedical image segmentation                          |
| **SegNet**                                                                   | 2015 | Encoder-decoder architecture for pixel-wise segmentation                        |
| **Neural Machine Translation by Jointly Learning to Align and Translate**    | 2014 | Introduced attention-based NMT for sequence translation                         |
