{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151bb70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:22:58,459 : INFO : collecting all words and their counts\n",
      "2025-04-13 17:22:58,459 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-13 17:22:58,460 : INFO : collected 51 word types from a corpus of 67 raw words and 10 sentences\n",
      "2025-04-13 17:22:58,460 : INFO : Creating a fresh vocabulary\n",
      "2025-04-13 17:22:58,462 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 12 unique words (23.53% of original 51, drops 39)', 'datetime': '2025-04-13T17:22:58.462697', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-13 17:22:58,462 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 28 word corpus (41.79% of original 67, drops 39)', 'datetime': '2025-04-13T17:22:58.462697', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-13 17:22:58,462 : INFO : deleting the raw counts dictionary of 51 items\n",
      "2025-04-13 17:22:58,464 : INFO : sample=1e-05 downsamples 12 most-common words\n",
      "2025-04-13 17:22:58,464 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 0.3086055670287033 word corpus (1.1%% of prior 28)', 'datetime': '2025-04-13T17:22:58.464017', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-13 17:22:58,465 : INFO : estimated required memory for 12 words and 50 dimensions: 10800 bytes\n",
      "2025-04-13 17:22:58,465 : INFO : resetting layer weights\n",
      "2025-04-13 17:22:58,466 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-13T17:22:58.466841', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-04-13 17:22:58,466 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 12 vocabulary and 50 features, using sg=1 hs=0 sample=1e-05 negative=5 window=2 shrink_windows=True', 'datetime': '2025-04-13T17:22:58.466841', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-04-13 17:22:58,466 : INFO : EPOCH 0: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,472 : INFO : EPOCH 1: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,475 : INFO : EPOCH 2: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,475 : INFO : EPOCH 3: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,480 : INFO : EPOCH 4: training on 67 raw words (2 effective words) took 0.0s, 4908 effective words/s\n",
      "2025-04-13 17:22:58,480 : INFO : EPOCH 5: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,480 : INFO : EPOCH 6: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,486 : INFO : EPOCH 7: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,488 : INFO : EPOCH 8: training on 67 raw words (1 effective words) took 0.0s, 2293 effective words/s\n",
      "2025-04-13 17:22:58,493 : INFO : EPOCH 9: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,494 : INFO : EPOCH 10: training on 67 raw words (1 effective words) took 0.0s, 2000 effective words/s\n",
      "2025-04-13 17:22:58,494 : INFO : EPOCH 11: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,499 : INFO : EPOCH 12: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,501 : INFO : EPOCH 13: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,504 : INFO : EPOCH 14: training on 67 raw words (1 effective words) took 0.0s, 3018 effective words/s\n",
      "2025-04-13 17:22:58,504 : INFO : EPOCH 15: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,510 : INFO : EPOCH 16: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,510 : INFO : EPOCH 17: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,510 : INFO : EPOCH 18: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,517 : INFO : EPOCH 19: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,518 : INFO : EPOCH 20: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,521 : INFO : EPOCH 21: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,523 : INFO : EPOCH 22: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,523 : INFO : EPOCH 23: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,528 : INFO : EPOCH 24: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,531 : INFO : EPOCH 25: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,533 : INFO : EPOCH 26: training on 67 raw words (1 effective words) took 0.0s, 2919 effective words/s\n",
      "2025-04-13 17:22:58,535 : INFO : EPOCH 27: training on 67 raw words (1 effective words) took 0.0s, 12723 effective words/s\n",
      "2025-04-13 17:22:58,536 : INFO : EPOCH 28: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,539 : INFO : EPOCH 29: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,541 : INFO : EPOCH 30: training on 67 raw words (1 effective words) took 0.0s, 3019 effective words/s\n",
      "2025-04-13 17:22:58,545 : INFO : EPOCH 31: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,549 : INFO : EPOCH 32: training on 67 raw words (1 effective words) took 0.0s, 1523 effective words/s\n",
      "2025-04-13 17:22:58,551 : INFO : EPOCH 33: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,552 : INFO : EPOCH 34: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,552 : INFO : EPOCH 35: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,557 : INFO : EPOCH 36: training on 67 raw words (1 effective words) took 0.0s, 55249 effective words/s\n",
      "2025-04-13 17:22:58,560 : INFO : EPOCH 37: training on 67 raw words (1 effective words) took 0.0s, 1801 effective words/s\n",
      "2025-04-13 17:22:58,563 : INFO : EPOCH 38: training on 67 raw words (2 effective words) took 0.0s, 20747 effective words/s\n",
      "2025-04-13 17:22:58,566 : INFO : EPOCH 39: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,566 : INFO : EPOCH 40: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,569 : INFO : EPOCH 41: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,571 : INFO : EPOCH 42: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,576 : INFO : EPOCH 43: training on 67 raw words (1 effective words) took 0.0s, 6321 effective words/s\n",
      "2025-04-13 17:22:58,578 : INFO : EPOCH 44: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,578 : INFO : EPOCH 45: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,581 : INFO : EPOCH 46: training on 67 raw words (1 effective words) took 0.0s, 1764 effective words/s\n",
      "2025-04-13 17:22:58,584 : INFO : EPOCH 47: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,584 : INFO : EPOCH 48: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,589 : INFO : EPOCH 49: training on 67 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2025-04-13 17:22:58,589 : INFO : Word2Vec lifecycle event {'msg': 'training on 3350 raw words (15 effective words) took 0.1s, 122 effective words/s', 'datetime': '2025-04-13T17:22:58.589832', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-04-13 17:22:58,591 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12, vector_size=50, alpha=0.025>', 'datetime': '2025-04-13T17:22:58.591454', 'gensim': '4.3.3', 'python': '3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized data for Gensim: ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'are', 'related', 'fields']\n",
      "\n",
      "--- Training Gensim Word2Vec (Skip-gram) Model ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "Vector for 'language' (shape (50,)):\n",
      " [ 0.01563514 -0.01902037 -0.00041106  0.00693839 -0.00187794  0.01676354\n",
      "  0.01802157  0.01307301 -0.00142324  0.01542081]\n",
      "\n",
      "Words most similar to 'language': [('representations', 0.0449172779917717), ('models', -0.010146022774279118), ('embeddings', -0.014475265517830849), ('machine', -0.023209011182188988), ('capture', -0.04407211393117905)]\n",
      "Words most similar to 'embeddings': [('processing', 0.16704076528549194), ('representations', 0.13204392790794373), ('learning', 0.1267007291316986), ('are', 0.0998455360531807), ('text', 0.042373016476631165)]\n",
      "\n",
      "Vocabulary size in Gensim model: 12\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import logging # Optional: To see Gensim's progress logs\n",
    "\n",
    "# --- 1. Sample Data (Tokenized) ---\n",
    "# Gensim expects input as a list of lists of tokens (like our previous example)\n",
    "corpus = [\n",
    "    \"natural language processing and machine learning are related fields\",\n",
    "    \"language models help understand text structure\",\n",
    "    \"vector representations capture word meanings\",\n",
    "    \"skip gram models learn embeddings from text\",\n",
    "    \"negative sampling improves training efficiency\",\n",
    "    \"word embeddings are useful for many nlp tasks\",\n",
    "    \"learning representations requires large amounts of text data\",\n",
    "    \"understanding context is key in language processing\",\n",
    "    \"machine learning algorithms power modern nlp\",\n",
    "    \"embeddings capture semantic relationships between words\"\n",
    "]\n",
    "\n",
    "# Simple tokenization (same as before)\n",
    "import re\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_corpus = [tokenize(sentence) for sentence in corpus]\n",
    "print(\"Sample tokenized data for Gensim:\", tokenized_corpus[0])\n",
    "\n",
    "# --- 2. Configure Logging (Optional) ---\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- 3. Train the Skip-gram Model ---\n",
    "print(\"\\n--- Training Gensim Word2Vec (Skip-gram) Model ---\")\n",
    "\n",
    "# Instantiate and train the model\n",
    "# Key parameters for Skip-gram:\n",
    "#   sentences: Your tokenized corpus\n",
    "#   vector_size: Dimensionality of the embeddings (e.g., 100)\n",
    "#   window: Context window size (e.g., 5)\n",
    "#   min_count: Minimum word frequency to include in vocab (e.g., 1 or 2)\n",
    "#   sg=1: THIS ENABLES SKIP-GRAM (sg=0 is for CBOW)\n",
    "#   workers: Number of CPU cores to use (e.g., 4)\n",
    "#   negative: Number of negative samples (e.g., 5). If > 0, enables Negative Sampling.\n",
    "#   hs=0: Disable Hierarchical Softmax (to use Negative Sampling). hs=1 enables HS.\n",
    "#   sample: Threshold for configuring subsampling of frequent words (e.g., 1e-3 or 1e-5)\n",
    "#   epochs: Number of iterations (epochs) over the corpus (Word2Vec default is 5)\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus,\n",
    "                 vector_size=50,    # Same as EMBEDDING_DIM before\n",
    "                 window=2,          # Same as WINDOW_SIZE before\n",
    "                 min_count=2,       # Same as MIN_WORD_FREQ before\n",
    "                 sg=1,              # Use Skip-gram\n",
    "                 workers=4,         # Use 4 CPU cores\n",
    "                 negative=5,        # Use Negative Sampling with 5 samples\n",
    "                 hs=0,              # Don't use Hierarchical Softmax\n",
    "                 sample=1e-5,       # Use subsampling with threshold 1e-5\n",
    "                 epochs=50)         # Train for 50 epochs\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- 4. Using the Model ---\n",
    "\n",
    "# Get the embedding vector for a word\n",
    "try:\n",
    "    vector_language = model.wv['language']\n",
    "    print(f\"\\nVector for 'language' (shape {vector_language.shape}):\\n\", vector_language[:10]) # Print first 10 dims\n",
    "except KeyError:\n",
    "    print(\"\\n'language' not in vocabulary (check min_count).\")\n",
    "\n",
    "# Find words most similar to a given word\n",
    "try:\n",
    "    similar_words = model.wv.most_similar('language', topn=5)\n",
    "    print(\"\\nWords most similar to 'language':\", similar_words)\n",
    "except KeyError:\n",
    "     print(\"\\n'language' not in vocabulary.\")\n",
    "\n",
    "try:\n",
    "    similar_words_emb = model.wv.most_similar('embeddings', topn=5)\n",
    "    print(\"Words most similar to 'embeddings':\", similar_words_emb)\n",
    "except KeyError:\n",
    "     print(\"\\n'embeddings' not in vocabulary.\")\n",
    "\n",
    "# Analogy task: king - man + woman = queen\n",
    "# Example: learning - machine + language = ? (might not work well on tiny corpus)\n",
    "# try:\n",
    "#     analogy = model.wv.most_similar(positive=['language', 'learning'], negative=['machine'], topn=1)\n",
    "#     print(\"\\nAnalogy 'learning' - 'machine' + 'language' = ?\", analogy)\n",
    "# except KeyError as e:\n",
    "#     print(f\"\\nAnalogy failed: Word '{e.args[0]}' not in vocabulary.\")\n",
    "\n",
    "\n",
    "# Get vocabulary details\n",
    "vocab_keys = list(model.wv.key_to_index.keys())\n",
    "print(f\"\\nVocabulary size in Gensim model: {len(vocab_keys)}\")\n",
    "# print(\"Sample vocab keys:\", vocab_keys[:10])\n",
    "\n",
    "# --- 5. Saving and Loading ---\n",
    "# model.save(\"skipgram_model.gensim\")\n",
    "# loaded_model = Word2Vec.load(\"skipgram_model.gensim\")\n",
    "# print(\"\\nModel saved and loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
