{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b81b73",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a578b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bebaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two 3x3 convolutional layers, \n",
    "    each followed by Batch Normalization and ReLU activation.\n",
    "    This is the fundamental building block throughout the U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the DoubleConv block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input feature channels.\n",
    "            out_channels (int): Number of output feature channels after the block.\n",
    "        \"\"\"\n",
    "        super().__init__() # Call the constructor of the parent class (nn.Module)\n",
    "        \n",
    "        # Define the sequence of operations for the double convolution block\n",
    "        # nn.Sequential allows chaining multiple modules together\n",
    "        self.conv = nn.Sequential(\n",
    "            # First Convolutional Layer\n",
    "            # nn.Conv2d: Applies a 2D convolution over an input signal composed of several input planes.\n",
    "            # in_channels: Number of channels in the input image/feature map.\n",
    "            # out_channels: Number of channels produced by the convolution.\n",
    "            # kernel_size=3: The size of the convolutional kernel (3x3 pixels).\n",
    "            # padding=0: Crucial for the original U-Net design. No padding is added, \n",
    "            #            which means the output feature map will be smaller than the input.\n",
    "            #            This is why the 'copy and crop' mechanism is needed for skip connections.\n",
    "            # bias=False: Typically set to False when Batch Normalization is used, \n",
    "            #             as Batch Norm introduces its own learnable bias.\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0, bias=False),\n",
    "            # nn.BatchNorm2d: Applies Batch Normalization over a 4D input (batch_size, channels, height, width).\n",
    "            #                 It normalizes the activations of the previous layer, \n",
    "            #                 stabilizing training and accelerating convergence.\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # nn.ReLU: Rectified Linear Unit activation function. f(x) = max(0, x).\n",
    "            #          Introduces non-linearity, allowing the network to learn complex patterns.\n",
    "            # inplace=True: Modifies the input tensor directly, saving memory by avoiding creating a new tensor.\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second Convolutional Layer (similar to the first one, but input channels are now out_channels)\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the DoubleConv block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input feature map to the block.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The output feature map after passing through the block.\n",
    "        \"\"\"\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22657cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling block in the U-Net's contracting path.\n",
    "    Consists of a MaxPool2d layer followed by a DoubleConv block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the Down block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input feature channels (from the previous DoubleConv).\n",
    "            out_channels (int): Number of output feature channels for the DoubleConv in this block.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # MaxPool2d: Reduces the spatial dimensions (height, width) of the input.\n",
    "        # kernel_size=2, stride=2: Takes the maximum value in a 2x2 window and moves 2 pixels at a time.\n",
    "        #                     This effectively halves the height and width of the feature map.\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # After max-pooling, apply the DoubleConv block to process the downsampled features.\n",
    "            # The input channels to DoubleConv are the same as the input to MaxPool2d.\n",
    "            # The output channels are then defined by out_channels, typically doubling with each downsampling step.\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the Down block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input feature map to the block.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The output feature map after downsampling and convolution.\n",
    "        \"\"\"\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fde9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block in the U-Net's expanding path.\n",
    "    Consists of a ConvTranspose2d (up-convolution), concatenation with a cropped skip connection,\n",
    "    and a DoubleConv block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the Up block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input feature channels for the ConvTranspose2d. \n",
    "                               This will be the combined channels from the lower (bottleneck) layer.\n",
    "            out_channels (int): Number of output feature channels for the DoubleConv in this block.\n",
    "                                This is typically half of the input channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.ConvTranspose2d: Performs transposed convolution (often called \"deconvolution\").\n",
    "        #                      It effectively upsamples the feature map.\n",
    "        # in_channels: Channels from the previous (lower) layer in the expanding path.\n",
    "        # out_channels: Half of the in_channels, as specified in the U-Net diagram \n",
    "        #               (\"halves the number of feature channels\").\n",
    "        # kernel_size=2, stride=2: Upsamples the feature map by a factor of 2 in both height and width.\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        # DoubleConv: Applied after concatenation of upsampled features and skip connection.\n",
    "        # Its input channels are the sum of upsampled channels and skip connection channels.\n",
    "        # up_in_channels (in_channels // 2) + skip_in_channels (in_channels // 2) = in_channels\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the Up block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input feature map from the previous (lower) upsampling block.\n",
    "            skip_connection (torch.Tensor): The corresponding high-resolution feature map \n",
    "                                            copied from the contracting path.\n",
    "                                            \n",
    "        Returns:\n",
    "            torch.Tensor: The output feature map after upsampling, concatenation, and convolution.\n",
    "        \"\"\"\n",
    "        # 1. Upsample the input from the lower layer\n",
    "        x = self.up(x) # x's spatial dimensions are now larger.\n",
    "\n",
    "        # 2. Handle the \"copy and crop\" mechanism for the skip connection\n",
    "        # Due to 'padding=0' in Conv2d layers, the encoder's feature maps are larger than the decoder's.\n",
    "        # We need to center-crop the skip_connection to match the spatial dimensions of the upsampled 'x'.\n",
    "        # Calculate the difference in dimensions (skip_connection is larger)\n",
    "        diff_y = skip_connection.size()[2] - x.size()[2]\n",
    "        diff_x = skip_connection.size()[3] - x.size()[3]\n",
    "\n",
    "        # Use F.pad to pad the upsampled 'x' tensor to match the dimensions of the skip_connection.\n",
    "        # This is a common practice when the upsampled tensor might be slightly smaller than expected\n",
    "        # due to odd dimensions or slight inconsistencies in ConvTranspose2d output size.\n",
    "        # The padding amounts are applied to (left, right, top, bottom)\n",
    "        # Note: The original paper describes cropping the skip_connection. \n",
    "        # In PyTorch implementations, sometimes padding the upsampled tensor is done for convenience\n",
    "        # if the upsampled output is slightly off by 1 pixel (e.g., 56x56 vs 57x57).\n",
    "        # Let's stick to the paper's literal \"copy and crop\" (cropping skip_connection)\n",
    "        # for a more direct interpretation.\n",
    "        \n",
    "        # Center-crop the skip_connection to match the size of x\n",
    "        # Calculate the starting and ending indices for cropping\n",
    "        crop_start_y = diff_y // 2\n",
    "        crop_end_y = skip_connection.size()[2] - (diff_y - diff_y // 2)\n",
    "        crop_start_x = diff_x // 2\n",
    "        crop_end_x = skip_connection.size()[3] - (diff_x - diff_x // 2)\n",
    "        \n",
    "        cropped_skip_connection = skip_connection[:, :, crop_start_y:crop_end_y, crop_start_x:crop_end_x]\n",
    "\n",
    "        # 3. Concatenate the upsampled features with the (cropped) skip connection\n",
    "        # dim=1 means concatenating along the channel dimension.\n",
    "        # This combines the precise spatial information from the encoder with the semantic context from the decoder.\n",
    "        x = torch.cat([cropped_skip_connection, x], dim=1) # The order matters conceptually but not mathematically here.\n",
    "        \n",
    "        # 4. Apply the DoubleConv block to the concatenated features\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9191fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Final output convolutional layer.\n",
    "    Maps the number of feature channels to the number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the OutConv layer.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input feature channels (from the last DoubleConv in the expanding path).\n",
    "            out_channels (int): Number of output classes for segmentation (e.g., 2 for foreground/background).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size=1):\n",
    "        # A 1x1 convolution is used to linearly combine the feature channels \n",
    "        # into the desired number of class scores for each pixel.\n",
    "        # It's essentially a fully connected layer operating on each pixel individually.\n",
    "        # padding=0 is default for kernel_size=1, so no change in spatial dimensions.\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the OutConv layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input feature map from the final Up block.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The output logits/scores for each class per pixel.\n",
    "                          These will typically be passed through a softmax or sigmoid \n",
    "                          activation outside the network for probabilities.\n",
    "        \"\"\"\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801aefe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The full U-Net architecture for biomedical image segmentation.\n",
    "    Consists of a contracting path (encoder) and an expanding path (decoder)\n",
    "    with skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        \"\"\"\n",
    "        Initializes the U-Net model.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input image channels (e.g., 1 for grayscale, 3 for RGB).\n",
    "            num_classes (int): Number of output segmentation classes (e.g., 2 for background/foreground).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Contracting Path (Encoder) ---\n",
    "        # The paper's architecture has 5 levels of feature maps:\n",
    "        # Level 1 (input_channels -> 64)\n",
    "        self.inc = DoubleConv(in_channels, 64) \n",
    "        \n",
    "        # Level 2 (64 -> 128, with downsampling)\n",
    "        self.down1 = Down(64, 128)\n",
    "        \n",
    "        # Level 3 (128 -> 256, with downsampling)\n",
    "        self.down2 = Down(128, 256)\n",
    "        \n",
    "        # Level 4 (256 -> 512, with downsampling)\n",
    "        self.down3 = Down(256, 512)\n",
    "        \n",
    "        # Level 5 (The deepest layer, or \"bottleneck\" - 512 -> 1024, with downsampling)\n",
    "        # This is where the most abstract contextual features are captured.\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # --- Expanding Path (Decoder) ---\n",
    "        # Level 5 (1024 -> 512, with upsampling and concatenation)\n",
    "        # Note: The in_channels for Up is the total channels *before* the initial up-conv.\n",
    "        #       So it's 1024 (from down4), which will be up-conved to 512, then concatenated with skip_connection (512 channels).\n",
    "        #       Thus, the DoubleConv inside Up1 takes 1024 channels as input and outputs 512.\n",
    "        self.up1 = Up(1024, 512) \n",
    "        \n",
    "        # Level 4 (512 -> 256, with upsampling and concatenation)\n",
    "        self.up2 = Up(512, 256)\n",
    "        \n",
    "        # Level 3 (256 -> 128, with upsampling and concatenation)\n",
    "        self.up3 = Up(256, 128)\n",
    "        \n",
    "        # Level 2 (128 -> 64, with upsampling and concatenation)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        # --- Output Layer ---\n",
    "        # Maps the final 64 channels to the desired number of output classes.\n",
    "        self.outc = OutConv(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the entire U-Net.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input image (e.g., [batch_size, 1, 572, 572]).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The predicted segmentation logits (e.g., [batch_size, num_classes, H_out, W_out]).\n",
    "        \"\"\"\n",
    "        # --- Contracting Path ---\n",
    "        # Each 'down' block returns the output of its DoubleConv, \n",
    "        # which will be used as a skip connection.\n",
    "        x1 = self.inc(x) # Output: 64 channels. Spatial: 572x572 -> 568x568\n",
    "        x2 = self.down1(x1) # Output: 128 channels. Spatial: 568x568 (maxpool) -> 284x284 (DoubleConv) -> 280x280\n",
    "        x3 = self.down2(x2) # Output: 256 channels. Spatial: 280x280 (maxpool) -> 140x140 (DoubleConv) -> 136x136\n",
    "        x4 = self.down3(x3) # Output: 512 channels. Spatial: 136x136 (maxpool) -> 68x68 (DoubleConv) -> 64x64\n",
    "        x5 = self.down4(x4) # Output: 1024 channels. Spatial: 64x64 (maxpool) -> 32x32 (DoubleConv) -> 28x28 (This is the bottleneck)\n",
    "\n",
    "        # --- Expanding Path ---\n",
    "        # Each 'up' block takes the upsampled feature map from the previous level (x_i)\n",
    "        # AND the corresponding skip connection from the contracting path (x_skip).\n",
    "        # The skip connections provide fine-grained spatial information.\n",
    "        x = self.up1(x5, x4) # Input: x5 (28x28, 1024), x4 (64x64, 512). Output of up1: (56x56, 512)\n",
    "        x = self.up2(x, x3) # Input: x (56x56, 512), x3 (136x136, 256). Output of up2: (112x112, 256)\n",
    "        x = self.up3(x, x2) # Input: x (112x112, 256), x2 (280x280, 128). Output of up3: (224x224, 128)\n",
    "        x = self.up4(x, x1) # Input: x (224x224, 128), x1 (568x568, 64). Output of up4: (448x448, 64)\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        # Final 1x1 convolution to map the feature channels to the number of classes.\n",
    "        # This gives the raw class scores (logits) per pixel.\n",
    "        logits = self.outc(x) # Output: (batch_size, num_classes, H_out, W_out), H_out, W_out are 388x388 for 572x572 input\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. Instantiate the U-Net model\n",
    "    # For grayscale input (1 channel) and 2 output classes (e.g., foreground/background)\n",
    "    model = UNet(in_channels=1, num_classes=2)\n",
    "    print(f\"Model architecture:\\n{model}\")\n",
    "\n",
    "    # 2. Create a dummy input tensor\n",
    "    # The paper's example input size is 572x572 for a 5-level U-Net with padding=0.\n",
    "    # This size ensures a clean integer output size (388x388 in this case).\n",
    "    batch_size = 1\n",
    "    input_channels = 1\n",
    "    input_height = 572\n",
    "    input_width = 572\n",
    "    dummy_input = torch.randn(batch_size, input_channels, input_height, input_width)\n",
    "    print(f\"\\nDummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "    # 3. Perform a forward pass\n",
    "    output_logits = model(dummy_input)\n",
    "    print(f\"Output logits shape: {output_logits.shape}\")\n",
    "\n",
    "    # Expected output shape: (Batch, num_classes, 388, 388)\n",
    "    # The actual spatial size (388x388) is smaller than the input (572x572) \n",
    "    # due to 'padding=0' in all Conv2d layers.\n",
    "    # This confirms the U-Net's characteristic output shrinking.\n",
    "\n",
    "    # 4. (Optional) Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        dummy_input = dummy_input.cuda()\n",
    "        output_logits_gpu = model(dummy_input)\n",
    "        print(f\"Output logits shape on GPU: {output_logits_gpu.shape}\")\n",
    "        print(\"Model and data moved to GPU.\")\n",
    "    else:\n",
    "        print(\"CUDA not available. Running on CPU.\")\n",
    "\n",
    "    # 5. Calculate total number of parameters (important for understanding model complexity)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal trainable parameters: {total_params:,}\") # ~31 million parameters for this configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
